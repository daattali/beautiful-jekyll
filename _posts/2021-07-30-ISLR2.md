---
layout : post
title : ISLR
subtitle : statistical learning - 2장
date : 2021-07-29
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

#### 2.1  What is statistical learning?

##### 2.1.1

X : predictor, features, independent variable

Y : response, dependent variable


$
Y = f(X) + \epsilon   
\hat Y  = \hat f(X)
$

reducible error  :  Y를 추정하기 위해 hat Y 를 구하는 것

irreducible error  :  Y는 Epsilon의 함수이므로 X를 가지고 추정이 불가하다



Why is the irreducible error larger than zero ?

1. the quantity epsilon may contain **unmeasured** variables that are useful in predicting Y

2. the quantity epsilon may also contain **unmeasurable** variation

$
E(Y - \hat Y )^2 \ \ = \ E[f(X) + \epsilon - \hat f(X)]^2 \ = \ [f(X) - \hat f(X)]^2 \ + \ Var(\epsilon)
$

마지막 등식의 앞 부분이 reducible error, 뒷 부분이 irreducible error 를 의미한다.



궁극적으로 풀고자 하는 문제가 prediction인지 inference(모델 해석, 독립/종속 변수 관계 해석) 인지에 따라서 f 를 추정하는 데에 다른 접근 방식이 요구된다. 예를 들어 prediction 같은 경우에는 비선형 모델이 좋다. 그러나 inference 의 경우에는 선형 모델이 더 좋을 것이다.



test data를 가지고 f를 추정하게 되는데 이때 statistical learning methods로서 parametric / non-parametric 방법이 있다.



##### 2.1.2

###### 1. Parametric Methods(model based approach)

step 1. 함수 형태를 우선적으로 가정한다(linear or non-linear)

step 2. training data를 통해 적합한다

step 3. parameter를 추정한다.



단점 : 

1. 모델을 잘못 설정하면 추정 자체가 의미가 없다.   
2. 1번 문제를 막기위해 flexible한 모델을 설정하면 추정해야할 파라미터가 많아진다.
3. 추정해야할 파라미터가 많아진다는 것은 오버피팅의 위험이 있다는 것.



###### 2. Non-parametric Methods

함수 형태를 우선적으로 가정하지 않는다.  대신 데이터들에 최대한 적합하는 f 를 국소적으로 찾아낸다. 대신 몇개의 파라미터에 의존하는 parametric method에 비해 수많은 데이터가 있어야만 f 에 대한 정확한 추정이 가능할 것이다. (즉 데이터 수가 적은 경우에는 단순한 모형으로 parametric 하게 접근하는 것이 좋다)

7장에서 배우는 spline은 non-parametric method인데 smoothness를 크게 하면 에러를 완전히 줄이는 fitting을 하게 된다(overfitting의 문제).



##### 2.1.3 Trade off between prediction accuracy and model interpretability

1. restrictive method(interpretability 가 높은 모델)는 쉽게 설명이 가능하고 인과성을 간단히 보이기 쉽다.

2. spline이나 boosting 같은 복잡한 모형은 predictor들이 response와 어떻게 연결되어있는지 알기 어렵다
3. lasso 같은 경우는 제일 restrictive한데 이는 일부 변수를 완전히 0으로 만들어버려 변수간의 관계를 찾기 더 쉽기 때문이다. 
4. Generalized Additive Models(GAMs) 같은 경우 linear 모델을 확장해서 유연하게 만든 것으로,  curve를 허용해서 새롭게 모델을 만든다.
5. 그러나 무엇보다 오버피팅의 문제에 있어서 predictive accuracy가 높을 것이라 예상했던 flexible 한 모델이 반드시 예측력이 좋은 것이 아니므로 주의할 것.

<img src="/assets/img/islr2-1.png" width="70%" height="70%" title="1" />

##### 2.1.4  Supervised vs Unsupervised

linear regression, GAMs, boosting, SVM 등은 모두 지도학습이다. 즉 찾고자하는 혹은 예측하고자 하는 target 값이 존재한다. 반면 비지도학습의 경우 관찰값(obs)만 존재할 뿐 target 값이 없다. 이 상황에서는 obs값들의 관계를 찾아내야 하는데 여기서 쓰이는 방법으로는 Clustering이 있다. 또한 predictor에 비해 response 의 개수가 적을 수도 있다. 이런 경우는 semi-supervised learning을 사용한다. 단 이 방법은 이 책의 범위를 넘는다.



##### 2.1.5  Regression vs Classification

양적 변수 예측 / 질적 변수 예측이냐에 따라 다르지만, KNN이나 boosting 같은 경우에는 두 가지 속성의 변수에서 모두 사용가능하다. 



#### 2.2 Assessing Model Accuracy

"<u>There is no free lunch in statistics : no one method dominates all others over all possible data sets.</u> "

##### 2.2.1 measuring the quality of fit

MSE(Mean Squared Error)

$
MSE = \frac {1}{n} \sum ^n (y_i - \hat f (x_i))^2
$

그러나, 이것은 training data로 만드는 것이므로 training MSE로 부르는 것이 더 나을 수도 있다. 우리는 예측의 문제를 다루는 데 있어 새로운 데이터를 얼마나 잘 찾아낼지에 관심이 있으므로 test MSE 가 더 궁금할 수 있다. 즉, 새로운 데이터를 (x_0, y_0) 라고 할 때 

$
Average(\hat f(x_0) - y_0)^2
$

이를 최소화 하는 모델을 만들고 싶은 것이다. 그러나 새로운 데이터가 항상 full-set으로 존재하는 것은 아니므로 이를 계산 하기 어렵다.(이에 대한 해결방안으로 cross-validation 이 있다) 그렇다고 training MSE를 최소화 하는 모델을 찾는다면 오버피팅의 문제가 발생한다. 즉 training MSE는 무한정 줄어들 수 있으나(random error까지 모델에 넣어버림) 그러한 모델에서는 test MSE가 매우 커진다. 

<img src="/assets/img/islr2-2.png" width="70%" height="70%" title="2" />

또한 모델은 training data와의 error를 줄이는 방향으로 학습되고 있기 때문에,  less flexible model의 test MSE가 더 적은 경우에도 overfitting 문제가 발생한 것일 수도 있다.

##### 2.2.2 The Bias - Variance Trade - Off

Expected test MSE ( overall expected test MSE는 이 값을 average 한 것 )

$
E(y_0 - \hat f (x_0))^2 = Var(\hat f(x_0)) + [Bias(\hat f(x_0))]^2 + Var(\epsilon)
$

Expected test MSE를 줄이기 위해서는 hat f(x_0) 의 추정값에 대한 분산과 편의를 모두 줄여야 한다. 또한 이 값이 모두 양수이므로 Expected test MSE는 irreducible error인 Var(epsilon) 의 값을 넘지 못한다.

그리고 분산과 편의의 trade-off  관계에 의해서 Expected test MSE는 모델이 특정 flexibility를 넘어서면 증가하게 된다.

<img src="/assets/img/islr2-3.png" width="70%" height="70%" title="3" />



##### 2.2.3  The Classification Setting

분류문제에서 추정함수의 정확도(accurarcy)를 평가하는 기준은 training error rate

$
\frac {1}{n}\sum^n I(y_i \neq \hat y_i)
$

regression 과 마찬가지로 새로운 데이터에 대한 모델의 성능을 알고 싶으므로 test error rate을 살핀다

$
Average(I(y_0 \neq \hat y_0))
$


###### The Bayes Classifier 

test error rate 을 최소화하는 방법으로서 bayes classifier 가 있는데 이는

$
P(Y=j |X=x_0)
$

가 가장 큰 class j 로 추정하는 분류기를 말한다. 

아래의 그림을 보면 노란색과 파란색으로 점들이 분류되어 있고 각각은 조건부 확률이 0.5 이상인지 여부에 따라 분류된 것이다 (두가지 class 밖에 없기 때문)

또한 보라색 점선은 Bayes Decision Boundary로 확률값이 정확히 0.5 인 부분이다.

<img src="/assets/img/islr2-4.png" width="70%" height="70%" title="4" />

앞서 말한대로 bayes classifier는 test error rate 을 최소화 하는데, 이렇게 최소화된 test error rate을 Bayes error rate이라고 한다. (즉 베이지 error 비율은 test set에서의 error 비율인 것)

Bayes error rate at X=x_0

$
1 - max_j P(Y=j|X=x_0)
$

(Bayes classifier는 확률 값이 가장 큰 class 를 선택하기 때문에)



simulation 결과 bayes error rate 은 0.13 정도인데 이는 경계값에 속하는 점들의 최대 분류 확률이 1 미만이기 때문에 발생하는 결과이다. 즉 이는 regression setting에서 irreducible error와 유사하다.

$
max_j P(Y=j | X=x_0) < 1 \ \ \ where\  obs\  are \  in \ boundary
$


###### K-Nearest Neighbors(KNN)

현실 데이터에서는 조건부 분포를 알기 어려운 경우가 많기 때문에 Bayes Classifier를 사용하기 어렵다. 따라서 조건부 분포를 추정하기 위한 시도들이 많이 전개되었는데, 이러한 시도로 탄생한 것이 KNN이다. 점 x 주변에 K개의 점들을 잡아서 조건부 분포를 살피는 것이다.

$
P(Y=j|X=x_0) = \frac {1}{K} \sum_{i\in N_0}I(y_i=j)
$

이를 통해 확률이 가장 큰 값으로 class를 설정하게 된다. 

KNN에서는 K값을 어떻게 설정 할 지가 관건이다. 이는 flexibility를 결정하는 값이다. K의 값을 매우 줄이면 오버피팅의 문제가 있고, 반대로 매우 크게 하면 bias가 매우 커진다(underfitting).

<img src="/assets/img/islr2-5.png" width="70%" height="70%" title="5" />


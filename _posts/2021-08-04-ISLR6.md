---
layout : post
title : ISLR
subtitle : statistical learning - 6장
date : 2021-08-03
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 6. Linear Model Selection and Regularization



#### 6.1 Subset Selection

inference(모델의 해석력, 직관성)이 좋은 선형모델을 다른 방식으로 fitting 할 수는 없을까?



(1) subset selection

domain 지식, 혹은 계수에 대한 유의성 검정 결과 등으로 불필요한 변수를 제거 할 수 있을 것이다.

(2) shrinkage

특정 변수에 대한 회귀계수를 0에 가깝게 만들어줌으로써, 분산을 줄일 수 있고 특히 Lasso는 아예 계수값을 0으로 만들어 변수 선택과 같은 효과를 가져오게 한다.

(3) dimension reduction(p --> m)

p개의 변수를 m개의 linear combination으로 표현한다든지, 혹은 projection을 통해서 변수의 개수를 m개로 줄여 이를 이용해 최소제곱법으로 모형 적합.



다른 방식으로 얻을 수 있는 것은?

(1) prediction accuracy

선형모형은 데이터셋이 많고, true 모형이 선형일 때에 뛰어난 성능을 발휘. but 그렇지 않은 경우 선형모형에 적합을 시키면 과적합이 발생. 이를 방지하기 위해,  회귀 계수에 constraining or shrinkage를 줌으로써 예측 성능을 높일 수 있다.



(2) 필요없는 변수의 계수 값을 0으로 만들어줌으로써 모델의 복잡성을 줄이고 해석력을 높일 수 있다. 



##### 6.1.1 Best Subset Selection

p개 변수(여기서 p가 연속형 변수)인 경우 
$$
2^p
$$
번 모형을 적합시켜서 최고의 모델을 선택하기. 그런데 p가 크면 모형 개수가 너무 많아진다.

알고리즘은 아래와 같다. 

<그림>

핵심은 각 변수 개수 별로 최고의 모델을 찾은 다음에(기준 RSS, R-square ※ 이 기준은 변수 개수가 같을 때 쓰는 것이다. 변수가 증가하면 R-square는 필연적으로 증가하기 때문에 변수 개수가 동일 할 때 선택 기준 ),  그 중에서 예측력이 좋은, 즉 cross-validated prediction error, mallow-cp, AIC, BIC, adjusted R-square 가 높은 모델을 선택.



##### 6.1.2. Stepwise Selection

단순한 forward, backward와는 조금 다르다

forward stepwise selection을 가지고 설명하자면,

변수가 없는 초기모델에서 

변수가 하나만 있는 모델에서 RSS나 R-square 가 제일 나은 모델을 선택하고

그 모델을 기준으로 또 변수를 하나 더 추가해서 그 중 RSS나 R-square 가 제일 나은 모델을 선택

이 과정을 반복해서 변수의 개수 별로 최적 모델을 찾은 다음에 그 중 예측력이 좋은, 즉 cross-validated prediction error, mallow-cp, AIC, BIC, adjusted R-square 가 높은 모델을 선택.

<그림>



backward stepwise selection은 그 반대

<그림>





subset selection method 보다 계산량이 줄어든다. 그러나 첫번째 선택한 변수를 기반으로 다음 변수가 선택되기 때문에 최적의 모델(변수)를 찾지 못할 수도 있다. 

또한 forward 방법은 backward와 다르게 high dimension(n<p)에서도 적용 가능하다.



이를 섞어놓은 hybrid approach도 존재



##### 6.1.3 Choosing the Optimal Model

(1) <u>indirectly</u> estimate test error by **making an adjustment to the training error** to account for bias



###### C_p

$$
C_p = \frac {1}{n}(RSS + 2p \hat \sigma^2)
$$

p는 변수의 개수를 의미

 뒷 항이 변수의 개수에 대한 penalty를 주는 것으로 변수의 개수가 많아지면 뒷 항은 커진다. 그리고 C_p 값은 작아야 좋으며 분산 추정량 sigma^2는 불편추정량이므로 C_p는 test MSE에 대한 불편추정량이다.  
$$
mallow's \ c_p = RSS/\hat \sigma^2 + 2p -n \ =\ \hat \sigma^2(C_p + n) 
$$


###### AIC

maximum likelihood를 사용해서 적합성을 설명.
$$
AIC = \frac {1}{n\hat \sigma^2}(RSS + 2p\hat\sigma^2)
$$
식에서도 알 수 있듯, C_p와 비례하기 때문에 설명 생략



###### BIC

베이지안 관점
$$
BIC = \frac {1}{n} (RSS + log(n)p\hat\sigma^2)
$$
C_p 와의 차이는 n의 크기가 커질수록, 즉 데이터 개수가 많아질 수록 변수 개수에 대한 penalty를 더욱 크게 주는 것이다. 따라서 C_p에 비해서 더욱 변수 개수가 작은 모형을 선택하려고 할 것이다. 



※여기서는 선형모형의 최소제곱법을 기준으로 모형을 적합하기 때문에 RSS로 썼는데 AIC, BIC의 일반적인 수식은 아래와 같다
$$
-2 logL + kp,\ \ L \ is\ likelihood
$$
AIC는 k=2, BIC는 k=log(n)



###### adjusted R-square

$$
Adjusted \ R^2 \ = \ 1 - \frac {RSS/(n-p-1)}{TSS / (n-1)}
$$

앞서 본 C_p, AIC, BIC와는 다르게 adjusted R-square는 큰 값일 수록 모형적합 및 예측이 뛰어난 것이다. 





(2) <u>directly</u> estimate the test error using validation set or CV 

따로 가정이 필요없고 test error rate을 직접 추론한다. 과거에는 대용량의 계산이 어려워서 C_p, AIC,  BIC 등을 사용했지만, 컴퓨팅 기술이 발달한 현재 CV가 모델 선택에 있어 훨씬 좋다.

앞 장에서도 설명했듯,  CV,  Validation set error 는 valid set을 어떻게 설정하는지에 따라  test mse 값이 바뀔 수 있다. 즉 test mse를 최소로 하는 변수의 개수 또한 바뀔 수 있다. 그럼 변수 개수를 어떻게 해야 최적의 모델을 고를 수 있을까?  이때는 one-standard-error라는 조건으로 모형을 결정한다. 

아래 그림은 CMU-STAT 383C - Statistical Modeling 1 Lecture 9 강의노트에서 캡쳐해온 것이다

<그림 4>

즉 CV의 결과로 변수 개수에 따른 test error rate을 구했다고 하자. 이 때 error를 최소로 하는 값에 대한 1 standard error  범위에 속한 모델(변수 개수) 중 가장 단순한 모델(변수 개수가 가장 작은 모델)을 선택한다.



#### 6.2 Shrinkage Methods

분산을 줄여준다!



##### 6.2.1 Ridge Regression

tuning parameter lambda를 사용해서
$$
\sum (y_i - X\beta)^2 + \lambda\sum\beta_j^2
$$
를 minimize 하는 
$$
\hat \beta ^R
$$
를 추정하는 것.



penalty 항이 존재해서 RSS를 계속해서 줄여나가지 않고 penalty 값을 줄여나가려고 한다. 



tuning parameter lambda는 penalty항의 크기를 결정함으로서 계수 추정값을 변화시키는데

그 값이 커지면서 규제(regulation)의 효과가 커지고 계수 추정량이 0에 가까워 질 것이다. 따라서 lambda값을 적절히 설정하는 것이 중요하다.

또한 predictor 값이 모두 0일 때의 response를 나타내는 intercept Beta_0는 shrinkage에 영향을 받지 않으며 만약 데이터들이 centered 된다면 절편 추정량은 response의 표본평균이 될 것이다.

<그림>

그림에서 보듯 적절한 lambda 값을 통해 규제를 주면 전혀 불필요한 변수들에 대한 계수값이 0이 되면서 분산이 줄어들고 fit도 더 잘되어 MSE가 줄어들 수 있다. 특히 
$$
X'X
$$
에 대한 가장 큰 고유벡터 방향이 회귀 계수 beta 값과 일치한다면 MSE의 향상이 더 좋을 것이다. (몽고메리 부분 참고)

또한 ridge에 의해 회귀계수가 축소가 되기 때문에 ridge 이전에 변수들의 scaling을 조절하는 것이 중요하다. 그렇지 않으면 (scale과 lambda값 둘 다에 영향을 받게 되어 오류가 생길 수 있음)







그렇다면 왜 Ridge regression이 least square 보다 좋은 것일까?

==> bias - variance trade off 

variance를 크게 줄임으로서 test MSE를 줄일 수 있다.

그림

predictor와 response가 서로 선형이라면 이들 관계는 특정한 값에 의해 크게 바뀔 수 있다. 즉 분산이 커질 우려가 있다. 또한 high dimension (p>n)  인 경우 least square 계수 추정량은 유일하지 않다. 이 때 ridge regression을 사용하면 분산을 안정화시킬 수 있다.

또한 변수 선택의 관점에서도 ridge를 사용하면 best subset을 하나하나 찾는것 보다 훨씬 비용이 절감되고 계산량이 줄어든다.



##### 6.2.2 The Lasso

Ridge 의 단점은 모든 변수가 모형에 들어간다는 것이다. 그것의 계수가 비록 매우 작은 값일지라도.... 따라서 예측의 문제보다는 모형 자체의 해석을 할 때 단순하게 만들어진 모델에 비해 복잡할 것이다.  이러한 점을 고려해서 Lasso regresssion이 나오게 되었다.
$$
\sum (y_i - X\beta)^2 + \lambda\sum|\beta_j|
$$
Lasso 는 계수값을 완전히 0으로 만드는데 이는 variable selection의 일종이라 볼 수도 있으며, Ridge에 비해 모델 해석이 쉽다(model interpretation , inference)

또한 ridge 처럼 lambda값을 잘 구하는 것이 중요한데, 이에 대한 것은 6.2.3에서 설명하도록 하겠다.



###### Another Formulation for Ridge and Lasso

라그랑주를 사용해서 다음과 같이 표현할 수 있다.
$$
minimize \ \sum (y_i - X\beta)^2  \ \ \ subject \ \  to \ \ \sum\beta_j^2 \leq s
$$

$$
minimize \ \sum (y_i - X\beta)^2  \ \ \ subject \ \  to \ \ \sum|\beta_j| \leq s
$$

이러한 방식으로 best subset selection 방식도 표현 가능한데, 단 계산량이 너무 많아서 ridge 나 lasso가 좋고, 변수 선택에 입장에서는 ridge 보다는 lasso 가 더 좋은 역할을 하게 된다. 
$$
minimize \ \sum (y_i - X\beta)^2  \ \ \ subject \ \  to \ \ \sum I(\beta_j \neq 0) \leq s
$$


###### Variable Selection Property of the Lasso

어떻게 Lasso는 계수값을 정확히 0으로 만드는 것일까? 아래 그림을 보자

<그림>

그림에서 보면 알 수 있듯 lasso 의 경우 코너해에서 회귀계수가 결정되는 양상을 보인다. 



###### comparing Lasso and Ridge

해석력의 관점에서는 lasso가 좋다 (변수 자체가 작아지니까) 그럼 prediction 관점에서는??

ridge나 lasso 모두 lambda 값에 의해 bias-variance trade-off 관계가 발생하고 lambda 값이 커지면서 분산이 안정화 된다. 다만 어떤 방식이 더 낮은 test error rate을 가질지는 데이터의 형태에 따라 다르고 변수의 개수에 따라서도 다르다.

일반적으로 계수값의 편차가 큰 경우에는 Lasso가 그렇지 않은 경우에는 Ridge가 더 좋은 prediction을 갖는다. 



###### Bayesian Interpretation for Ridge and Lasso

bayes' theorem 그대로 이용해서 posterior의 mode값을 계수 추정량으로 선택
$$
P(\beta | X, Y) \propto f(Y|X, \beta ) P(\beta) 
$$
likelihood는 선형모델로 고정, epsilon은 iid 성질을 가진 normal
$$
Y = X\beta + \epsilon
$$
prior는 
$$
P(\beta) = \prod g(\beta_j)
$$
이 때 

ridge는 g가 Gaussian with mean zero and standard deviation a function of lambda.

lasso는 g가 Double-Exponential with mean zero and scale parameter a function of lambda

<그림>



##### 6.2.3 Choosing the Tuning Parameter

lambda를 어떻게 설정할 것인가?

==> grid search. 후보를 정해놓고 cross-validation 결과 나온 error 값 중 가장 낮은 error를 갖는 lambda를 선택한다. 



#### 6.3 Dimension Reduction Methods

앞에서 분산을 줄이는 과정으로 predictor의 subset을 선택한다던지, ridge 혹은 lasso를 하는 방식을 설명했다. 여기에서는 적합시키는 predictor 자체를 바꾸는 방법을 설명하겠다. 

기본 틀은 다음과 같다

(1) 기존 확률변수
$$
X_1, X_2, ... , X_p
$$
에 대해서 M<p 인 M개의
$$
Z_1, Z_2, ...,Z_M
$$
변수를 X들의 linear combination으로 만들어 낸다
$$
Z_m = \sum_{j=1} ^p \phi_{jm}X_j
$$


(2) 이제 확률변수 Z를 가지고 적합을 한다. lienar 모델인 경우
$$
y_i = \theta_0 + \sum_{m=1} ^M \theta_m Z_{im} + \epsilon_i, \ \ \ \ \ \ i=1,2,...,n
$$


(3) phi 값을 어떻게 설정할 것인지에 대해서는 두가지 방법이 존재한다. (6.3.1 / 6.3.2)



##### 6.3.1 Principal Components Regression

<그림>

가장 큰 분산을 가지는 green line : the first principal component 

mathmatically?

formula : 
$$
Z_1 = \phi_1 \times(X_1-\bar X_1) + \phi_2 \times(X_2-\bar X_2)
$$
여기서 Z 값이 principal component score이며 Z_1의 분산이 가장 크다.



또한 ,이 때 
$$
\phi
$$
값은 principal component loading이며 
$$
\sum \phi_i^2 = 1
$$
이라는 성질을 갖는다.(이러한 제약이 없다면 임의로 분산을 마구 키울 수 있기 때문)



PCA에 대한 또 다른 해석으로는 첫번째 PC가 sum of the squared perpendicular distance를 최소화 하는 line이라는 것이다. 즉 데이터와 가능한 가장 가까운 직선을 긋는 것이라 볼 수 있다. 



###### Principal Components Regression Approach

아마 PCA를 공부하면서도 가장 중요한 문장이 나온다

"**The key idea is that often a small number of principal components suffice to explain most of the variability in the data**"

즉 분산이 큰 방향을 축으로 데이터를 사영시켰을 때 나오는 principal component 몇개를 가지고 데이터를 충분히 설명할 수 있다면 이를 변수로 삼아 Y와의 관계를 찾을 때 fitting이 더 좋다는 것이다.(변수 개수가 작으니까 당연히 오버피팅도 막을 수 있고) (<u>단, PCA를 통해 차원의 축소가 잘 안된다면 그냥 원래 데이터를 가지고 ridge나 lasso를 하는 것이 훨씬 fitting이 잘 될수 있다.</u>)

pca를 통해 축소가 잘 안되는 경우?  데이터의 변수가 원래 작아서 각각이 모두 의미를 담고 있는 경우



참고 : PCR은 Ridge regression과 매우 유사하며 Ridge가 PCR의 연장선상에 있다고도 볼 수 있다



###### how to find the number of principal components

PCR에서 적절한 변수의 개수는 어떻게 찾을 것인가. 

Principal component score 들은 크기에 따라 순차적으로 정해지고 PC line은 서로 직교하게 만들어진다. 그런데 만약 다음 score 값이 거의 0에 근사한다는 것은 이전의 principal component를 가지고 데이터를 충분히 설명가능하다는 말이다. 

그러나 이는 다소 모호하다

 확실한 기준을 위해선 CV를 통해서 test mse를 최소로 하는 변수 개수를 선택하면 될 것이다. 



###### final check

PCR은 데이터의 분산을 다루는 것이므로 scale에 민감하다. 따라서 PCR을 수행하기 전에는 반드시 데이터들을 정규화해주어야 한다!



##### 6.3.2 Partial Least Squares

PCR은 PC direction을 찾는 데에 Y(target) 값을 전혀 이용하지 않는다. un-supervised way

따라서 PCR을 하면서 찾은 best direction이 실제로 best가 아닐 수 있다. 

==> Y를 고려해서 차원을 축소하는 지도학습법 partial least square가 등장.

요약 : PLS를 통해서 response와 predictor를 동시에 설명하는 direction을 찾자!

<그림>



step 1

PCR과 마찬가지로 표준화를 먼저 한다. 단 PLS는 response 를 같이 고려하기 때문에 response variable 또한 표준화 해줘야 한다. 



step 2

simple linear regression Y onto X_i ( i = 1,2, ... , p) 에서 나온 각 변수들에 대한 계수 값(phi)을 가지고 새로운 변수 Z_1을 만들어 낸다. 즉,
$$
Z_1 = \sum \phi_iX_i
$$
이렇게 하면 Y와 큰 상관관계를 가지는 변수에 대해 큰 weight를 줄 수 있다. 



step 3

second PLS direction을 만들기 위해 simple linear regression X_i onto Z_1 

여기서 나온 잔차들은 각 변수들에서  Z_1이 설명하지 못하는 부분이다. 이 residual을 다시 X와 simple linear regression 해서 나오는 계수를 가지고 Z_2를 만든다.  이 과정을 반복한다.



step 4 

차원을 얼마나 축소할 수 있을까? CV를 사용해라.



#### 6.4 Considerations in High Dimensions



##### 6.4.1 High-Dimensional Data

다양한 정보를 얻게 되면서 데이터 분석에서 이용되는 feature가 많아졌다. n보다 p가 더 큰 경우 오버피팅의 문제가 발생한다. 

##### 6.4.2 What Goes Wrong in High Dimensions?

overfitting ==> 예측력이 떨어짐

예측력의 평가지표인 C_p, AIC, BIC 등을 계산하는 데에 문제가 있음. 왜? error의 분산인 sigma 값을 추정하기 어렵기 때문에. 

adjusted R-square 또한 계산이 어려움. high dimension인 경우 과적합 발생으로 R-square가 1일수도 있기 때문 

##### 6.4.3 Regression in High Dimensions

변수선택이나 차원 축소를 통해서 high dimension 문제를 해결할 수 있다. 

차원의 저주?  흔히 feature를 많이 넣으면 fitting의 결과도 좋을 것이라 생각한다. 이것은 일부는 맞다. target과 관련이 큰 feature를 집어넣으면 그리고 이 데이터들의 noise가 작다면 성능이 좋아질 수 있다. 그러나 만일 추가된 feature가 noise가 많아 target과 큰 관계를 보여주지 않는다면, 그래서 기존의 데이터 개수만으로는 충분히 관계를 찾아낼 수 없다면 모델의 성능은 더욱 악화될 것이다. 

또한 좋은 feature라고 할지라도 bias 감소보다 그에 따른 분산의 증가가 더 클수도 있기 때문에 이를 항상 고려해야한다. 

##### 6.4.4 Interpreting Results in High Dimensions

high dimensions 에서는 multicollinearity 의 문제도 피할 수 없다. 

변수선택으로 얻은 모델을 과신하지말고 다른 좋은 모델은 없는지 검증해보아야 한다. 

모델 평가 측도로 overfitting에 취약한 SSE(sum of squared error), p-values, R-squared 들을 선택해서는 안된다. test data 에 대한 예측력은 장담할 수 없기 때문.
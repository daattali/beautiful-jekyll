---
layout : post
title : ISLR
subtitle : statistical learning - 6장 - LAB
date : 2021-08-05
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 6장 코드 정리

~~~R


## Best Subset Selection

library(ISLR)
fix(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters) # 변수에 missing variable 이 존재하는 경우 이를 삭제
sum(is.na(Hitters)) # 0

#regsubsets 명령어를 사용해서 best subset 구하기 
#best의 기준은 가장 작은 RSS
library(leaps)
regfit.full = regsubsets(Salary~.,data=Hitters)
summary(regfit.full)

regfit.full = regsubsets(Salary~.,data=Hitters, nvmax = 19) #nvmax 명령어는 뽑고싶은 최대 변수 개수를 나타낸다.
reg.summary = summary(regfit.full)
reg.summary$rsq
reg.summary$adjr2

par(mfrow=c(1,1))
plot(reg.summary$rss, xlab= "varibale number", ylab= "RSS", type='l' ) #type 에 영어 l 을 붙이면 점들을 연결한 선이 그려진다. 
plot(reg.summary$adjr2, xlab= "varibale number", ylab= "ADJR2" )

which.max(reg.summary$adjr2) #언제 최대가 될까?
points(11, reg.summary$adjr2[11],col='red', cex=2, pch=20)#점찍기

#regsubsets()는 내장함수로 plot을 가지고 있음. 각 scale 별로 최적의 변수 조합을 보여준다.
plot(regfit.full, scale="r2")
plot(regfit.full, scale='adjr2')
plot(regfit.full, scale='Cp')
plot(regfit.full, scale='bic')

coef(regfit.full,6) #변수가 6개일 때 RSS 기준 best인 변수들을 추출 


## Forward and Backward STepwise Selection

regfit.fwd = regsubsets(Salary~.,data=Hitters, nvmax = 19, method = "forward") #nvmax 명령어는 뽑고싶은 최대 변수 개수를 나타낸다.
#backward로 하고 싶으면 method = "backward"


## validation set approach vs CV

#train / test 만들기 위해 임의로 index 나눠주기
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
test = (!train)

regfit.best = regsubsets(Salary~., data=Hitters[train,], nvmax=19)

#model.matrix를 사용해서 test data 를 matrix로 만들기
test.mat = model.matrix(Salary~.,data=Hitters[test,])

#각 변수 개수 별로 MSE 값 보이기
val.errors = rep(NA,19)
for (i in 1:19){
    coefi = coef(regfit.best, id=i)
    pred = test.mat[,names(coefi)] %*% coefi
    val.errors[i] = mean((Hitters$Salary[test]-pred)^2)
}
val.errors
which.min(val.errors)
coef(regfit.best, 7)

#K-fold, k=10
k=10
set.seed(1)
folds = sample(1:k, nrow(Hitters),replace=TRUE)
cv.errors = matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

#predict 메소드 만들기
predict = function(obj, newdata, id,...){
    form = as.formula(obj$call[[2]])
    mat= model.matrix(form, newdata)
    coefi = coef(obj, id=id)
    xvars = names(coefi)
    mat[,xvars] %*% coefi
}

#j : 몇번째 fold 인지, i : 변수의 개수가 몇개인지
for (j in 1:k){
    best.fit = regsubsets(Salary~., data=Hitters[folds !=j,],nvmax=19)
    for(i in 1:19){
        pred = predict(best.fit, Hitters[folds ==j,], id=i)
        cv.errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)
    }
}

#cv.errors 함수를 열별로 평균 구하기
mean.cv.errors = apply(cv.errors, 2, mean) #1 : 행단위 연산, 2 : 열단위 연산 

par(mfrow=c(1,1))
plot(mean.cv.errors, type='b')
which.min(mean.cv.errors)

rm(list = ls())





##Ridge // glmnet의 alpha=0


library(ISLR)
fix(Hitters)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters) # 변수에 missing variable 이 존재하는 경우 이를 삭제
sum(is.na(Hitters)) # 0

x = model.matrix(Salary~., Hitters)[,-1] #model.matrix 는 계산을 위한 행렬을 만들어주며, 특히 질적 변수를 더미화 시켜주는 기능까지 있다.
y = Hitters$Salary

library(glmnet)
grid = 10^(seq(10,-2,length=100))
ridge.mod = glmnet(x,y, alpha=0, lambda = grid)  #glmnet은 표준화 과정이 들어가있음.
ridge.mod$lambda[50]
coef(ridge.mod)[,50]

predict(ridge.mod, s=50, type='coefficients')[1:20,] 

set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]

ridge.mod = glmnet(x[train, ], y[train],alpha=0, lambda=grid, thresh=1e-12)
ridge.pred = predict(ridge.mod, s=4, newx=x[test, ]) 

#cv.glm으로 lambda 값 구하기
set.seed(1)
cv.out = cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.out)
bestlambda = cv.out$lambda.min
bestlambda

#가장 작은 mse를 가지는 lambda를 가지고 예측해보기
out = glmnet(x,y,alpha=0)
predict(out, type="coefficients", s=bestlambda)[1:20,] #여기 [1:20,] 은 predict메소드에서 나오는 결과값 중 20개를 리스트로 뽑아내느 것.

##lasso는 glmnet의 인자 alpha=1 을 주면 된다.


##PCR
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data = Hitters, scale=TRUE, validation="CV") #scale=true 는 standardizing.
summary(pcr.fit)

validationplot(pcr.fit, val.type='MSEP')


#train / valdiation

set.seed(1)
pcr.fit=pcr(Salary~., data = Hitters, subset = train, scale=TRUE, validation="CV") #scale=true 는 standardizing.
validationplot(pcr.fit, val.type='MSEP')
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)

##parital least square
#pcr 대신 plsr 로 함수를 바꾸면 나머지는 pcr 과 같다.
set.seed(1)
pls.fit = plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation='CV')
validationplot(pls.fit, val.type="MSEP")
pls.pred = predict(pls.fit, x[test], ncomp=2)
~~~


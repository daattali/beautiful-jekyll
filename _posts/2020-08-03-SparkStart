---

layout: post title: "Introducing chalk" description: "Chalk is a high quality, completely customizable, performant and 100% free blog template for Jekyll." thumb_image: "documentation/sample-image.jpg"

tags: [web, jekyll]
-------------------

#Spark란

데이터 엔지니어링을 시작하기 위해 필수적인 연산 플랫폼 스파크에 대해 공부를 시작하며 첫 포스팅을 남깁니다. 아파치 스파크는 범용적이며 빠른 속도로 작업을 수행하기 위한 클러스터용 연산 플랫폼입니다.

스파크의 초기 모델은 하둡 파일 시스템(HDFS)에 저장된 데이터를 연산하기 위해 만들어졌는데, 역시 천재들은 빠른 속도로 발전시켜 지금은 하둡을 포함한 카산드라(NOSQL)와 각종 RDB들까지 호환하기에 이릅니다. 심지어는 아마존에서 서비스하는 S3 저장소까지 연동시킬 수 있어 현재 데이터 엔지니어의 필수 플랫폼으로 자리하게 되었습니다.

스파크는 데이터를 메모리에서 분석해버리는 F1급 속도를 자랑합니다. 당연히 데이터 연산은 메모리에서 돌리는게 아니야? 라는 의문을 ~~컴퓨터를 전공하는 사람이라면~~ 가질 수 있지만 메모리 연산이 가능한 관계형 데이터베이스는 형식을 유지하는데 리소스를 사용하여 생각보다 빠르지 않고 이후 대용량 데이터 처리를 위한 하둡의 맵리듀스는 중간 과정의 결과를 디스크에 저장하며 진행해야 하기에 속도가 (스파크 보다) 빠르지 않습니다.

이러한 빠른 속도를 가진 스파크는 범용성을 중시하여 데이터 처리에 SQL 쿼리와 머신러닝 라이브러리를 제공합니다. 또한 다수의 클러스터 위에서 돌아가는 연산 작업을 스케줄링하고 자원을 분배하는 역할을 함으로써 스파크 자체의 최적화만으로도 대부분의 최적화가 이루어 지는 중앙제어장치와 같은 역할을 수행합니다.

스파크를 둘러싼 분산 데이터 셋 RDD와 대표적인 패키지에 대해 알아보겠습니다.

처음 스파크를 시작하는 사람으로 추후 공부를 통해 내용을 추가하겠습니다.

---

### 스파크 RDD

Resilient Distributed Dataset(RDD)는 스파크의 데이터 작업의 핵심 개념입니다. 간단히 말하면 변경 불가능한 분산 데이터 셋이라고 말할 수 있습니다. 스파크는 RDD를 만들거나 변환하고 RDD를 이용한 연산을 호출하는 방식으로 작동합니다. 거의 모든 작업은 RDD로 이루어져 있다고 감히 말할 수 있을 정도로 중요한 개념입니다.

< RDD이미지 추가 >

스파크의 작업은 크게 2가지로 트랜스포메이션과 액션입니다.

트랜스포메이션은 간단히 말하면 RDD를 만드는 작업입니다. 외부 데이터를 읽어오는(Load) 작업도 가능하고 자체적으로 데이터를 생성하는 방식도 가능하긴 합니다. 스파크에서 데이터를 처리하는 모든 과정은 RDD를 사용하는 것이므로 결국 트랜스포메이션은 RDD를 만드는 작업이라고 말할 수 있습니다. 외부 혹은 자체 데이터를 읽어오는 작업만 트랜스포메이션은 아닙니다. filter를 통해 기존의 RDD내의 필요한 데이터를 추출하여 저장하는 작업도 RDD를 생성하는 개념입니다. 리턴값이 RDD일테니까요.

중요한 점은 어떠한 작업도 원본 RDD를 변경하지 않습니다. RDD는 자체적으로 변경이 불가능한 데이터 셋이니까요.

액션은 드라이버 프로그램에 최종 결과 값을 되돌려주거나 외부 저장소에 값을 저장하는 연산 작업이다. 글자 개수를 세는 count, 전체 데이터를 가져오는 collect(메모리에 처리 할 수 있을 정도의 용량일 경우), 캐싱(persist 등), 로컬에 데이터를 저장하는 등 다양한 액션이 존재한다.

또한 액션은 스파크의 실질적인 작동을 의미한다. 이 의미는 간단히 말하면 어떤 트랜스포메이션을 하든 모든 실행은 액션이 실행되었을 때 실제로 실행된다는 뜻이다. 예를 들어 다음과 같은 순서로 명령을 했을 때

Load:test.txt -> Filter:'HI' in test

명령들은 실제로 실행되지 않고 메타데이터 속에 어떤 명령을 했는지에 대해 저장된다. 그 후

test.count('Spark')

명령을 실행하면 그제서야 스파크는 test.txt 파일을 읽으며 HI가 포함된 문자열을 저장하고 Spark의 개수를 세는 것이다. 이 방법은 Lazy, 즉 게으른 방식이라고 표현하며 적은 수의 데이터를 처리 할 때는 큰 필요성이 느껴지지않지만 빅데이터 이야기라면 이 방법의 효과는 확실하다. 데이터를 읽는 순간 메모리에 올리는 것보다 작업이 들어올 때 필요한 부분만을 메모리에 올림으로써 최대한 효율적으로 작업하려 노력한다.

---

### 스파크SQL

정형 데이터를 처리할 때 거의 완벽한 방법이라고 일컬어지는 SQL을 스파크는 지원합니다. RDB에서는 스키마의 문제로 비슷한 프로그램으로 짜여진 데이터베이스 파일을 사용해야 하는 경우가 많다면 스파크의 SQL은 SQL뿐만 아니라 하이브 테이블, 파케이(주로 사용), JSON 등 다양한 데이터 소스를 지원합니다. 즉 거의 모든 데이터 처리 과정에서 스파크를 얹어서 사용할 수 있습니다.

스파크 SQL을 사용할 때 일반적인 경우 DataFrame 개념을 사용하게 됩니다. col(열) * row(행) 형태의 2차원 표 형식으로 생각하면 쉬운데, 실제로는 DataFrame이 스파크에 기본 내장되어 있지 않다. 이러한 형태로 만드는 DataFrame API를 사용하거나 DataFrame를 임시 테이블에 넣어 기본 API를 사용하는 방법이 있다. 즉 데이터프레임 자체는 데이터 셋의 스키마와 같은 논리 구조를 가진 상자와 같은 형태이고, 읽는 건 내 맘대로 라는 뜻이다.

https://12bme.tistory.com/438

<실행 이미지 추가 예정>

---

### 스파크 스트리밍

스트리밍 데이터는 연속적으로 들어오는 데이터를 뜻한다. 예를 들면 사용자의 접속 로그데이터는 시간별로 생성되는 스트리밍 데이터라고 말할 수 있다.

스파크는 스트리밍 데이터를 자체적인 라이브러리를 통해 처리할 수 있으며, 일정 간격으로 batch로 묶어 처리한다.

1초 간격으로 데이터를 처리한다고 가정하고, 10명의 유저가 접속했다면 그 로그를 batch로 묶어 한 번에 처리하는 것이다. 일반적으로 batch를 통해 들어오는 데이터 중 사용 할 데이터를 filtering한 후 처리를 진행한다.

<실행 이미지 추가 예정>

---

### 스파크 MLlib / Graph X

MLlib와 Graph X는 각각 머신러닝 라이브러리와 그래프 관련 알고리즘 라이브러리이다.

<실행 이미지 추가 예정>

---

### 스파크 클러스터 매니저

추후에 이해되면 추가

---

---
layout : post
title : Bagging, Boosting, Stacking, Blending
subtitle : data mining - 앙상블 알고리즘
date : 2021-07-29
#categories:
tags : [datascience, machine learning, datamining]
toc_sticky : True
comments: true
---

### 1. Bagging

Bagging은 bootstrap aggregating을 줄인 말이다. overfitting을 방지하기 위한 앙상블 알고리즘의 하나로 분류와 회귀 모두에 사용된다. 앞선 글에서 boostrap을 상세히 설명했는데, boostrapping을 잘 알고 있다고 매우 쉬운 앙상블 알고리즘이라 할 수 있다. Boostraping은 주로 decision tree 모델에서 자주 사용되나 다른  모델에서도 사용할 수 있다.

전체 샘플에서 boostraping 샘플 K개를 추출해 내어, 모델에 K번 적합한다. 이후 K개의 모델에서 나온 K 개의 결과치를 가지고 이를 평균을 내거나(회귀) 혹은 투표(voting)(분류)을 통해 최종 예측치를 뽑아낸다



### 2. Boosting

"Weak learner to Strong learner" 이것은 boosting 알고리즘의 지향점을 나타낸다. Boosting은 bias와 variance를 동시에 줄이고자 하는 앙상블 알고리즘이다. Weak learner로 부터 잘못 예측한 부분에 대해 큰 가중치를 두고 이를 조정해가면서 최적의 예측치를 찾고자 한다.(weaker learner에 주목하는 것이 bias를 줄이고자 / 모델들을 결합한다는 데서 variance를 줄이고자 )

또한 부스팅 알고리즘에는 여려가지 종류가 있는데 기본적인 구조는 동일하되, weighting을 어떠한 방식으로 주는 지에 대해 차이가 있다. Adaboost는 가장 기본적인 boosting 알고리즘인데, weak learner를 다루는 첫번째 부스팅 알고리즘이었다. 순차적 학습과정에서 먼저 학습된 모델이 잘못 예측한 부분을 다음 학습시에 이용하면서 단점을 보완해나간다. 
$$
H(X) = \sum^t \alpha_i h_i
$$
H : strong classifier

alpha : weight of weak classifier

h : weak classifier

여기서 weak classifier는 weighted error가 가장 작은 모델. 또한 weighted error는 한번 시행할 때 잘못 분류할 수록 가중치를 크게 주는것.  헷갈리는 부분이 weighted error는 개별 모델의 시행에서 오차가 클 수록 가중치를 크게 주는 것이고, alpha의 값은 모델 자체의 성능에 대한 가중치이므로 모델의 성능이 좋을수록(오차가 작을수록) 가중치를 크게 준다.

종합하면 잘못 예측(분류)되면 weighted error가 커지고, error 가 커지니까 weak classifier가 되지 못함. 즉 이번 단계에서 제일 좋은 모델(h)은 weighted error가 가장 작은 모델

각 단계에서 나온 모델들을 가지고 alpha로 가중치를 준 앙상블이 boosting 알고리즘이다.



최근에는 LPBoost, TotalBoost, BrownBoost, XGBoost 등이 나와서 많이 사용되고 있고, 다른 boosting 알고리즘이 convex opimization 인 것에 비해, BrownBoost는 noise 가 많은 상황에서도 학습이 가능한 non-convex optimization에 기반한 boosting 알고리즘이다.

### 3. Stacking

Stacking은 모델들의 예측값을 다시 training 데이터로 활용해 모델을 만드는 앙상블 알고리즘이다. 이는 학습 이후 최종적으로 성능을 조금 더 향상시키기 위한 최후의 수단으로 여겨진다. 그 이유는 두번의 학습과정을 거치면서 overfitting에 굉장히 취약하기 때문이다.



stacking의 구조는 다음과 같다



학습모델 3가지를 만들었다고 하자

training set을 3가지 각 모델에 넣어 training data의 예측값을 얻고, 이 값과 training data의 target 값(실제값)을 가지고 최종 학습을 한다. 이후, test data를 가지고 최종 예측을 하게된다. 

*파이썬 코드의 경우, 파이선 머신러닝 완벽 가이드라는 책에서 배포한 코드가 구글링하면 쉽게 찾을 수 있고 모두 동일한 코드이기 때문에 생략한다. 다만 한가지 이해가 안되는 부분이 있다. CV기반의 stacking의 경우*

 *train set을 train-validation으로 나누어서 train으로 모델링 -> validation data로 예측 -> K-fold를 통해 예측값 K 개 형성 -> 이를 가지고 train_target 데이터와 함께 모델 적합 -> K fold과정에서 만든 모델로 test data 넣어서 예측한 값을 마지막으로 만든 모델에다 넣어서 예측 이라는 순서는 쉽게 이해가 간다.*

*다만 cv기반이 아닌 경우에 y_test를 모델 적합에 사용한다는데에서 모델에 최종 예측하고 싶은 값을 포함시킨다는게 과연 의미있는 것인지, 아니면 설명을 위해 그렇게 만든 것인지는 의문이 든다.*



이러한 과정을 한꺼번에 압축해서 만든 StackingCVRegressor 패키지가 존재한다

~~~python
from mlxtend.regressor import StackingCVRegressor

#5개의 모델 위에 xgboost를 하나 더 올려서 stacking 한다.

StackingCVRegressor(regressors=(gb, lightgb, xgboost, svr, lasso),
                                meta_regressor=xgboost,
                                use_features_in_secondary=True,
                                n_jobs=-1)
~~~







### 4. Blending

Blending은 stacking와 유사한 앙상블 알고리즘이다.

우선 training , validation, test 로 데이터를 나누고 training data만으로 모델에 적합시킨다. 이후, valiation set에 대한 예측값과 test set에 대한 예측값을 구한 뒤에 이 두가지 데이터를 가지고 새로이 모델에 적합시켜 최종 test set에 대한 예측값을 구하는 것이다. 



stacking 과 blending의 차이점은

1. stacking은 training set의 예측값을 활용 / blending은 validation set 의 예측값을 활용
2. stacking은 예측값만을 / blending은 예측값과 원래의 validation / test set을 모두 활용

~~~python
#fitting
model = xgb.XGBRegressor()
model.fit(x_train, y_train)
#prediction
prediction_val = model.predict(x_validation)
prediction_test = model.predict(x_test)

prediction_val=pd.DataFrame(prediction_val)
prediction_test=pd.DataFrame(prediction_test)

#fitting
model2 = ExtraTreesRegressor()
model2.fit(x_train,y_train)
#prediction
prediction_val2=model2.predict(x_validation)
prediction_test2=model2.predict(x_test)
prediction_val2=pd.DataFrame(prediction_val2)
prediction_test2=pd.DataFrame(prediction_test2)

#validation 예측값과 test 예측값을 활용해서 새로이 모델에 피팅
total_validation = pd.concat([x_validation, prediction_val,prediction_val2],axis=1)
total_test = pd.concat([x_test, prediction_test,prediction_test2],axis=1)

model3 = Ridge(alpha=10)
model3.fit(total_validation,y_validation)
model3.score(total_test,y_test)
~~~

blending 파이썬 코드는

https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ 

를 참조했습니다.
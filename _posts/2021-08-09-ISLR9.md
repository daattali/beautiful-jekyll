---
layout : post
title : ISLR
subtitle : statistical learning - 9장
date : 2021-08-09
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 9. Support Vector Machines

사람들은 흔히 the maximal margin classifier, the support vector classifier, the support vector machine을 통틀어서 support vector machine이라 부른다.

SVM은 'out of the box' classifier인데 특수한, 제한된 경우에서만 쓰이는 모형에서 더 일반적인 모형 순으로 나열할 때

maximal margin classifier => support vector classifier => support vector machines 순이다. 오른쪽으로 가면서 class가 선형모델로만 구분되는데서 비선형모델로도 구분이 가능한 것으로 발전해왔고, 또한 현재 SVM이 binary class 뿐만 아니라 다양한 class로도 구분되도록 모형이 발전해왔다.



#### 9.1 Maximal Margin Classifier



##### 9.1.1 What Is a Hyperplane?

p-dimensional space에서 hyperplane의 정의는 'flat affine subspace of dimension p-1'

p-dimension에서 hyperplane에 대한 수식

$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0
$

hyperplane은 p-dimension을 두개로 나눈다



##### 9.1.2 Classification Using a Separating Hyperplane 

classification의 방법으로 chap 4에서 logistic, LDA, chap 8에서 trees, bagging and boosting을 배웠다. 이제 새로운 접근법을 배운다. 여기서는 hyperplane을 나누는 방식의 접근을 배울 것이다.

separating hyperplane이 존재한다면 이를 활용해서 classifier를 만들수 있다. 예를들어 predictor를 대입한 값이 hyperplane보다 크면 1, 그렇지 않으면 -1을 부여해서 class 분류가 가능. 또한 predictor를 대입한 값 
$$
f(X^*)
$$
가 크면 이 데이터는 hyperplane에서 멀다는 것이므로 hyperplane에 의한 분류가 보다 확실하다고 말할 수 있다. 



##### 9.1.3 The Maximal Margin Classifier

hyperplane을 통해서 data를 완벽히 분류해낼 수 있다면, 무수히 많은 hyperplane이 존재할 것이다. 그렇다면 이러한 것들 중에서 어떤 hyperplane을 어떻게 선택해야 할까? 합리적인 기준은 무엇일까?

하나의 방법으로 제시되는 것은 Maximal Margin Hyperplane 이다. 이는 training observation에서 가장 먼 hyperplane이다. 

"the hyperplane that has the farthest minimal distance to the training observation"라고 표현되는데 이는 수직거리 중 가장 짧은 거리(Margin, minimal distance)들 중에서 가장 먼 거리를 갖는 hyperplane을 선택하는 것이다. 그러나 여기서도 overfitting의 문제가 나타난다. 특히 변수의 개수가 많아지는 경우 maximal margin classifier 는 오버피팅될 가능성이 높다. 

maximal margin이므로, hyperplane이 놓일 수 있는 가능한 공간에서 가장 '가운데'에 평면이 놓이게 될것이다.  이를 설명하는 것이 아래의 그림이다.

<img src='{{"/assets/img/islr9-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림 아래 설명부분에 보면 support vector라는 것이 나온다. 왜 support vector인가?

우선 이들은 p 차원 공간안에 있는 벡터이며, 이 벡터가 아주 살짝만 움직여도 maximal margin hyperplane이 바뀔 수 있기 때문에 maximal margin hyperplane을 'support' 한다고 하여 support vector라고 명명된다. 

또한 maximal margin hyperplane은 support vector와 같은 일부 데이터에만 의존하기 때문에 hyperplane에서 멀리 떨어진 데이터가 움직이는 것은 hyperplane에 영향을 안준다. 



##### 9.1.4 Construction of the Maximal Margin Classifier

Maximal margin hyperplane은 optimization의 문제이다. 즉

$
Maximize_{\beta_0, \beta_1,...,\beta_p } M
$

$
subject \ \ to \sum^p \beta_j ^2 = 1
$

$
y_i(\beta_0 + \beta_1 x_{i1} + ... \beta_{p}x_{ip}) \geq M
$

세번째 조건은 분류가 올바르게 되었는지(M은 양수)이며 두번째 조건은 hyperplane의 경우 
$$
\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0
$$
이므로 평면을 만들때 해당되는 조건은 아니다. 다만, 이 조건으로 인해서 
$$
y_i(\beta_0 + \beta_1 x_{i1} + ... \beta_{p}x_{ip})
$$
가 margin을 나타내는 수직거리가 되고 이 값이 M 보다 크게 되는 최대의 M 값을 구하는 것은 결국 

Maximal Margin Hyperplane을 의미하는 것이다.

##### 9.1.5 The Non-separable Case

hyperplane을 만들 수 없는 경우?

soft-margin이라 불리는 class를 ''거의'' 나누는 hyperplane을 만드는 것이다.  이를 다음 장에서 support vector classifier로 배운다. 



#### 9.2 Support Vector Classifiers



##### 9.2.1 Overview of the Support Vector Classifier

(1) hyperplane을 통해 데이터를 완전히 분류할 수 없는 경우 존재

(2) 완전히 분류되더라도 오버피팅의 위험이 있거나, 아주 작은 변동에도 모델 자체가 크게 변할 위험 존재



==> soft margin을 활용해서 robustness를 달성하며, 소수의 데이터를 제외한 대부분의 데이터에서 더 좋은 성능을 가지도록 만들 수 있다.



이러한  방법을 책에서는 support vector classifier 혹은 soft margin classifier 라고 부른다. 

일부의 데이터에 대한 분류 오류(wrong side of the margin, wrong side of the hyperplane)를 감소하고 서라도 대부분의 데이터에서 훨씬 더 좋은 분류 성능을 가진다면 이러한 모델을 선택하게 될 것.

##### 9.2.2 Details of the Support Vector Classifier

support vector classifier의 식은 아래와 같다.

$
Maximize_{\beta_0, \beta_1,...,\beta_p,\epsilon_1,...,\epsilon_n } M
$

$
subject \ \ to \sum^p \beta_j ^2 = 1
$

$
y_i(\beta_0 + \beta_1 x_{i1} + ... \beta_{p}x_{ip}) \geq M(1-\epsilon_i)
$

$
\epsilon_i \geq 0 \ , \ \ \sum \epsilon_i \leq C
$

여기서 epsilon의 값은 slack variable로 관측치가 margin이나 hyperplane 기준 반대 방향에 있도록 허용하는 정도를 나타내는 수치이다. 만약 모든 i 에 대해 epsilon_i 이 0이라면 모든 관측치들이 올바르게 분류되도록 hyperplane을 만들어야 한다. 그러나 epsilon_j 값이 0보다 크다면, j 관측치는 반드시 올바른 분류의 margin의 반대 방향에 위치하게 될 것이다.  나아가 epsilon_k 값이 1보다 크다면 k 관측치는 hyperplane의 반대방향에 위치하게 될 것이다. 

tuning parameter C의 역할은 epsilon이 용인하는 오류들의 정도와 개수를 결정해준다. C=0이라면 모든 epsilon이 0이라는 의미이므로 이는 9.1에서 배웠던 maximal margin hyperplane과 동일하다.  만일 C>0 이라면 C 값 이상의 점의 개수는 다른 hyperplane에 위치할 수 없다.(왜냐면 epsilon이 1 이상이어야 반대편 평면에 점이 위치하기 때문에) 

C가 크다는 말은 오류의 용인 범위가 넓다는 말이므로 margin은 넓어진다. Margin이 넓다는 것은 모형이 그만큼 robust하다는 것이고  bias - variance trade-off 관계를 통해서 볼 때,  분산이 작고 bias 가 커질 것이다.  반대로 C가 작아지면 용인 범위가 좁아지는 것으로 margin은 좁아질 것이다. 즉 대부분의 train 데이터가 모두 올바르게 분류되기 때문에 overfitting 의 문제를 피할 수 없다. C는 tuning parameter로 CV값을 통해 결정된다.

앞에 maximal margin hyperplane에서와 동일하게 support vector classifier 또한 일부의 데이터들로 인해서 hyperplane이 결정된다. 차이점은 support vector classifier는 일부의 오류 데이터를 용인한다는 것에 있기 때문에 margin에 놓여진 모든 데이터들(참이든 오류든)이 support vector로 여겨진다. 즉 오류데이터 또한 hyperplane 결정에 관여한다는 것이 maximal margin hyperplane과의 차이라 하겠다. 

이제 C 와 support vector 간의 관계를 보자. C 값이 커진다는 것은 support vector 들이 많아진다는 것이다. 이 말은 곧 hyperplane을 결정하는 데에 많은 데이터가 요구된다는 것이다. 



###### logistic , LDA, SV classifier

support vector classifier는 일부 데이터에 의해 hyperplane이 결정되는데 이는 logistifc regression과 유사하다. logistic regression 또한 boundary에서 먼 일부 데이터들은 boundary 결정에 크게 관여하지 않는다(insensitive).

이는 LDA와는 정반대의 학습방법이라 할 수 있다. LDA의 경우는 class에 대한 모든 관측치들의 평균과 분산을 고려해서 분류 기준을 만들기 때문이다.



#### 9.3 Support Vector Machines

non-linear decision boundary?? support vector machines은 비선형의 boundary를 자동으로 만들어낸다.



##### 9.3.1 Classification with Non-linear Decision

support vector classifier는 선형 boundary를 만들어낸다. 그러나 비선형의 boundary를 사용하고 싶다면?

이전 chapter에서 공부했듯, 우리는 quadratic , cubic term을 만들어내어 비선형의 데이터를 모형에 적합시켰다. 즉 support vector classifier의 hyperplane을 만들때 이차 혹은 삼차항이 들어간 식을 사용함으로써, 비선형 boundary를 만들어 낼 수 있다.  식은 아래와 같다.

feature term 

$
X_1, X_2, .... , X_p
$

에서

$
X_1, X_1^2, X_2, X_2^2, ... ,X_p,X_p^2
$

로 feature space를 확장시켜 최적화를 한다


$
Maximize_{\beta_0, \beta_{11}, \beta_{12},,...,\beta_{p1},\beta_{p2}, \epsilon_1,...,\epsilon_n } M
$

$
subject \ \ to \ \ y_i(\beta_0 + \sum_j^p \beta_{j1} x_{ij} +  \sum _j^p \beta_{j2}x_{ij}^2) \geq M(1-\epsilon_i)
$

$
\epsilon_i \geq 0 \ , \ \ \sum \epsilon_i \leq C \ , \ \ \sum_j^p\sum_k^2 \beta_{jk} ^2 = 1
$

변수의 차원을 높여서 모델에 추가하면 계산량이 많아진다. Support Vector Machine은 support vector classifier에서 사용된 feature space를 보다 확장시켜서 계산량을 적정 수준으로 맞춰준다.



##### 9.3.2 The Support Vector Machine

feature space를 어떻게 확장시켜줄 것인가? ==> Kernel을 사용해서.

support vector classifier의 해를 찾는 과정(여기서 해는 아마 계수값 beta인듯?)은 관측치들의 내적과 관련이 있다. 즉 식으로 이를 보이면, linear support vector classifier는

$
f(x) = \beta_0 + \sum_i ^n \alpha_i \langle x,x_i \rangle
$

여기서 x 는 new point, x_i 는 training point 인데 training data가 support vector인 경우에만 alpha 값이 non-zero이다. 즉 S를 support vector들의 집합이라고 할 때,

$
f(x )= \beta_0 + \sum _{i \in S}\alpha_i \langle x,x_i \rangle
$
 

이를 일반화 시켜서 kernel K 함수(kernel은 두 데이터들의 관계를 설명)를 통해 classifier를 만들 수 있다. 여러가지 커널에 대해 classifier의 식은 아래와 같다.

$
f(x )= \beta_0 + \sum _{i \in S}\alpha_i K( x,x_i)
$




(1) linear kernel (Pearson correlation을 통해 두 관측치 관계를 보는 것)

$
K(x_i, x_i') = \sum _j ^p x_{ij}x_{i'j'}
$

(2) polynomial kernel(degree of d) (linear kernel에 비해 훨씬 flexible boundary를 만들 수 있다)

$
K(x_i, x_i') = (1 + \sum _j ^p x_{ij}x_{i'j'})^d
$

     (2)와 같이 non-linear kernel을 사용해서 classifier를 만드는 것을 support vector machine이라고 한다. 만약 d=1이라면 SVM은 SV classifier와 동일한 것이다.



(3) radial kernel (Radial Basis Function , RBF) (Gaussian kernel이라고도 불림) (gamma 값이 클수록 non-linear)

$
K(x_i, x_i') = exp(-\gamma \sum _j ^p( x_{ij}-x_{i'j'})^2)
$

(3)의 원리는 다음과 같다. 특정 test 데이터가 만약 training obs와 유클리드 거리가 멀다면

$
\sum( x_{ij}-x_{i'j'})^2
$

의 크기는 커질것이고 결과적으로 kernel의 값은 작아질 것이다. 즉 거리가 멀리 떨어진 데이터들은 boundary(hyperplane)을 설명하는 데에 큰 역할을 하지 못할 것이고, kernel은 local behavior를 가지게 된다. 즉 training data 근처에 있는 test data 만이 정확한 분류가 될 것이다. 

앞서 말했지만 kernel을 사용하면 계산량이 줄어드는데, radial kernel의 경우 feature space가 implicit하고 infinite-dimensional 이므로 계산량이 절대적으로 감소한다.(?)

<img src='{{"/assets/img/islr9-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



##### 9.3.3 An Application to the Heart Disease Data

생략.



#### 9.4 SVMs with More than Two Classes

SVM은 개념적으로 두가지 이상의 분류에 적합하지 않다.  그럼에도 불구하고 class가 많을 경우 svm을 사용해서 많은 class를 분류하려는 시도가 있었다.



##### 9.4.1 One-Versus-One Classification

k개의 class가 존재할 때, 2개의 class를 뽑는 가능한 모든 조합을 찾아서 각각을 1 그리고 -1로 두고 SVM을 {k combination 2} 만큼 실행. 이 중에서 가장 빈도가 높은 class를 선택한다.

##### 9.4.2 One-Versus-All Classification

one , the others



#### 9.5 Relationship to Logistic Regression

hyperplane을 만들면서 오류에 대한 어느정도의 허용 범위를 지정한다든지, 혹은 커널을 사용해서 비선형적인 boundary를 만드는 등의 시도는 사실 매우 독창적인 시도라고 보일 수 있따. 그러나 SVM과 과거의 분류 방식들은 여전히 많은 부분에서 접점을 가진다. 

support vector classifier 부분에서 언급했던 제약조건하에서 최적화 식을 상기해보자. 그 식은 다음과 같이 쓸 수 있다.

$
minimize_{\beta_0, \beta_1, ..., \beta_p} \{\sum max[0, 1-y_if(x_i)] + \lambda \sum _j ^ p \beta_j^2 \}
$

우선 penalty 항을 보자.

여기서 lambda가 크다는 것은 그만큼 오류를 많이 허용한다는 뜻으로 support vector classifier 최적화 식에 나왔던 C의 값이 크다는 것과 동일하게 작용한다.(ridge에서의 penalty 값 => lambda가 크면 분산이 작아진다) 

이제 loss fuction을 보자

$
L(X, y , \beta) = \sum max[0, 1-y_if(x_i)]
$

이를 hinge loss라고 한다. 식에서도 알 수 있듯, support vector가 아닌 plane에 의해 올바르게 분류된 부분은 loss를 무조건 0으로 만들고, support vector 부분만이 loss를 통제할 수 있다. 즉 support vector에 의해 plane이 만들어지는 것이다.  logistic function의 loss function 또한 이와 비슷한데 차이는 완전한 0을 만들지 않는것에 있다.

<img src='{{"/assets/img/islr9-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

이러한 차이에도 불구하고, 0 혹은 0에 가까운 작은 값을 만드는 것은 boundary와 매우 멀리 떨어져 있는 값이므로 logistic과 SVMs은 매우 유사한 분류 결과를 가지고 온다. class들이 매우 잘 분류되어 있다면 SVM이 더 좋은 결과를, 반대로 class들이 꽤나 겹쳐 있는 경우에는 logistic regression이 더 좋은 결과를 가지고 올 것이다. 

앞에서도 말했듯, C, lambda, margin 들을 잘 선택하는 것은 결국 bias - variance 를 결정하는 데에 결정적인 역할을 하게 된다. 

SVMs 만이 kernel을 사용하는가? 그것은 아니다. logistic에서도 사용은 가능하다. 단 SVM 만큼은 아닌듯

또한 SVR도 있다. target 변수가 연속형인 경우 regression을 사용하는데, 여전히 그 원리는 loss를 최소화하는 계수값을 찾는 데에 있다. 단 loss가 바뀐다. 특정 margin 밖에 있는 잔차들의 값을 loss로 보고 이를 줄이고자 한다. 


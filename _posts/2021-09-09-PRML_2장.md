---
layout : post
title : PRML_study_chap_2
subtitle : 
date : 2021-09-08
#categories:
tags : [datascience, PRML2]
toc_sticky : true
use_math : true
comments: true
---




### 2.4 The Exponential Family

<br>

Exponential family 는 공통의 중요한 성질을 가지고 있다. 

기본적인 형태는 

$$
p(x|\eta) = h(x)g(\eta)exp\{ \eta^Tu(x)\}
$$

혹은

$$
p(x|\eta) = h(x)exp\{ \eta^Tu(x) - A(\eta)     \}
$$


$\eta$ : natural parameter

u(x) : some function of x 

g(x) : coefficient that the distribution is normalized

여기서는 지수족 분포의 예로 두가지를 설명한다. 인상 깊은 부분은 exponential family 안에 들어있는 u(x)를 잘 변형했을 때, bernoulli -> sigmoid function , multinomial -> softmax function 으로 표현된다는 것이다.(사후적인듯?)

1. Bernoulli distribution



<img src='{{"/assets/img/kernel-6.jpg"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



2. multinomial distribution


<img src='{{"/assets/img/kernel-7.jpg"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>




#### 2.4.1 Maximum likelihood and sufficient statistics

<br>

$$
\int p(x|\eta) d(x) = \int h(x)g(\eta)exp\{ \eta^Tu(x)\} d(x) = 1 
$$


이 성립하기 때문에 위 식을 활용해 $\eta $ 로 미분하면 다음과 같은 식을 얻을 수 있다

$$
- \nabla ln (g(\eta)) = E(u(x))
$$

이는 적률생성함수를 통해서도 증명이 가능한데, 아래는 서울대학교 이재용 교수님의 이론통계 1 의 강의 내용을 정리한 것이다.

<img src='{{"/assets/img/PRML11.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

또한 N 개의 data가 존재할 때 로그가능도함수를 구해 한번 미분하면 아래와 같은 식을 얻을 수 있다.

$$
- \nabla ln (g(\eta)) = \frac {1} {N} \sum (u(x))
$$

이 식은 지수족에서 충분통계량과 MLE 사이의 관계를 보여주며

아래와 같이 증명이 가능하다. 여기서 MLE의 일치성에 대한 개념을 생각해볼 수 있다.

<img src='{{"/assets/img/PRML12.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>


#### 2.4.2 Conjugate priors

지수족의 경우 conjugate prior가 존재하는데, 그 형태는 다음과 같다.

prior : $ p(\eta|\chi, \nu) = f(\chi, \nu)g(\eta)^{\nu}exp\{ \nu \eta^T \chi \} $

posterior : $ p(\eta|X, \chi, \nu) \propto g(\eta)^{\nu + N} exp( \eta^T \ \{\sum u(x_n) + \nu \chi \}) $

여기서 $\nu $ 의 값은 pseudo-observation in prior를 나타낸다. 

<br>


#### 2.4.3 Noninformative priors

어떤 prior를 선택해야 할까? prior가 zero에 가깝다면 posterior 또한 당연히 0에 가깝게 될것이다.

다르게 생각해보자

prior를 선택하기 어렵다면? 도메인 지식이 부족하다면?

posterior에 미치는 영향이 적은 prior를 선택하면 될 것. 이를 noninformative prior라고 한다. *"letting the data speak for themselves"* 

prior 를 상수로 둔다고 생각해보자.

1. 만약 parameter가 unbounded라면, 그 적분값이 infinite 일 것이다. 이를 improper prior 라고 한다. 그런데 improper prior라도 posterior는 잘 정의 된다!! 

2. parameter를 non-linear transformation 하는 경우. 
예를 들어 $\lambda $ 에 대해 $ p( \lambda) $ 가 상수로 정의되어 있고, $\lambda = \eta^2 $ 이라면, $p(\eta) = p(\lambda) 2\lambda \propto \lambda $ 가 성립. 즉 변환에 대해 prior 가 상수가 아니게 된다. 

###### examples of noninformative priors

1. location parameter mu 에 대해 translation invariance 를 가지는 분포 $f(x|\mu) = f(x-\mu)$ 

    (==> $\hat x = x + c $ , $\hat \mu = \mu + c $  이면  $ f(\hat x- \hat \mu) $ 가 되고 translation invariance property)

    에 대해서 prior를 이러한 translation invariance 성질을 반영하도록 만들면 prior가 동일한 간격의 interval에 동일한 probability mass를 주도록 만들면 된다. 이렇게 하기 위해서는 prior가 상수여야하며 $p(\mu - c) = p(\mu)$ 가 성립한다,

    더 구체적인 예로, 정규분포의 평균(location parameter)에 대한 사전분포로 정규분포 $N(\mu_0, \sigma_0^2)$ 가 있는데 사전분포의 분산을 매우 크게 하면 결국 사후분포의 평균은 데이터들에 의해 만들어진 표본평균(ML 추정량)이 되기 때문에 사전분포의 영향이 줄어든다

    이 뜻은 사전분포의 분산을 매우 크게 만들어 p = constant 인 분포가 되었을 때 사전분포의 영향을 거의 받지 않는 사후분포를 만들어 낼 수 있으므로 noninformative prior 라고 할 수 있다.

2. scale parameter sigma 에 대해 scale invariance 를 가지는 분포 $f(x|\sigma) = f(x/\sigma)/\sigma$

    (==> $\hat x = cx $ , $\hat \sigma = c\sigma $ 이면 $f(\hat x|\hat \sigma) = f(\hat x/ \hat \sigma)/ \hat \sigma$ 이 되고 scale invariance property)

    에 대해서 prior를 이러한 scale invariance 성질을 반영하도록 만들면 $p(\sigma) = p(\frac{1}{c} \sigma) \frac{1}{c} \propto \frac {1}{\sigma} $를 만족하는 사전분포 p는 sigma에 대해 적분하면 divergent => improper prior

    변수 변환에 의해 $p(ln (\sigma))= constant $ 이면 동일한 inverval 간격에 대해 같은 probability mass 를 가진다. 

    더 구체적인 예로 정규분포의 inverse varaince 에 대해 사전분포를 감마분포로 설정할 수 있다. 감마분포의 모수 a,b에 대해 그 값이 모두 0에 가까워질수록 사전분포는 improper &  감마 사후분포의 모수 값이 오로지 데이터에 의해 결정되게 된다. 
    
<img src='{{"/assets/img/kernel-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>
    
<img src='{{"/assets/img/kernel-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>
    



<br>

### 2.5 Nonparametric Methods

parametric method는 가정한 모델이 잘못될 경우, 예측의 문제에 있어서 매우 안좋은 결과를 가져올 것이다. 모형 가정이 거의 없는 non-parametric methods 로 히스토그램을 활용한 분포 추정에 대해 우선 살펴보자. 

(1) partition X into distinct bins of width 

$
\Delta_i
$

(2) count the numbr 

$
n_i
$
in bin i th

(3) total number N

(4) normalized probability values for each bin

$
p_i = \frac {n_i}{N \Delta_i} , \ \int p(x)dx= 1
$

<img src='{{"/assets/img/kernel-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>

그림에서 보듯 델타의 값이 커질 수록 smoothing의 효과가 크다. 그러나, 너무 커지면 분포를 제대로 잡아내지 못할 것. 

histogram의 장점 :

(1) 히스토그램이 만들어지면 원래의 데이터만큼 세세한 사항들이 소실되어 버림. 그러나 데이터가 매우 크다면 이를 직관적으로 보여주는 좋은 그림이 됨

(2)  데이터가 지속적으로 축적되는 경우에도 히스토그램을 업데이트 하기 쉽다.

histogram의 단점 :

(1)  히스토그램으로 변형시 데이터의 연속성이 사라질 수 있다. bin 경계의 불연속성

(2)  변수(variable, predictor)가 많은 경우, 구간들이 많아지면서 데이터의 차원이 매우 커진다.


이런 장단점을 고려해 볼 때 histogram은 

특정 데이터와 주변 데이터를 함께 고려하려는 시도(히스토그램에서의 막대기)를 통해 보고싶은 부분의 확률 밀도를 대략적으로 추측할 수 있다. 그리고 bin의 width를 조절함으로써 smoothing parameter에 대한 통제를 할 수 있다. 




이제 히스토그램보다 scaling with dimensionality가 더 좋은 아래의 두가지 방법을 살펴보자.

<br>


#### 2.5.1 

(1) unknown probability density : p(x) : 추정 대상

(2) small region : R

(3)
$
P = \int _R p(x)dx
$

(4) total data : N

(5) total number of points that lie inside R :  K

$
K \sim B(N,P)
$

(＊) 여기서 N이 값이 매우 크다면 K 데이터는 대부분 평균값인 NP에 위치할 것

즉 
$
K \simeq NP
$

(＊＊) 추가적인 가정으로 R 이 매우 작다고 가정하자. 이 경우 R의 부피를 V 라고 하면, (3)으로부터,

$
P \simeq p(x)V
$

(＊＊＊)두 가정을 종합하면 

$
p(x) \simeq \frac {K}{NV}
$


위 식에 이제 주목해보자. N값은 이미 고정되어있고, V값을 가정하면 K의 값을 찾아야 p(x)를 찾을 수 있다. 이 때 K의 값을 찾는 방법으로 kernel function을 사용한다. (만약 K가 고정되어 있다면? K가 고정된 채로 V를 찾는 것 : K-Nearest Neighbour)


###### 1

kernel function - (*Parzen window*)

$
k(u) = \begin{cases} 1 & |u_i|\leq 1/2 \\ 0 & otherwise \end{cases}
$

$
K = \sum k(\frac {x-x_n} {h})
$

이를 다시 바꿔서 표현하면 (＊＊＊) 식을 이용해서


kernel density estimator(kernel density model) 
$
p(x) = \frac {1}{N} \sum \frac{1}{h^d} k(\frac {x-x_n} {h})
$

h^d 는 V 대신 쓴 것.(이는 이미 결정되어있는 것)

kernel function k(u)는 centered on orign 인데 위에서 만든 kernel density model은 centered on x_n inside the cube side h

<br>
<br>


이렇게 커널함수를 사용해서 p(x)를 추론하면 histogram에서 발생하는 문제와 같이 인위적인 불연속점이 존재한다. 

###### 2
kernel function - *gaussian kernel*

smoother than parzen window
h는 표준편차로 smoothing parameter 역할.


kernel density model(Gaussian)

$$
p(x) = \frac{1}{N} \sum \frac{1}{\sqrt{2 \pi h^2}} exp{-\frac {||x-x_n||^2}{2h^2}}
$$

즉 가우시안 커널을 통과시킨 data point x_n 에 대한 합을 kernel function으로 잡는 것.  

아래 그림은 smoothing parameter h에 따른 분포의 변화를 나타낸다.

<img src='{{"/assets/img/kernel-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

###### 3

kernel function의 성질은 

$
k(u) \geq 0 \ , \ \int k(u) du =1
$



#### 2.5.2 Nearest-neighbour methods

kernel method와는 다르게 

$
p(x) \simeq \frac {K}{NV}
$

에서 K 값이 고정되어있고 적절한 V값을 조절하면서 density funciton을 추정하는 방식이 nearest neighbour method이다.

일례로, 만일 데이터 밀도가 높은 부분에 큰 V값을 준다면 충분한 데이터 구조를 뽑아내지 못할 것이다. 반대로 데이터 밀도가 낮은 곳에 작은 V값을 준다면 오히려 noisy estimates를 얻게 될 것이다. 

위에 식 p(x)를 sphere에 대한 밀도함수라고 하고, 사전에 정한 K 개의 데이터를 포함하도록 sphere의 반지름을 계속 키워나간다고 하자. 


<img src='{{"/assets/img/kernel-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

아래 그림은 K 값의 변화에 따른 추정 밀도함수(엄밀히 말하면 전 영역을 적분하면 1이 아니므로 밀도함수는 아니다) 를 나타내는데, kernel method와 달리 K가 smoothing parameter 역할을 하고 있다.


KNN은 classification에서도 쓰이는데, 이 책에서는 misclassification을 줄이기 위해 베이지안 관점을 도입한다. posterior가 가장 큰 class에다가 분류하고 싶은 데이터의 class를 정해주는 것이 오류를 최소화하는 것일 텐데, 수식은 아래와 같다.

N_k : class k 에 속한 데이터 개수, C_k : k번째 class , N : 전체 데이터 개수, K_k : 체적 V에 속한 데이터 개수

$$
p(x|C_k) = \frac {K_k}{N_k V}
$$

$$
p(x) = \frac {K}{NV}
$$

$$
p(C_k) = \frac {N_k}{N}
$$


$$
p(C_k | x) = \frac {p(x|C_k)p(C_k)}{p(x)} = \frac{K_k}{K}
$$



<br>

또한, N 값이 매우 커지면, K=1 일 때 KNN의 error rate 은 bayes error rate 의 두배를 넘지 않는다.

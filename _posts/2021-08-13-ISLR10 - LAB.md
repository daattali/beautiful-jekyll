---
layout : post
title : ISLR
subtitle : statistical learning - 10장 - LAB
date : 2021-08-13
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 10장 코드 정리

~~~R
## Principal Components Analysis

#row name
states= row.names(USArrests)
states

#columns name
names(USArrests)

apply(USArrests, 2, mean) #여기서 두번째 인자는 1일 때 행, 2일 때 열 
"""
  Murder  Assault UrbanPop     Rape 
   7.788  170.760   65.540   21.232 
"""

pr.out = prcomp(USArrests, scale=TRUE) #sclae = T 를 통해서 PCA 이전에 정규화를 해줌
pr.out$center #PCA 이전의 평균과 표준편차가 제시된다.
pr.out$scale
pr.out$x # 각 데이터의 pc score 값이 나온다

pr.out$rotation #loading 값을 제공.
"""
                PC1        PC2        PC3         PC4
Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
Rape     -0.5434321 -0.1673186  0.8177779  0.08902432
"""

# 두개의 PC 값으로 biplot 그리기
biplot(pr.out, sclae=0) #sclae=0를 주면 화살표가 loading 을 나타냄

pr.out$sdev #loading의 표준편차
pr.var = pr.out$sdev^2
pve = pr.var / sum(pr.var)
pve  # 0.62006039 0.24744129 0.08914080 0.04335752
#의미는 첫번째 PC가 전체 분산의 62%를 설명

plot(pve, xlab = "PC", ylab = 'proportion of variance', ylim=c(0,1), type='b')
plot(cumsum(pve), xlab='PC', ylab='culmulative proportion of variance', ylim=c(0,1), type='b')

#이렇게 일일이 하지 않더라도 summary를 통해 수치로 체크하고 plot함수를 바로 쓸 수 있다.
summary(pr.out)
plot(pr.out) #pc 들의 분산 비율을 알 수 있게 해줌

## Clustering
set.seed(2) #nstart에서 random 값을 주기 때문에 seed 값을 고정해야 reproducible 하다.
x= matrix(rnorm(50*2), ncol=2)
x[1:25,1] = x[1:25,1] +3
x[1:25,2] = x[1:25,2] -4
km.out = kmeans(x, 2,nstart=20) #K=2, nstart 값은 1보다 크면 처음 임의로 cluster를 random으로 할당
km.out$cluster #50개의 데이터셋에 대해 어떻게 클러스터 되었는지 나온다
km.out$tot.withinss # within cluster sum of squre : K-means clustering 에서 줄이고자 하는 것.
plot(x, col=(km.out$cluster+1), main="k-means with k=2",pch=20, cex=2)


## Hierarchical Clustering
x=scale(x) # 표준화 해주기.
hc.complete = hclust(dist(x), method = 'complete') # dist() 함수는 matrix 원소간 거리를 잰다.
hc.average = hclust(dist(x), method = 'average')
hc.single = hclust(dist(x), method = 'single')

plot(hc.average, main="complete linkage", cex= 0.9)
cutree(hc.average,2) #cluster label을 제시하는데 몇개로 컷 할건지지

#dissimilarity 의 또다른 측도인 correlation - based distance
X = matrix(rnorm(30*3), ncol=3)
dd = as.dist(1-cor(t(x)))
plot(hclust(dd, method='complete'))


## clustering은 PCA 이후에 할 수도 있다. 오히려 이게 결과가 더 좋을 때도 있다. 
#즉 PCA는 noise를 제거하는 단계 처럼

hc.out = hclust(dist(pr.out$x[,1:3]), method='complete')
plot(hc.out)

~~~


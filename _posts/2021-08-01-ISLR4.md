---
layout : post
title : ISLR
subtitle : statistical learning - 4장
date : 2021-08-01
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 4. Classification



#### 4.1 An Overview of Classification

분류의 문제 또한 각 카테고리에 속할 확률값에 의존하는 것이기 때문에 회귀문제와 결코 다르지 않다.



#### 4.2 Why not Linear Regression?

target 변수가 범주형 변수일 때 linear regression을 사용하면 coding에 따라 결과값이 크게 달라질 것이며 각 범주간 차이를 올바르게 인코딩하기도 어렵다.

또한 binary variable 에 대해서 회귀식을 작성하면 prediction 값이 반드시 [0,1] 사이에 들어가지도 않는다. Y의 class가 세가지 이상인 경우에는 선형 회귀를 사용하기 위한 Y 값의 인코딩 또한 쉽지 않다.



#### 4.3 Logistic Regression

response variable이 binary 인 경우에 대해 사용 가능하다. 조건부 확률로 class를 결정하게 되는데, 그 기준은 연구자가 임의로 정할 수 있다. 즉 확률 0.1를 기준으로 0 or 1로 분류되도록 식을 만들 수 있다.

##### 4.3.1 The Logistic Model

<그림1>



P(X) 는 조건부확률
$$
P(X) = \frac {e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
$$
이를 다시 바꾸면
$$
\frac{P(X)}{1-P(X)} = e^{\beta_0 + \beta_1X}
$$
또 바꾸면
$$
log{\frac{P(X)}{1-P(X)}} = {\beta_0 + \beta_1X}
$$
여기서 좌변을 log-odds or logit. 즉 로지스틱 회귀에서 로짓은 X와 linear 한 관계를 가진다. 

(※ odds는 경마경기에서 배당을 할 때 주로 사용되는데 쉽게 말해서 "성공확률 / 실패확률" 을 의미한다)



##### 4.3.2 Estimating the Regression Coefficients

LSE 보다는 MME로 추정한 회귀계수가 훨신 더 좋은 통계적 성질을 가지게 된다.

비선형 모델에서도 MME를 통한 추정이 가능하다

로지스틱 회귀의 경우
$$
l(\beta_0, \beta_1) = \prod_{i : y_i=1} P(X_i) \prod _ {j:y_j=0} (1-P(X_{j}))
$$
적합 결과 계수 추정량과 standard error, Z-statistic 등을 얻을 수 있는데 여기서 Z-statistic이란 linear 모형에서의 t-statistic과 같이 계수의 유의성 검정 통계량을 의미한다.



##### 4.3.3 Making Predictions

예측을 위해 변수 값을 집어넣고 나온 결과값(확률)에 따라 class를 결정한다. 또한 로지스틱회귀를 모형을 만들 때 사용되는 predictor가 반드시 quantitative 인 것은 아니다. qualitative predictor를 사용해서 로지스틱 회귀를 만들 수 있다. 예를 들어 Y 값이 A, B  X 값이 C,D 라면 C or D 에 따른 A, B 확률을 로지스틱 회귀로 구할 수 있는 것이다. 



##### 4.3.4 Multiple Logtistic Regression

$$
log{\frac{P(X)}{1-P(X)}} = {\beta_0 + \beta_1X_1 + ... + \beta_pX_p}
$$



선형회귀와 마찬가지로 다중회귀에서도 변수간 영향(correlation)을 고려해야한다. 책에서 드는 예시를 살펴보자

<그림>

우선 그림(figure 4.3 왼쪽) 과 같이 balance를 함께 고려한 로지스틱 회귀식은 orange line이 blue line보다 아래에 있다. 그러나 balance를 고려하지 않았을 때는 orange line이 보다 위에 있는 것을 볼 수 있다. 이는 동일한 변수에 대한 계수값의 부호가 바뀐 것이라 할 수 있다. 

만약 이 데이터를 보고 카드사에서 카드를 학생들에게 발급해줄지 말지를 결정한다고 하자. 학생인지 여부에 따른 평균값만을 본다면 default rate이 훨씬 높지만 만약 balance 가 높은 사람에 대해서는 학생일 때 오히려 default rate이 떨어진다는 사실을  파악해야만 올바른 전략을 제시할 수 있을 것이다.



##### 4.3.5 Logistic Regression for >2 Response Classes

multiple class classification 에서는 LDA를 더 많이 쓴다. 다만 R 패키지를 통해서 multiple class classification을 아래 식 처럼 기존 로지스틱 회귀로 접근 가능하다
$$
P(Y=1|X)
$$

$$
P(Y=2|X)
$$

$$
P(Y=3|X) = 1-P(Y=1|X) -  P(Y=2|X)
$$



#### 4.4 Linear Discriminant Analysis

(1) response variable의 class 가 잘 나뉘어져 있는 경우에 logistic regression model이 불안정하다.

(2) 데이터의 개수가 적고 response variable의 class 별로 predictor가 정규분포를 따르는 경우, LDA가 더 안정적이다.

(3) response variable의 class가 여러개인 경우 LDA가 훨씬 더 많이 쓰인다.



##### 4.4.1 Using Bayes' Theorem for Classification

$$
\pi_k : prior : k 번째\ class에\ 속할 \ 확률
$$

$$
f_k(X) = P(X=x|Y=k) : likelihood
$$

$$
P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum _l \pi_l f_l(x)} : Posterior
$$

prior를 추정하는 것은 표본을 통해 쉽게 가능하다. 그러나 likelihood 분포를 추정하기란 쉽지 않다.이는 연구자의 가정에 의한 것이기 때문이다. 그러나 chap 2 에서 본 대로 bayes classifier는 error rate이 낮기 때문에 likelihood 분포만 잘 추정한다면 좋은 classifier를 만들 수 있을 것이다.



##### 4.4.2 Linear Discriminant Analysis for p=1

여기서는 정규모형을 가정한 상태로 사후분포를 구하고 있다.  즉 LDA는 기본적으로 베이즈 추론에 기반한 방식. but bayes classifier는 가능도분포를 자체를 모두 가정하지만 LDA는 기본적인 모형만 가정하고 모수는 데이터에서 가져오며 특히 class 별 등분산 가정을 도입. 그리고 bayes classifier와 LDA 성능은 거의 유사하다.

why is it linear? discriminant functions(사후분포) are linear function of x

test error rate good!



##### 4.4.3 Linear Discriminant Analysis for p>1

분류에서 train error rate이 낮을 때?

만일 데이터 개수 자체가 적은 경우 오버피팅의 위험이 존재

또한 분류되는 대상의 데이터 개수 자체가 불균등한 경우 error rate이 평소 보다 낮을 수 있으므로 해석에 있어 이를 반드시 고려할 것.(이는 sensitivity와 specificity와도 관련이 있다)

sensitivity(true positive) : refers to the proportion of those who have the condition that received a positive result on this test. (증상이 있다고 판명난 사람이 실제로 환자인지) (여기서 판정은 분류를 의미)

specificity(true negative) : refers to the proportion of those who do not have the condition that received a negative result on this test. ( 정상이라고 판명난 사람이 실제로 정상인인지) (여기서 판정은 분류를 의미)

sensitivity가 낮은 경우 분류의 기준이 되는 확률을 낮춤으로서 sensitivity를 높일 수 있다. 그러나 error rate 은 오히려 높아질 수 있다. 여기서는 분석의 목적이 더욱 중요해진다. error rate을 희생해서라도 sensitivity를 높이는 것이 중요한 때에는 분류의 기준이 되는 확률을 낮추는 것이 더욱 유리할 것이다.



ROC curve : 모든 가능한 thresholds에 대해 두 종류의 error를 보여준다.

여기서 두 종류의 error 란 (true postivite / false positive) => 즉 sensitivity / 1- specificity => 분류기의 성능을 비교 가능

AUC : area under the curve : 분류기의 성능을 보여줌. 영역기 클수록 좋은 성능

<그림>

<그림>



##### 4.4.4 Quadratic Discriminant Analysis(QDA)

LDA와 유사한데, 각 class 별 등분산 가정 X.

등분산 가정이 없다면 QDA 과정에서 분산값을 모두 추정해야 하므로 그만큼 분산값이 큰 flexible한 모형이 만들어진다. LDA를 사용하면 분산이 낮고(overfitting 방지) 예측력이 좋을 수도 있다. 그러나 LDA에서 등분산으로 가정한 그 값이 매우 나쁜 값이라면 LDA 모델 자체의 bias가 커질 수 있다.

즉 LDA는 데이터 개수 자체가 적어서 분산을 줄이는 것이 모델 추정에 중요한 경우, QDA 보다 매력적인 분류 방식이다.

<그림>

<그림>



#### 4.5 A Comparison of Classification Methods

4장에서는 Logistic Regression, LDA, QDA, KNN(이건 2장)에 대해서 배웠다.

Logistic 이나 LDA나 모두 parametric 방법으로서 fitting 과정에서 Logistic 은 MME를 통해, LDA는 표본을 가지고 모수를 추정해 베이즈 정리를 활용하는 것에만 차이가 있다. LDA에서 가정하는 likelihood가 맞다면 아마 Logistic 보다는 LDA가 훨씬 뛰어난 성능을 보일 것이다. 
$$
logistic : log(\frac {p_1}{1-p_1}) = \beta_0 + \beta_1x
$$

$$
LDA : log(\frac {p_1(x)}{1-p_1(x)}) = c_0 + c_1x
$$





이와 다르게 KNN은 non-parametric한 접근방식이다. 이 방법은 decision boundary에 대한 어떠한 가정도 없기 때문에 실제로 class들의 decision boundary가 비선형일 때 보다 좋은 성능을 가지고 있을 것이다. 다만 non-parametric의 특성상 어떤 변수들이 significant 한 것인지 알기는 어렵다.



QDA는 KNN와 LDA 방식의 절충안이라고 생각하면 이해하기 쉬울 것이다. LDA와는 다르게 quadratic decision boundary를 가지고 있어 보다 flexible 하며, KNN에 비해 decision boundary에 대한 가정이 있기 때문에 데이터 개수가 적을 때에 보다 유용할 것이다.

교재에 6가지 상황에 대한 scenario가 나온다

scenario를 볼 때, true decision boundary가 linear이면 LDA나 logistic이 좋고, non-linear이거나 이것보다 더 복잡한 decision boundary를 가질 경우에는 QDA나 KNN 방식이 좋을 것이다. 그러나 KNN의 경우 K가 너무 작아서 smoothness가 떨어지는 경우 오히려 성능 저하기 있을 수 있기 때문에 올바른 K를 설정하는 것이 중요하다. 

또한 로지스틱 모형에서 quadratic term을 줄 수도 있지만 variance 가 늘어난 것 보다 훨씬 많은 bias가 감소하는지(즉, 이득이 있는지) 확인하자. 


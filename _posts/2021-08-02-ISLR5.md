---
layout : post
title : ISLR
subtitle : statistical learning - 5장
date : 2021-08-02
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 5. Resampling Methods 

resampling 방법으로는 크게 두가지 

(1) Cross-Validation for estimating the test error ==> model assessment, model selection

(2) Bootstraping ==> providing a measure of accuracy of a parameter estimate



#### 5.1 Cross-Validation

6장에서 training error rate 을 조정해서 test error rate을 추정하기 위한 방법들을 배우지만 여기 5장에서는 train 데이터를 분리하는 방식으로 test error rate을 찾는 방법들을 공부할 것이다.  

##### 5.1.1 The validation Set Approach

validation set approach 장점 : random split을 통해 다양한 경우에서의 validation set mse를 볼 수 있다. 각 split 마다 동일한 추이가 나온다면 유용할 것

validation set approach 단점 : split마다 변동성이 매우 심할 수도 있다. 각 split에 속한 일부 변수 때문에 valid error rate 이 과대 추정될 수도 있다.

##### 5.1.2 Leave-one-out Cross-Validation(LOOCV)

용어 그대로 하나의 데이터를 제외하고 모두 training set으로 적합시킨다.

<img src='{{"/assets/img/islr5-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

$
MSE_i = (y_i -\hat y_i)^2
$

$
CV_{(n)} = \frac {1}{n} \sum_i MSE_i
$

장점 : 

(1) 하나를 제외하고 모든 데이터를 적합시켰기 때문에 valid MSE가 far less biased.

(2) 데이터를 split 하는 데서 no randomness => 결과과 균일 / 동일

<img src='{{"/assets/img/islr5-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

단점 : n(데이터 개수) 가 매우 큰 경우에 반복을 계속 해야 한다

그러나 이를 극복하기위한 계산법이 존재.  least square linear or polynomial regression에서 

$
CV_{(n)} = \frac{1}{n} \sum_i (\frac {y_i - \hat y_{(i)}}{1-h_i})^2
$

h_i 는 i 번째 데이터의 leverage 값으로 이 값이 클 수록 CV 값이 커질 것이다. 



##### 5.1.3 K-Fold Cross-Validation

$
CV_{(K)} = \frac {1}{K} \sum_i MSE_i
$

LOOCV 보다 더 좋은 장점은

(1) 계산 비용 감소

(2) bias - variance trade - off 

결국 CV를 통해 알고싶은 것은 모형이 test set에 대해서도 어느정도의 일반성을 갖느냐 인데, LOOCV나 K-Fold 나 거의 유사하게 test mse를 가장 작게 해주는 구간이 비슷하다. 그럼 계산비용이 작은 CV가 더 좋지 않을까?

<img src='{{"/assets/img/islr5-6.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

##### 5.1.4 Bias-Variance Trade-Off for K-Fold

앞 5.1.3에서 살짝 나왔단 bias - variance trade off 이야기를 좀 더 이어나가 보자.

계산 비용을 제외하고서라도 K-Fold 방법이 LOOCV 보다 test error rate을 더 잘 추정해준다.

즉 K-Fold 는 LOOCV와 validation set approach의 중간점이라 할 수 있다.  validation set approach는 적합에 사용되는 표본이 부족해서 test error rate을 과추정하게 될 수 있고, 반대로 LOOCV는 1개를 제외하고 모두 적합에 이용되기 때문에 unbiased의 장점이 있는데 이를 혼합시켜서 bias를 살짝 가지면서도 분산을 줄이는 것이 K-Fold 가 되는 것이다. 

그럼 LOOCV는 variance가 매우 큰 것인가? 그러할 수 있다. 왜냐하면 1개를 제외하고 모두 적합하는 과정을 n 번 반복하고 이를 평균 내어 error를 추정하기 때문에 각 error 간에 매우 큰 공분산이 생겨버리기 때문이다.  

##### 5.1.5 Cross-Validation on Classification Problems 

error의 척도만 다를 뿐 같다

분류의 문제에서 LOOCV error rate은
$
CV_{(n)} = \frac {1}{n} \sum_i I(y_i \neq \hat y_i)
$


#### 5.2 The Boostrap

이전 글에서 boosstrap에 대해 다뤘기 때문에, 교재에 나온 그림 두개 첨부

<img src='{{"/assets/img/islr5-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<img src='{{"/assets/img/islr5-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

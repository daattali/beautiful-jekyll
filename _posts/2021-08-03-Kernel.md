---
layout : post
title : PRML
subtitle : Kernel
date : 2021-08-02
#categories:
tags : [datascience, datamining, machinelearning, PRML, Pettern Recognition and Machine Learning]
toc_sticky : true
use_math : true
comments: true
---

Kernel 에 관해 언제 한번 전체적으로 정리해보고자 했는데, PRML에 나오는 kernel에 관련한 부분들을 종합적으로 정리해보고자 한다. 큰 목차들은 모두 PRML을 참고해서 하도록 하겠다.

### 2.5 Nonparametric Methods

parametric method는 가정한 모델이 잘못될 경우, 예측의 문제에 있어서 매우 안좋은 결과를 가져올 것이다. 모형 가정이 거의 없는 non-parametric methods 로 히스토그램을 활용한 분포 추정에 대해 우선 살펴보자. 

(1) partition X into distinct bins of width 

$
\Delta_i
$

(2) count the numbr 

$
n_i
$
in bin i th

(3) total number N

(4) normalized probability values for each bin

$
p_i = \frac {n_i}{N \Delta_i} , \ \int p(x)dx= 1
$

<img src='{{"/assets/img/kernel-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림에서 보듯 델타의 값이 커질 수록 smoothing의 효과가 크다. 너무 커지면 분포를 제대로 잡아내지 못할 것. 

histogram의 장점 :

(1) 히스토그램이 만들어지면 원래의 데이터만큼 세세한 사항들이 소실되어 버림. 그러나 데이터가 매우 크다면 이를 직관적으로 보여주는 좋은 그림이 됨

(2)  시계열 데이터의 경우 적용이 쉽다(?)

histogram의 단점 :

(1)  히스토그램으로 변형시 데이터의 연속성이 사라질 수 있다. bin 경계의 불연속성

(2)  변수(variable, predictor)가 많은 경우, 구간들이 많아지면서 데이터의 차원이 매우 커진다.


이런 장 단점을 고려해 볼 때 histogram은 

특정 데이터의 주변 데이터를 함께 고려하려는 시도, 그리고 smoothing parameter에 대한 통제

에 대한 insight를 주고 있다.



#### 2.5.1 Kernel Density Estimator

(1) unknown probability density : p(x) : 추정 대상

(2) small region : R

(3)
$
P = \int _R p(x)dx
$

(4) total data : N

(5) total number of points that lie inside R :  K

$
K \sim B(N,P)
$

(＊) 여기서 N이 값이 매우 크다면 K 데이터는 대부분 평균값인 NP에 위치할 것

즉 
$
K \simeq NP
$

(＊＊) 추가적인 가정으로 R 이 매우 작다고 가정하자. 이 경우 R의 부피를 V 라고 하면, (3)으로부터,

$
P \simeq p(x)V
$

(＊＊＊)두 가정을 종합하면 

$
p(x) \simeq \frac {K}{NV}
$


위 식에 이제 주목해보자. N값은 이미 고정되어있고, V값을 가정하면 K의 값을 찾아야 p(x)를 찾을 수 있다. 이 때 K의 값을 찾는 방법으로 kernel function을 사용한다. 

kernel function

$
k(u) = \begin{cases} 1 & |u_i|\leq 1/2 \\ 0 & otherwise \end{cases}
$

$
K = \sum k(\frac {x-x_n} {h})
$

이를 다시 바꿔서 표현하면 (＊＊＊) 식을 이용해서

$
kernel \ density \ estimator :: p(x) = \frac {1}{N} \sum \frac{1}{h^d} k(\frac {x-x_n} {h})
$

h^d 는 V 대신 쓴 것.



이렇게 커널함수를 사용해서 p(x)를 추론하면 histogram에서 발생하는 문제와 같이 인위적인 불연속점이 존재한다. 

smoother kernel function으로서는 가우시안 커널 function이 있다.  h는 표준편차로 smoothing parameter로 쓰인다.

kernel function의 성질은 

$
k(u) \geq 0 \ , \ \int k(u) du =1
$


<img src='{{"/assets/img/kernel-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

6장의 내용들은 이후에 추가적으로 적도록 하겠다.

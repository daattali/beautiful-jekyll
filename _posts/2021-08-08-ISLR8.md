---
layout : post
title : ISLR
subtitle : statistical learning - 8장
date : 2021-08-08
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 8. Tree-Based Methods

tree-based method는 간단하게 말해서 preditor space(X 공간)를 층화 / 구분 짓고 각 영역의 데이터들에 대한 평균 혹은 중간값으로 예측을 해내는 구조를 말한다. 

트리 기반의 방식들은 simple 하며 interpretation이 뛰어나나, 예측에 있어 다른 방법들에 비해 열등하다. 따라서 이를 보완하고자 bagging,boosting, Random Forest 등이 나왔는데 이 방식들은 트리를 여러개 만들어서 이들을 결합해 예측력을 높이려는 시도들이다. 즉 기존의 트리 기반 방식에 비해 모델의 해석력은 떨어지나, 예측력을 높인 것이다.



#### 8.1 The Basis of Decision Trees



##### 8.1.1 Regression Trees

upside-down 형식으로 계속해서 가지를 쳐 나가며 분류하는데 각 가지에 의해 구분된 공간들을 terminal nodes 혹은 leave 라고 한다. 또한 predictor space가 나눠지는 점들을 internal nodes 라고 한다. Node를 이어주는 부분들을 branch라고 한다.

<그림>



 upside down 형식의 tree 구조를 해석해보자

years 가 target 변수를 해석하는 가장 큰 요인이 됨을 그림에서 알 수 있다. 또한 hits의 경우 years가 낮은 집단에 대해서는 target 변수를 결정하는 큰 요인이 되지는 못하지만, years가 높은 경우에는 hits가 target변수를 결정하는 큰 기준이 된다. 그림에서 나타듯, tree based method는 직관적이고 모델을 해석하기 쉽다.



###### building regression tree

1. predictor space를 겹치지 않게 나눈다. R_j
2. 각 영역 R_j 에 속한 관측치들은 모두 같은 값으로 예측하게 된다



어떻게 겹치지 않게 predictor space를 나눌 수 있을까?

여기서도 결국 RSS를 최소화 하는 방식으로 space를 나누고자 한다. 즉,
$$
\sum_j \sum _ i (y_i - \hat y_{R_j})^2
$$
를 최소화하는 box
$$
R_1, R_2, ....,R_J
$$
로 영역을 나누면 된다. 

그러나 현실적으로 모든 box들을 고려하는 것은 계산상으로 문제가 있기 때문에 top down 방식으로 각 step마다 최고의 성능(가장 rss를 작게하는)의 split을 고른다. 이를 recursive binary splitting이라 한다(변수 선택의 forward selection과 유사). **특이한 점이 있다면, 가지수를 무한대로 늘리는 것이 아니라 각 변수마다 여러가지 branch 중 하나만 선택해서 terminal nodes를 만든다는 것에 있다.** 



###### tree pruning

tree의 split이 너무 많게 되면 복잡해지고 분산이 커져 예측력이 떨어질 수 있다. **(1)** 이를 해결하기 위해, 모델을 결정할 때 모델의 복잡성에도 불구하고 RSS가 더 작아지는 이점이 더 클 때까지 분할을 하는 방법을 생각할 수 있다. 그러나 이는 어쩌면 매우 작은 tree(분할이 몇개 없는)를 만들거나 혹은 직관적으로 의미없는 tree 일 수도 있다(즉 더 많은 분할이 이뤄진 뒤에 RSS의 급격한 감소가 일어날 수도 있는데 이를 고려하지 않기 때문에). 따라서 교재에서 추천하는 방법은 **(2)** 매우 큰 모델을 하나 만든 다음 역으로 이를 줄여나가 subtree를 찾는 것이다.

이제 모델을 줄여나가는 방법을 알아보자

pruning 의 방법으로서 test error rate을 확인하는 것이 있다. 그러나 이를 위해서 각 subtree 마다 CV 혹은 validation set approach를 하는 것은 복잡한 계산을 더욱 복잡하게 만드는 것이다. 따라서 subtree를 몇개 선정해서 CV를 진행한다. 

그럼 subtree는 또 어떻게 결정할 것인가?

==> Cost Complexity prunning(weakest link pruning) : nonnegative tuning parameter alpha 를 가지고 판정. 수식은 아래와 같은데 T는 terminal node를 의미한다.
$$
\sum _m ^ {|T|} \sum _{x_i \in R_m} (y_i - \hat y_{R_m})^2 + \alpha |T|
$$
lasso 처럼 alpha값을 가지고 node의 개수를 조절하면서 RSS를 최소화 하는 모델을 결정하는 것이다. 

CV값을 가지고 몇개의 후보군 alpha를 선택 

pruning의 알고리즘은 다음과 같다.

<그림>

정리하면 큰 모델을 만들어낸 다음, 적절한 alpha 후보군을 가지고 CV를 해서 test error rate(이에 대한 예측값이 CV)을 최소로 하는 alpha를 정한다. alpha를 정한다는 것은 곧 변수의 개수를 통제하고 정하는 다는 것이므로 CV error 를 최소로 하는 변수의 개수를 선택하게 된다. 



##### 8.1.2 Classification Trees

어떤 class에 속할지 예측하는 데에 목적. class prediction, class proportion 두 개 다 관심 대상

tree growing은 여전히 classification tree에서도 주요 사항인데 regression에서는 RSS를 기준으로 삼았던 것과는 달리 여기선 classification error rate을 기준으로 삼는다. 

classification error rate : the fraction of the training observation in that region that do not belong to the most common class (class는 본래 분류, region은 추정 분류) 

책에는 아래와 같은 식을 보여주는데 이는 1-error rate으로 accuracy를 말하는 것 같다.
$$
E = 1 - max_k(\hat p_{mk})
$$

$$
\hat p _{mk}
$$

는 proportion of the training observation in the m-th region that are from the k-th class.



방금 전에도 기술했지만 error rate은 prediction accuracy와 관련된 것으로 accuracy를 목적으로 할 때에는 error rate을 주목해서 보겠지만, tree-growing에 대한 판단을 할 때에는 다른 기준을 사용한다.



1. Gini index ( a measure of total variance <u>across</u> the K classes)

$$
G = \sum \hat p_{mk}(1- \hat p _{mk})
$$

이는 각 노드의 purity 를 측정하는 것으로 hat p 값이 0 혹은 1에 가까울 수록 그 값이 낮아진다. 따라서 이 값은 작으면 작을수록 좋은 것이다.



2. Cross-entropy

$$
D = - \sum \hat p _{mk} log (\hat p _{mk})
$$

이 값 역시 hat p 값이 0 혹은 1에 가까울수록 0에 가까운 값을 가진다. 이 또한 purity를 측정하는 값이다. 



질적 변수 또한 분류의 기준이 될 수도 있다. 또한 purity를 기준으로 tree-growing을 실행했을 때, 같은 predicted value를 갖는 terminal node 도 존재한다. 즉 하나의 기준에 따라 나눠진 값이 모두 같은 값을 가질 수도 있다. 



##### 8.1.3 Trees Versus Linear Methods

본래 데이터 관계가 선형이면 당연히 linear 한 방법이 좋을 것.

본래 데이터 관계를 차치하고서도 분석의 목적이 interpreability나 visualization에 있다면 tree-based method 가 더 좋을 것.



##### 8.1.4 Advantages and Disadvantages of Trees

장점

1. 다른사람들에게 모델에 대해 설명하는 경우 linear regression 보다도 설명하기가 쉽다
2. 보다 인간의 사고 방식과 유사하다
3. 시각적으로 보여주기 쉽다
4. 더미변수 없이도 질적변수를 다룰 수 있다.



단점

1. 예측력이 떨어진다. 이를 극복하기 위해 아래 내용들이 나옴.



#### 8.2 Bagging, Random Forest, Boosting

위 세가지는 tree를 예측력이 높은 모델을 만들기 위한 하나의 block으로 여긴다.



##### 8.2.1 Bagging(creating, fitting, combining)

boostrap + aggregation

목적 : 예측력을 위해 분산을 줄여보자. 

어떻게? 평균으로 예측하면서 ( 표본평균의 분산이 모 분산에 비해 작아지는 것과 동일한 원리)

그런데 multiple training set을 얻기 힘드니까 boostrap을 사용하는 것.

B개의 다른 boostrapped training data set을 가지고 학습을 시켜 모델
$$
\hat f ^{*b}(x)
$$
을 만들고 이를 평균 내어 추정한다.

그럼 붓스트랩 data set 개수 B는 어떻게 결정되는 것인가? 딱히 정해진 것은 없으며 개수가 많아도 overfitting의 위험이 없다. 즉 error를 가장 작게 만드는 큰 값이 좋을 것이다.



bagging은 사실 많은 회귀문제에 사용될 수 있는 방법인데 특히 분산이 큰 decision tree에 유용하다. 또한 boostraping을 사용할 때 block으로 여겨지는 tree들은 deep하고 prune 되지 않은 것이다.



classification 에서 bagging을 사용하는 방법은 각 트리에서 나온 분류 예측값들을 가지고 voting을 통해 결정한다. 



##### Out - of - Bag Error Estimation

boostrap 방법의 특징을 이용해서 error를 측정하는데 이는 boostrap data set B가 매우 클 때 leave-one-out cross-validation과 유사하다(즉 데이터가 부족할 때 k-fold 처럼 많은 수의 데이터를 validation set으로 놓기 힘들 때 유용하다). Data set이 너무 큰 경우 CV error를 계산하는데 필요한 계산량이 매우 많은데 이 대신에 OOB error를 사용할 수 있다.

방법은 다음과 같다. boostraping 을 하는데 사용하는 observation 중에서 2/3 만 사용해서 boostrapping sample을 만든다. 그 뒤 이를 적합시켜 모델을 만들고 남은 1/3의 관측치에 대한 예측값을 구하고 OOB MSE를 구한다. 이 error 값이 bagged model의 test error에 대한 추정값이 된다. 



##### Variable Importance Measures

bagging을 하면 해석력이 떨어지고 visualization이 어렵다. 즉 더이상 statistical learning procedure을 트리 형태로 보이기 어렵다. 이는 어떠한 변수가 더 중요한 것인지 시각화하기 어렵다는 것이다. 단 이러함을 감수하고서도 예측력은 더 좋아질 것이다. 

그럼에도 불구하고 bagging을 통해 알아낼 수 있는 부분은, 각 변수들에 대해서 그 변수를 split 했을 때, RSS 감소량 혹은 Gini-index의 감소량의 총합(각 모델과 bagging에 의한 rss, gini-index)이 크면 그 변수는 상대적으로 더 중요한 것이라고 할 수 있다. 



##### 8.2.2 Random Forests

Random Forest는 bagged tree를 개선시킨 것으로서 boostarping 이전 변수의 일부분만 뽑아서 이들을 가지고 모델에 적합시킨다. 이 방법은 예측값의 correlation을 낮춰줄 수 있다. 만일 특정변수가 매우 영향력이 크다면 bagged tree들은 모두 특정변수를 top split으로 둔 model이 될 것이다. 이러한 경우 결과값이 매우 유사해질 것이고 평균 시 variance의 감소 효과가 미미하다. Random Forest는 predictor 중 일부분만 변수로 넣게 함으로써 uncorrelated quantity들을 평균시킬 수 있다. 

이때 각 split은 m 개의 변수로 이뤄지며 전체 변수 개수가 p 일 때
$$
m =\sqrt (p)
$$
이다. 

(oob는 행을 떼어내는 것, RF는 열을 떼어내는 것)

RF는 bagging의 일종이므로 bagging data set B의 크기는 overfitting과 관련이 없다. 



##### 8.2.3 Boosting

Boosting 또한 tree-based method 뿐만 아니라 다른 regression 혹은 classification에서도 쓸 수 있다. 

Boosting의 방식 또한 'creating + fitting+combining'으로 bagging과 유사하다. 

단 tree가 sequentially 증가한다는 데에 차이가 있다. "**each tree is grwon using information from previously grown trees**" 이 말 뜻은  정해진 모델들에서 나온 값들을 combining하지는 bagging과는 다르게, boosting 은 잔차를 살피고 이에 따라 가중치를 달리주면서 모델을 점점 만들어나간다는 것이다.

또한 boostrapping을 통해 표본을 만들어 내는 것이 아니라, 각 tree들이 본래 데이터의 변형된 형태에 적합이 된다는 것에 차이가 있다. 

알고리즘은 다음과 같다

<그림>



알고리즘이 의미하는대로, boosting은 y의 값이 아닌 업데이트 된 잔차를 target 변수처럼 생각하고 적합하는 데에 있다. 적합된 모델은 기존의 적합 모델과 더해지고 lambda에 따라서 여러가지 적합 모델이 잔차를 attack(상쇄시키려 노력, 잔차를 없애려고 노력)한다. 

충분히 예상하겠지만 이러한 과정은 모델의 학습 속도를 늦추게 한다. 

boosting은 세가지 tuning parameter를 갖는다.

(1) the number of trees B. 여기서 B는 그림의 알고리즘에 나오는 B이다. 즉 업데이트 횟수. boosting은 bagging이나 RF와는 달리 overfitting이 될 수 있기 때문에 CV를 통해 최적의 횟수를 결정해야한다.(앞에서 보았듯, bagging이나 RF는 B의 개수가 overfitting을 결정하진 않았다)

(2) the shrinkage parameter lambda. 이는 부스팅을 통해 학습을 하는 비율을 조절한다. 일반적으로 0.01 혹은 0.001을 사용한다.  이 값이 작으면 좋은 성능을 위한 학습의 횟수가 많이 요구된다.

(3) the number of d. 즉 변수의 개수. 이는 앙상블의 복잡성을 통제. 변수의 개수인 만큼 interaction depth를 결정한다고 말할 수 있다. 즉 변수가 많아지면 변수간의 교호작용도 많이 고려해야한다.



또한 Boosting은 RF와 다르게 tree의 성장이 이전 tree에 영향을 받기 때문에 더 작은 tree 또한 설명력이 좋을 수 있다. 
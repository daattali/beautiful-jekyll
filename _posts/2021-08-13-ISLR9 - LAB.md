---
layout : post
title : ISLR
subtitle : statistical learning - 9장 - LAB
date : 2021-08-09
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 9장 코드 정리



~~~R
## support vector classifier

#kernel 값을 linear로 주면 svm() 함수에서 support vector classifier를 실행할 수 있다.
set.seed(1)
x = matrix(rnorm(20*2), ncol=2)
y = c(rep(-1,10), rep(1,10))
fix(x)
x[y==1,] = x[y==1,] + 1 #y==1 에 해당하는 부분, 즉 x 데이터 중에서 11행 부터 20행까지의 수에 1을 모두 더해주어라.


plot(x, col=(3-y))

#svm이 작동되기 위해서는 y 변수를 as.factor를 통해서 범주형 변수로 만들어 주어야 한다.
dat= data.frame(x=x, y=as.factor(y))
dat
library(e1071)
svmfit = svm(y~., data = dat, kernel ='linear', cost =1000, scale = FALSE) #cost 값이 작으면 margin 값이 더욱 넓어진다.
plot(svmfit, dat)

svmfit$index
summary(svmfit)

#library e1071에 있는 tune 함수를 통해 CV를 할 수 있다. cost가 작으면 margin이 넓어진다.
set.seed(1)
tune.out = tune(svm, y~., data=dat, kernel = "linear", ranges = list(cost = c(0.001,0.01,0.1,1.5,10,100)))
summary(tune.out)

#tune 함수는 best model을 저장한다
bestmod = tune.out$best.model
summary(bestmod)

#predict
xtest = matrix(rnorm(20*2), ncol=2)
ytest = sample(c(-1,1),20, rep=TRUE)
xtest[ytest==1, ] = xtest[ytest==1,] + 1
testdat = data.frame(x=xtest, y =as.factor(ytest))
testdat
ypred = predict(bestmod,xtest)
table(predict = ypred, truth=testdat$y)




## support vector machine
set.seed(1)
x = matrix(rnorm(200*2), ncol=2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] -2
y = c(rep(1,150),rep(2,50))
dat = data.frame(x=x,y=as.factor(y))
plot(x,col=y) # boundary is non-linear

train = sample(200,100)
svmfit = svm(y ~ .,data = dat[train,], kernel='radial', gamma =1, cost=1)
plot(svmfit, dat[train,])

#cost 값을 키우면 training error를 줄일 수 있긴 하지만 overfitting의 위험 존재. ==> CV를 통해 최적값 구하기
set.seed(1)
tune.out = tune(svm, y~., data= dat[train,], kernel='radial', ranges=list(cost = c(0.1,1,10,1000), gamma = c(0.5,1,2,3,4)))
summary(tune.out)

#예측해보기
table(true=dat[-train,"y"],pred=predict(tune.out$best.model,newx=dat[-train,]))



## ROC curve ( TP rate / FP rate )
library(ROCR)
rocplot = function(pred, truth,...) {
    predob = prediction(as.numeric(pred), as.numeric(truth))
    perf = performance(predob, "tpr",'fpr')
    plot(perf,...)
}

svmfit.opt = svm(y~., data = dat[train,], kernel='radial', gamma=2, cost=1, decision.values=T)
fitted = predict(svmfit.opt, data= dat[train,], decision.values=T) #decision.values=T를 줌으로써 확률값이 아닌 예측 class 가 나옴
rocplot(fitted, dat[train,'y'], main = 'training data')

#gamma를 조절해서 boundary를 flexible 하게 만들 수 있다.
#gamma 값이 커진다는 것은 커널 function K 가 커지는데 특히 gamma에 의해서 K 가 커지면 본래 support vector와의 거리가 작았음에도 그 거리를 확대시키고, 음의 부호에 의해 커널 자체의 값을 작게 만들어 버린다. 즉 모델 자체가 매우 flexible 해진다는 것이다.

svmfit.flex = svm(y~., data = dat[train,], kernel='radial', gamma=50, cost=1, decision.values=T)
fitted = predict(svmfit.flex, data= dat[train,], decision.values=T) #decision.values=T를 줌으로써 확률값이 아닌 예측 class 가 나옴
rocplot(fitted, dat[train,'y'], add=T, col='red')
# plot에서도 볼 수 있듯, FPR이 낮을때에도(1종 오류  / 전체 오류 가 낮을 때에도) TPR이 높다. 즉 specifity를 유지하면서 sensitivity를 높였다. 
# 즉 training data에 대한 accuracy를 높인것.(과적합의 위험)

#test data에 모델이 얼마나 잘 작동하는지 보기 위해

fitted = predict(svmfit.opt, data= dat[-train,], decision.values=T) #decision.values=T를 줌으로써 확률값이 아닌 예측 class 가 나옴
rocplot(fitted, dat[-train,'y'], main = 'test data')

fitted = predict(svmfit.flex, data= dat[-train,], decision.values=T) #decision.values=T를 줌으로써 확률값이 아닌 예측 class 가 나옴
rocplot(fitted, dat[-train,'y'],add=T, col='red')



## SVM with multiple case(Response class 가 여러개 일 때)
set.seed(1)
x=rbind(x, matrix(rnorm(50*2), ncol=2)) #행 결합
y = c(y, rep(0,50))
dat = data.frame(x=x, y=as.factor(y))
plot(x, col=(y+1))

svmfit = svm(y~., data=dat, kernel = 'radial', cost=10, gamma=1)
plot(svmfit, dat)     

~~~




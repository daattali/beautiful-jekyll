# When making a new entry, copy the commented out entry, paste it, uncomment, and fill out the fields.
# If any of the fields are confusing look at previous examples for guidance.
#- abs: null
#  authors: null
#  award: null
#  bib: null
#  img: null
#  links: {}
#  short_id: null
#  site: null
#  title: null
#  venue: null
#  video_embed: null
#  tags: null
# Current tags for research areas, see the research areas page for more details:
# deform_obj_manip, 3D_afford_obj_manip, multimodal, rl_algs, auto_driving, active_perception, self_sup_rob
- abs: "We present HACMan++, a reinforcement learning framework using a novel action space of spatially-grounded parameterized motion primitives for manipulation tasks."
  authors: Bowen Jiang*, Yilin Wu*, Wenxuan Zhou, Chris Paxton, David Held
  award: null
  bib: >
    @inproceedings{jiang2024hacman++,
      title={HACMan++: Spatially-Grounded Motion Primitives for Manipulation},
      author={Jiang, Bowen and Wu, Yilin and Zhou, Wenxuan and Paxton, Chris and Held, David},
      booktitle={Robotics: Science and Systems (RSS)},
      year={2024}
    }
  img: ../pics/hacman++.gif
  links:
      '[arXiv]': https://arxiv.org/pdf/2407.08585
  short_id: jiang2024hacman++
  site: https://sgmp-rss2024.github.io/
  title: "HACMan++: Spatially-Grounded Motion Primitives for Manipulation"
  venue: Robotics: Science and Systems (RSS), 2024
  video_embed: <iframe width="560" height="396" src="https://sgmp-rss2024.github.io/static/videos/all_v2.mp4" title="HACMan++:Spatially-Grounded Motion Primitives for Manipulation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
      - rl
      - affordance

- abs: "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly using or adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates corresponding simulation environments by populating pertinent objects and assets with proper spatial configurations. Afterwards, the agent decomposes the proposed high-level task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our work attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments."
  authors: Yufei Wang*, Zhou Xian*, Feng Chen*, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan
  award: null
  bib: >
    @inproceedings{wang2024robogen,
      title={RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation},
      author={Wang, Yufei and Zhou, Xian and Chen, Feng and Wang, Tsun-Hsuan and Wang, Yian and Fragkiadaki, Katerina and Erickson, Zackory and Held, David and Gan, Chuang},            
      booktitle={International Conference on Machine Learning (ICML)},
      year={2024}       
    }
  img: ../pics/robogen-icml.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2311.01455
      '[Code]': https://github.com/Genesis-Embodied-AI/RoboGen
  short_id: wang2024robogen
  site: https://robogen-ai.github.io/
  title: "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"
  venue: International Conference on Machine Learning (ICML), 2024
  video_embed: null

- abs: Reward engineering has long been a challenge in Reinforcement Learning research, as it often requires extensive human effort. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent’s visual observations, by leveraging feedback from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent’s image observations based on the text description of the task goal, and then learn a reward function from the preference labels. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains — including classic control, as well as manipulation of rigid, articulated, and deformable objects — without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions.
  authors: Yufei Wang*, Zhanyi Sun*, Jesse Zhang, Zhou Xian, Erdem Bıyık, David Held&dagger;, Zackory Erickson&dagger;
  award: null
  bib: >
    @inproceedings{wang2024rlvlmf,
      title={RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback},
      author={Wang, Yufei and Sun, Zhanyi and Zhang, Jesse and Xian, Zhou and Biyik, Erdem and Held, David and Erickson, Zackory},            
      booktitle={International Conference on Machine Learning (ICML)},
      year={2024}       
    }
  img: ../pics/RL-VLM-F-24-ICML.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2402.03681
      '[Code]': https://github.com/yufeiwang63/RL-VLM-F
  short_id: wang2024rlvlmf
  site: https://rlvlmf2024.github.io/
  title: "RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback"
  venue: International Conference on Machine Learning (ICML), 2024
  video_embed: null

- abs: Robot-assisted dressing could profoundly enhance the quality of life of adults with physical disabilities. To achieve this, a robot can benefit from both visual and force sensing. The former enables the robot to ascertain human body pose and garment deformations, while the latter helps maintain safety and comfort during the dressing process. In this paper, we introduce a new technique that leverages both vision and force modalities for this assistive task. Our approach first trains a vision-based dressing policy using reinforcement learning in simulation with varying body sizes, poses, and types of garments. We then learn a force dynamics model for action planning to ensure safety. Due to limitations of simulating accurate force data when deformable garments interact with the human body, we learn a force dynamics model directly from real-world data. Our proposed method combines the vision-based policy, trained in simulation, with the force dynamics model, learned in the real world, by solving a constrained optimization problem to infer actions that facilitate the dressing process without applying excessive force on the person. We evaluate our system in simulation and in a real-world human study with 10 participants across 240 dressing trials, showing it greatly outperforms prior baselines. Video demonstrations are available on our project website. 
  authors: Zhanyi Sun*, Yufei Wang*, David Held&dagger;, Zackory Erickson&dagger;
  award: null
  bib: >
    @article{sun2024force,
      title={Force-Constrained Visual Policy: Safe Robot-Assisted Dressing via Multi-Modal Sensing},
      author={Sun, Zhanyi and Wang, Yufei and Held, David and Erickson, Zackory},
      journal={IEEE Robotics and Automation Letters},
      year={2024}
    }
  img: ../pics/FCVP-RAL-24.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2311.04390
  short_id: sun2024safe
  site: https://sites.google.com/view/dressing-fcvp
  title: "Force Constrained Visual Policy: Safe Robot-Assisted Dressing via Multi-Modal Sensing"
  venue: Robotics and Automation Letters (RAL), 2024
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/6lmWB-6uwoc?si=EzwZXtFb0En-f8w4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  tags:
    - deformable
    - multimodal

- abs: The ability to identify important objects in a complex and dynamic driving environment is essential for autonomous driving agents to make safe and efficient driving decisions. It also helps assistive driving systems decide when to alert drivers. We tackle object importance estimation in a data-driven fashion and introduce HOIST - Human-annotated Object Importance in Simulated Traffic. HOIST contains driving scenarios with human-annotated importance labels for vehicles and pedestrians. We additionally propose a novel approach that relies on counterfactual reasoning to estimate an object's importance. We generate counterfactual scenarios by modifying the motion of objects and ascribe importance based on how the modifications affect the ego vehicle's driving. Our approach outperforms strong baselines for the task of object importance estimation on HOIST. We also perform ablation studies to justify our design choices and show the significance of the different components of our proposed approach.
  authors: Pranay Gupta, Abhijat Biswas, Henny Admoni, David Held
  award: null
  bib: >
    @article{gupta2023object,
      title={Object Importance Estimation using Counterfactual Reasoning for Intelligent Driving},
      author={Gupta, Pranay and Biswas, Abhijat and Admoni, Henny and Held, David},
      journal={arXiv preprint arXiv:2312.02467},
      year={2023}
    }
  img: ../pics/gupta2023object.png
  links: 
      '[arXiv]': https://arxiv.org/abs/2312.02467
      '[Code]': https://github.com/vehicle-importance/oiecr
  short_id: gupta2023object
  site: https://vehicle-importance.github.io/
  title: "Object Importance Estimation using Counterfactual Reasoning for Intelligent Driving"
  venue: Robotics and Automation Letters (RAL), 2024
  video_embed: null
  tags: 
      - auto_driving

- abs: "Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack. In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations - the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably SE(3)-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an SE(3) invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably SE(3) equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations."
  authors: Ben Eisner, Yi Yang, Todor Davchev, Mel Vecerik, Jonathan Scholz, David Held
  award: null
  bib: >
    @article{eisner2024deep,
      title={Deep {SE}(3)-Equivariant Geometric Reasoning for Precise Placement Tasks},
      author={Ben Eisner and Yi Yang and Todor Davchev and Mel Vecerik and Jonathan Scholz and David Held},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=2inBuwTyL2}
    }
  img: ../pics/reldist.webp
  links:
      - '[arXiv]': https://arxiv.org/abs/2404.13478
      - '[OpenReview]': https://openreview.net/forum?id=2inBuwTyL2
      - '[Code]': https://github.com/r-pad/taxpose
  short_id: eisner2024geometric
  site: https://sites.google.com/view/reldist-iclr-2023
  title: "Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks"
  venue: International Conference on Learning Representations (ICLR), 2024
  video_embed: null
  tags:
      - geometric_reasoning
- abs: Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
  authors: Fan Yang, Wenxuan Zhou, Zuxin Liu, Ding Zhao, David Held
  award: null
  bib: >
    @article{yang2023reinforcement,
      title={Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization},
      author={Yang, Fan and Zhou, Wenxuan and Liu, Zuxin and Zhao, Ding and Held, David},
      journal={arXiv preprint arXiv:2310.06903},
      year={2023}
    }
  img: ../pics/ICRA2024_SEMDP.gif
  links:
      '[arXiv]': https://arxiv.org/pdf/2310.06903.pdf
  short_id: yang2023reinforcement
  site: https://sites.google.com/view/safemdp
  title: "Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization"
  venue: International Conference on Robotics and Automation (ICRA), 2024
  video_embed: null
  tags:
      - safe_reinforcement_learning

- abs: Relative placement tasks are an important category of tasks in which one object needs to be placed in a desired pose relative to another object.  Previous work has shown success in learning relative placement tasks from just a small number of demonstrations, when using relational reasoning networks with geometric inductive biases. However, such methods fail to consider that demonstrations for the same task can be fundamentally multimodal, like a mug hanging on any of n racks. We propose a method that retains the provably translation-invariant and relational properties of prior work but incorporates additional properties that account for multimodal, distributional examples. We show that our method is able to learn precise relative placement tasks with a small number of multimodal demonstrations with no human annotations across a diverse set of objects within a category.
  authors: Jenny Wang*, Octavian Donca*, David Held
  award: null
  bib: >
    @article{wang2024taxposed,
      title={Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation},
      author={Wang, Jenny and Donca, Octavian and Held, David},
      journal={IEEE International Conference on Robotics and Automation (ICRA), 2024},
      year={2024}
    }
  img: ../pics/icra2024_taxposed.gif
  links: 
      '[arXiv]': https://arxiv.org/abs/2405.04609
      '[Poster]': https://sites.google.com/view/tax-posed/poster
      '[Slides]': https://sites.google.com/view/tax-posed/slides
      '[Code]': https://github.com/himty/taxposeD
  short_id: wang2024taxposed
  site: https://sites.google.com/view/tax-posed
  title: "Learning Distributional Demonstration Spaces for Task-Specific Cross-Pose Estimation"
  venue: International Conference on Robotics and Automation (ICRA), 2024
  video_embed: null
  tags:
      - affordance

- abs: Understanding and manipulating articulated objects, such as doors and drawers, is crucial for robots operating in human environments. We wish to develop a system that can learn to articulate novel objects with no prior interaction, after training on other articulated objects. Previous approaches for articulated object manipulation rely on either modular methods which are brittle or end-to-end methods, which lack generalizability. This paper presents FlowBot++, a deep 3D vision-based robotic system that predicts dense per-point motion and dense articulation parameters of articulated objects to assist in downstream manipulation tasks. FlowBot++ introduces a novel per-point representation of the articulated motion and articulation parameters that are combined to produce a more accurate estimate than either method on their own. Simulated experiments on the PartNet-Mobility dataset validate the performance of our system in articulating a wide range of objects, while real-world experiments on real objects' point clouds and a Sawyer robot demonstrate the generalizability and feasibility of our system in real-world scenarios.
  authors: Harry Zhang, Ben Eisner, David Held
  award: null
  bib: > 
    @inproceedings{zhang2023fbpp,
      title={FlowBot++: Learning Generalized Articulated Objects Manipulation via Articulation Projection},
      author={Zhang, Harry and Eisner, Ben and Held, David},
      journal={Conference on Robot Learning (CoRL)},
      year={2023}
    }
  img: ../pics/fbpp.gif
  links:
      '[arXiv]': https://arxiv.org/abs/2306.12893
  short_id: fbpp
  site: https://sites.google.com/view/flowbotpp/home
  title: "FlowBot++: Learning Generalized Articulated Objects Manipulation via Articulation Projections"
  venue: Conference on Robot Learning (CoRL), 2023
  video_embed: <iframe width="560" height="396" src="https://www.youtube.com/embed/MEFYeVMUuic" title="Learning Generalized Articulated Objects Manipulation via Articulation Projections" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
      - 3D_afford_obj_manip
- abs: Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about gripper-object interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized initial poses, randomized 6D goals, and diverse object categories, our policy demonstrates strong generalization to unseen object categories without a performance drop, achieving an 89% success rate on unseen objects in simulation and 50% success rate with zero-shot transfer in the real world. Compared to alternative action representations, HACMan achieves a success rate more than three times higher than the best baseline. With zero-shot sim2real transfer, our policy can successfully manipulate unseen objects in the real world for challenging non-planar goals, using dynamic and contact-rich non-prehensile skills.
  authors: Wenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton*, David Held*
  award: <award>Oral Presentation</award>  (Selection rate 6.6%)
  bib: >
    @inproceedings{zhou2023hacman,
      title={HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation},
      author={Zhou, Wenxuan and Jiang, Bowen and Yang, Fan and Paxton, Chris and Held, David},
      journal={Conference on Robot Learning (CoRL)},
      year={2023},
    }
  img: ../pics/hacman-v2.gif
  links:
      '[arXiv]': https://arxiv.org/abs/2305.03942
      '[Code]': https://github.com/HACMan-2023/HACMan
      '[PDF]': https://openreview.net/pdf?id=fa7FzDjhzs9
  short_id: hacman
  site: https://hacman-2023.github.io/
  title: "HACMan: Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation"
  venue: Conference on Robot Learning (CoRL), 2023
  video_embed: <iframe width="560" height="396" src="https://hacman-2023.github.io/static/videos/zoomed_website.mp4" title="Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
      - 3D_afford_obj_manip
      - rl_algs
- abs: Many fabric handling and 2D deformable material tasks in homes and
    industry require singulating layers of material such as opening a bag or
    arranging garments for sewing. In contrast to methods requiring specialized
    sensing or end effectors, we use only visual observations with ordinary parallel
    jaw grippers. We propose SLIP, Singulating Layers using Interactive Perception,
    and apply SLIP to the task of autonomous bagging. We develop SLIP-Bagging, a
    bagging algorithm that manipulates a plastic or fabric bag from an unstructured
    state, and uses SLIP to grasp the top layer of the bag to open it for object
    insertion. In physical experiments, a YuMi robot achieves a success rate of 67%
    to 81% across bags of a variety of materials, shapes, and sizes, significantly
    improving in success rate and generality over prior work. Experiments also
    suggest that SLIP can be applied to tasks such as singulating layers of folded
    cloth and garments.
  authors: Lawrence Yunliang Chen, Baiyu Shi, Roy Lin, Daniel Seita, Ayah Ahmad, Richard Cheng, Thomas Kollar, David Held, Ken Goldberg
  award: null
  bib: >
    @inproceedings{slipbagging2023,\n
      title={{Bagging by Learning to Singulate Layers Using Interactive Perception}},\n
      author={Lawrence Yunliang Chen and Baiyu Shi and Roy Lin and Daniel Seita and Ayah Ahmad and Richard Cheng and Thomas Kollar and David Held and Ken Goldberg},\n
      booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n
      year={2023}\n}"
  img: ../pics/slip-bagging-160.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2303.16898
  short_id: 2023slipbagging
  site: https://sites.google.com/view/slip-bagging/
  title: "Bagging by Learning to Singulate Layers Using Interactive Perception"
  venue: International Conference on Intelligent Robots and Systems (IROS), 2023
  video_embed: null
- abs: 'Robot-assisted dressing could benefit the lives of many people such as older adults and individuals with disabilities. Despite such potential, robot-assisted dressing remains a challenging task for robotics as it involves complex manipulation of deformable cloth in 3D space. Many prior works aim to solve the robot-assisted dressing task, but they make certain assumptions such as a fixed garment and a fixed arm pose that limit their ability to generalize. In this work, we develop a robot-assisted dressing system that is able to dress different garments on people with diverse poses from partial point cloud observations, based on a learned policy. We show that with proper design of the policy architecture and Q function, reinforcement learning (RL) can be used to learn effective policies with partial point cloud observations that work well for dressing diverse garments. We further leverage policy distillation to combine multiple policies trained on different ranges of human arm poses into a single policy that works over a wide range of different arm poses. We conduct comprehensive real-world evaluations of our system with 510 dressing trials in a human study with 17 participants with different arm poses and dressed garments. Our system is able to dress 86\% of the length of the participants arms on average. Videos can be found on the anonymized project webpage: https://sites.google.com/view/one-policy-dress.'
  authors: Yufei Wang, Zhanyi Sun, Zackory Erickson*, David Held*
  award: null
  bib: >
    @inproceedings{Wang2023One,\n            title={One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments},\n            author={Wang, Yufei and Sun, Zhanyi and Erickson, Zackory and Held, David},\n            booktitle={Robotics: Science\
    \ and Systems (RSS)},\n            year={2023}\n       }"
  img: ../pics/RSS23-dressing.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2306.12372
  short_id: Wang2023One
  site: https://sites.google.com/view/one-policy-dress
  title: 'One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments'
  venue: 'Robotics: Science and Systems (RSS), 2023'
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/SQRCCQYAyiY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  tags:
    - deformable
    - affordance
- abs: "To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments."
  authors: Siddharth Ancha, Gaurav Pathak, Ji Zhang, Srinivasa Narasimhan, David Held
  award: null
  bib: >
    @inproceedings{ancha2023rss,\n
        title     = {Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits},\n
        author    = {Siddharth Ancha AND Gaurav Pathak AND Ji Zhang AND Srinivasa Narasimhan AND David Held},\n
        booktitle = {Proceedings of Robotics: Science and Systems},\n
        year      = {2023},\n
    \ address   = {Daegu, Republic of Korea},\n
    \ month     = {July},\n
      }
  img: ../pics/rss2023lightcurtains.png
  links:
    '[arXiv]': https://arxiv.org/abs/2302.12597
    '[Talk]': https://youtu.be/szUpgr36T9c
  short_id: Ancha2023Active
  site: https://siddancha.github.io/projects/active-velocity-estimation/
  title: Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits
  venue: 'Robotics: Science and Systems (RSS), 2023'
  video_embed: null
  tags:
    - active-perception
    - self-supervised
- abs: "Predicting how the world can evolve in the future is crucial for motion planning in autonomous systems. Classical methods are limited because they rely on costly human annotations in the form of semantic class labels, bounding boxes, and tracks or HD maps of cities to plan their motion — and thus are difficult to scale to large unlabeled datasets. One promising self-supervised task is 3D point cloud forecasting from unannotated LiDAR sequences. We show that this task requires algorithms to implicitly capture (1) sensor extrinsics (i.e., the egomotion of the autonomous vehicle), (2) sensor intrinsics (i.e., the sampling pattern specific to the particular LiDAR sensor), and (3) the shape and motion of other objects in the scene. But autonomous systems should make predictions about the world and not their sensors! To this end, we factor out (1) and (2) by recasting the task as one of spacetime (4D) occupancy forecasting. But because it is expensive to obtain ground-truth 4D occupancy, we “render” point cloud data from 4D occupancy predictions given sensor extrinsics and intrinsics, allowing one to train and test occupancy algorithms with unannotated LiDAR sequences. This also allows one to evaluate and compare point cloud forecasting algorithms across diverse datasets, sensors, and vehicles."
  authors: Tarasha Khurana, Peiyun Hu, David Held, Deva Ramanan
  award: null
  bib: >
    @inproceedings{Khurana2023point,\n
        title={Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting},\n
        author={Khurana, Tarasha and Hu, Peiyun and Held, David and Ramanan, Deva},\n
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and\
    \ Pattern Recognition (CVPR)},\n
        year={2023}\n
      }
  img: ../pics/cvpr2023_logo.png
  links:
  short_id: Khurana2023point
  site:
  title: "Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting"
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2023
  video_embed: null
  tags:
    - autonomous-driving
    - self-supervised
- abs: "Physical interaction with textiles, such as assistive dressing, relies on advanced dextreous capabilities. The underlying complexity in textile behavior when being pulled and stretched, is due to both the yarn material properties and the textile construction technique. Today, there are no commonly adopted and annotated datasets on which the various interaction or property identification methods are assessed. One important property that affects the interaction is material elasticity that results from both the yarn material and construction technique: these two are intertwined and, if not known a-priori, almost impossible to identify through sensing commonly available on robotic platforms. We introduce Elastic Context (EC), a concept that integrates various properties that affect elastic behavior, to enable a more effective physical interaction with textiles. The definition of EC relies on stress/strain curves commonly used in textile engineering, which we reformulated for robotic applications. We employ EC using Graph Neural Network (GNN) to learn generalized elastic behaviors of textiles. Furthermore, we explore the effect the dimension of the EC has on accurate force modeling of non-linear real-world elastic behaviors, highlighting the challenges of current robotic setups to sense textile properties."
  authors: Alberta Longhini, Marco Moletta, Alfredo Reichlin, Michael C. Welle, Alexander Kravberg, Yufei Wang, David Held, Zackory Erickson, Danica Kragic
  award: null
  bib: >
    @inproceedings{Longhini2023elastic,\n
        title={Elastic Context: Encoding Elasticity for Data-driven Models of Textiles },\n
        author={Longhini, Alberta and Moletta, Marco  and  Reichlin, Alfredo and Welle, Michael C.  and  Kravberg, Alexander and Wang, Yufei and Held, David and Erickson, Zackory  and Kragic, Danica },\n
        booktitle={IEEE International Conference on Robotics and Automation (ICRA), 2023},\n
        year={2023}\n
      }
  img: ../pics/ICRA23-ElasticContext.png
  links:
    '[arXiv]': https://arxiv.org/abs/2209.05428
  short_id: Longhini2023elastic
  site: https://arxiv.org/abs/2209.05428
  title: "Elastic Context: Encoding Elasticity for Data-driven Models of Textiles"
  venue: International Conference on Robotics and Automation (ICRA), 2023
  video_embed: null
- abs: "We study the problem of learning graph dynamics of deformable objects which generalize to unknown physical properties. In particular, we leverage a latent representation of elastic physical properties of cloth-like deformable objects which we explore through a pulling interaction. We propose EDO-Net (Elastic Deformable Object - Net), a model trained in a self-supervised fashion on a large variety of samples with different elastic properties. EDO-Net jointly learns an adaptation module, responsible for extracting a latent representation of the physical properties of the object and a forward-dynamics module, which leverages the latent representation to predict future states of cloth-like objects, represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties of cloth-like deformable objects, 2) transferring the learned representation to new downstream tasks."
  authors: Alberta Longhini*, Marco Moletta*, Alfredo Reichlin, Michael C. Welle, David Held, Zackory Erickson, Danica Kragic
  award: null
  bib: >
    @inproceedings{Longhini2023EDO,
      title={EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics},
      author={Longhini, Alberta and Moletta, Marco and  Reichlin, Alfredo and Welle, Michael C. and Held, David  and Erickson, Zackory  and Kragic, Danica },
      booktitle={IEEE International Conference on Robotics and Automation (ICRA), 2023},
      year={2023}
    }
  img: ../pics/EDONet.png
  links:
    '[arXiv]': https://arxiv.org/abs/2209.08996
  short_id: Longhini2023EDO
  site: https://arxiv.org/abs/2209.08996
  title: "EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics"
  venue: International Conference on Robotics and Automation (ICRA), 2023
  video_embed: null
- abs: State estimation is one of the greatest challenges for cloth manipulation due to cloth's high dimensionality and self-occlusion. Prior works propose to identify the full state of crumpled clothes by training a mesh reconstruction model in simulation. However, such models are prone to suffer from a sim-to-real gap due to differences between cloth simulation and the real world. In this work, we propose a self-supervised method to finetune a mesh reconstruction model in the real world. Since the full mesh of crumpled cloth is difficult to obtain in the real world, we design a special data collection scheme and an action-conditioned model-based cloth tracking method to generate pseudo-labels for self-supervised learning. By finetuning the pretrained mesh reconstruction model on this pseudo-labeled dataset, we show that we can improve the quality of the reconstructed mesh without requiring human annotations, and improve the performance of downstream manipulation task.
  authors: Zixuan Huang, Xingyu Lin, David Held
  award: null
  bib: >
    @inproceedings{huang2023act,\n
        title={Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking},\n
        author={Huang, Zixuan and Lin, Xingyu and Held, David},\n
        booktitle={IEEE International Conference on Robotics and Automation (ICRA), 2023},\n
        year={2023}\n
      }
  img: ../pics/2023act.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2302.09502
  short_id: 2023act
  site: "https://sites.google.com/view/ss-mesh-recon/"
  title: Self-supervised Cloth Reconstruction via Action-conditioned Cloth Tracking
  venue: International Conference on Robotics and Automation (ICRA), 2023
  video_embed: null
  tags:
    - deformable
    - self-supervised
    - tracking
- abs: We formulate grasp learning as a neural field and present Neural Grasp Distance Fields (NGDF).
    Here, the input is a 6D pose of a robot end effector and output is a distance to a continuous manifold of
    valid grasps for an object. In contrast to current approaches that predict a set of discrete candidate grasps,
    the distance-based NGDF representation is easily interpreted as a cost, and minimizing this cost produces a
    successful grasp pose. This grasp distance cost can be incorporated directly into a trajectory optimizer for
    joint optimization with other costs such as trajectory smoothness and collision avoidance. During optimization,
    as the various costs are balanced and minimized, the grasp target is allowed to smoothly vary, as the learned
    grasp field is continuous. In simulation benchmarks with a Franka arm, we find that joint grasping and planning
    with NGDF outperforms baselines by 63% execution success while generalizing to unseen query poses and unseen object shapes.
  authors: Thomas Weng, David Held, Franziska Meier, Mustafa Mukadam
  award: null
  bib: >
    @article{weng2023ngdf,\n
      title={Neural Grasp Distance Fields for Robot Manipulation},\n
      author={Weng, Thomas and Held, David and Meier, Franziska and Mukadam, Mustafa},\n
      booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n
      year={2023}\n}"
  img: ../pics/ngdf_icra2023.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2211.02647
    '[Code]': https://github.com/facebookresearch/NGDF/
  short_id: 2022ngdf
  site: https://sites.google.com/view/neural-grasp-distance-fields
  title: Neural Grasp Distance Fields for Robot Manipulation
  venue: International Conference on Robotics and Automation (ICRA), 2023
  video_embed: null
  tags:
    - affordance
- abs: Thin plastic bags are ubiquitous in retail stores, healthcare, food
    handling, recycling, homes, and school lunchrooms. They are challenging
    both for perception (due to specularities and occlusions) and for
    manipulation (due to the dynamics of their 3D deformable structure). We
    formulate the task of manipulating common plastic shopping bags with two
    handles from an unstructured initial state to a state where solid objects can
    be inserted into the bag for transport. We propose a self-supervised learning
    framework where a dual-arm robot learns to recognize the handles and rim of
    plastic bags using UV-fluorescent markings; at execution time, the robot does
    not use UV markings or UV light. We propose Autonomous Bagging (AutoBag), where
    the robot uses the learned perception model to open plastic bags through
    iterative manipulation.  We present novel metrics to evaluate the quality of a
    bag state and new motion primitives for reorienting and opening bags from
    visual observations. In physical experiments, a YuMi robot using AutoBag is
    able to open bags and achieve a success rate of 16/30 for inserting at least
    one item across a variety of initial bag configurations
  authors: Lawrence Yunliang Chen, Baiyu Shi, Daniel Seita, Richard Cheng, Thomas Kollar, David Held, Ken Goldberg
  award: null
  bib: >
    @inproceedings{autobag2023,\n
      title={{AutoBag: Learning to Open Plastic Bags and Insert Objects}},\n
      author={Lawrence Yunliang Chen and Baiyu Shi and Daniel Seita and Richard Cheng and Thomas Kollar and David Held and Ken Goldberg},\n
      booktitle={IEEE International Conference on Robotics and Automation (ICRA), 2023},\n
      year={2023}\n}"
  img: ../pics/autobag-ICRA2023.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2210.17217
  short_id: 2023autobag
  site: https://sites.google.com/view/autobag
  title: "AutoBag: Learning to Open Plastic Bags and Insert Objects"
  venue: International Conference on Robotics and Automation (ICRA), 2023
  video_embed: null
- abs: A simple gripper can solve more complex manipulation tasks if it can utilize
    the external environment such as pushing the object against the table or a vertical
    wall, known as "Extrinsic Dexterity." Previous work in extrinsic dexterity usually
    has careful assumptions about contacts which impose restrictions on robot design,
    robot motions, and the variations of the physical parameters. In this work, we
    develop a system based on reinforcement learning (RL) to address these limitations.
    We study the task of “Occluded Grasping” which aims to grasp the object in
    configurations that are initially occluded; the robot needs to move the object into
    a configuration from which these grasps can be achieved. We present a system with
    model-free RL that successfully achieves this task using a simple gripper with
    extrinsic dexterity. The policy learns emergent behaviors of pushing the object
    against the wall to rotate and then grasp it without additional reward terms on
    extrinsic dexterity. We discuss important components of the system including the
    design of the RL problem, multi-grasp training and selection, and policy
    generalization with automatic curriculum. Most importantly, the policy trained in
    simulation is zero-shot transferred to a physical robot. It demonstrates dynamic
    and contact-rich motions with a simple gripper that generalizes across objects
    with various size, density, surface friction, and shape with a 78% success rate.
  authors: Wenxuan Zhou, David Held
  award: <award>Oral Presentation</award>  (Selection rate 6.5%)
  bib: >
    @inproceedings{zhou2022ungraspable,\n
        title={{Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity}},\n
        author={Zhou, Wenxuan and Held, David},\n
        booktitle={Conference on Robot Learning (CoRL)},\n
        year={2022}\n
      }
  img: ../pics/ungraspable.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2211.01500
    '[Code]': https://github.com/Wenxuan-Zhou/ungraspable
    '[Poster]': https://drive.google.com/file/d/1O1mKGiqHc17ain8Ne4PuQK5Ji165-0F_/view
    '[Talk]': https://drive.google.com/file/d/1iHdgwRWlRIiEF75-KBhnMZqo-DZ9MkKs/view?usp=sharing
  short_id: 2022corlungraspable
  site: https://sites.google.com/view/grasp-ungraspable
  title: Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/6ZEPiwF18mA" title="Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
- abs: How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship “cross-pose” and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired pose relationship (placing a pan into the oven or the mug onto the mug rack). We demonstrate our method’s capability to generalize to unseen objects, in some cases after training on only 10 demonstrations in the real world. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments across a number of tasks.
  authors: Chuer Pan*, Brian Okorn*, Harry Zhang*, Ben Eisner*, David Held
  award: null
  bib: >
    @inproceedings{pan2022tax,\n
        title={TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation},\n
        author={Pan, Chuer and Okorn, Brian and Zhang, Harry and Eisner, Ben and Held, David},\n
        booktitle={Conference on Robot Learning (CoRL)},\n
        year={2022}\n
      }
  img: ../pics/taxpose-web.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2211.09325.pdf
  short_id: 2022corltaxpose
  site: https://sites.google.com/view/tax-pose/home
  title: 'TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation'
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/1GTEXuk1TEk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - affordance
- abs: "Point clouds are a widely available and canonical data modality which
      \ conveys the 3D geometry of a scene. Despite significant progress in classifica-
      \ tion and segmentation from point clouds, policy learning from such a modality
      \ remains challenging, and most prior works in imitation learning focus on learn-
      \ ing policies from images or state information. In this paper, we propose a novel
      \ framework for learning policies from point clouds for robotic manipulation with
      \ tools. We use a novel neural network, ToolFlowNet, which predicts dense per-
      \ point flow on the tool that the robot controls, and then uses the flow to derive the
      \ transformation that the robot should execute. We apply this framework to imita-
      \ tion learning of challenging deformable object manipulation tasks with continuous
      \ movement of tools, including scooping and pouring, and demonstrate significantly
      \ improved performance over baselines which do not use flow. We perform 50 phys-
      \ ical scooping experiments with ToolFlowNet and attain 82% scooping success.
      \ See https://tinyurl.com/toolflownet for supplementary material."
  authors: "Daniel Seita, Yufei Wang\u2020, Sarthak J Shetty\u2020, Edward Yao Li\u2020\
    , Zackory Erickson, David Held"
  award: null
  bib: >
    @inproceedings{Seita2022toolflownet,\n
        title={{ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds}},\n
        author={Seita, Daniel and Wang, Yufei and Shetty, Sarthak, and Li, Edward and Erickson, Zackory and Held, David},\n
        booktitle={Conference on Robot Learning (CoRL)},\n
        year={2022}\n
      }
  img: ../pics/toolflownet.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2211.09006
    '[Poster]': https://docs.google.com/presentation/d/1CqSfyEfcakvNYrZblt4KmScOub06qmN6jRar8YlPHNM/edit?usp=sharing
    '[Talk]': https://docs.google.com/presentation/d/1IAFnXzlVvSO3GXlX7NfAwVPhl77YgbBnGpkdN6WjIPk/edit?usp=sharing
  short_id: 2022corltoolflownet
  site: https://sites.google.com/view/point-cloud-policy
  title: 'ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from
    Point Clouds'
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/cJbtqOAGyI4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
- abs: "Effective planning of long-horizon deformable object manipulation requires\
    \ suitable \n                    abstractions at both the spatial and temporal\
    \ levels. \n                    Previous methods typically either focus on short-horizon\
    \ tasks or make \n                    strong assumptions that full-state information\
    \ is available, which prevents \n                    their use on deformable objects.\
    \ In this paper, we propose PlAnning with \n                    Spatial-Temporal\
    \ Abstraction (PASTA), which incorporates both spatial abstraction \n        \
    \            (reasoning about objects and their relations to each other) and temporal\
    \ \n                    abstraction (reasoning over skills instead of low-level\
    \ actions). Our framework \n                    maps high-dimension 3D observations\
    \ such as point clouds into a set of latent \n                    vectors and\
    \ plans over skill sequences on top of the latent set representation. \n     \
    \               We show that our method can effectively perform  challenging sequential\
    \ deformable \n                    object manipulation tasks in the real world,\
    \ which require combining multiple \n                    tool-use skills such\
    \ as cutting with a knife, pushing with a pusher, and spreading \n           \
    \         dough with a roller."
  authors: Xingyu Lin*, Carl Qi*, Yunchu Zhang, Zhiao Huang, Katerina Fragkiadaki,
    Yunzhu Li, Chuang Gan, David Held
  award: null
  bib: >
    @inproceedings{\n
    \ lin2022planning,\n
        title={Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation},\n
        author={Xingyu Lin and Carl Qi and Yunchu Zhang and Yunzhu Li and Zhiao Huang and Katerina Fragkiadaki and Chuang Gan and David Held},\n
        booktitle={6th Annual Conference on Robot Learning},\n
        year={2022},\n
        url={https://openreview.net/forum?id=tyxyBj2w4vw}\n
      }
  img: ../pics/2022_pasta.gif
  links:
    '[OpenReview]': https://openreview.net/forum?id=tyxyBj2w4vw
    '[PDF]': https://arxiv.org/pdf/2210.15751.pdf
    '[Code]': https://github.com/Xingyu-Lin/PASTA
  short_id: 2022corlpasta
  site: https://sites.google.com/view/pasta-plan
  title: <a href="https://sites.google.com/view/pasta-plan">Planning with Spatial-Temporal
    Abstraction from Point Clouds for Deformable Object Manipulation</a>
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/ZPV_bMeszWU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - affordance
    - deformable
- abs: "Orientation estimation is the core to a variety of vision and robotics tasks such as camera and object pose estimation. Deep learning has offered a way
    \ to develop image-based orientation estimators; however, such estimators often require training on a large labeled dataset, which can be time-intensive
    \ to collect. In this work, we explore whether self-supervised learning from unlabeled data can be used to alleviate this issue. Specifically, we assume
    \ access to estimates of the relative orientation between neighboring poses, such that can be obtained via a local alignment method. While self-supervised
    \ learning has been used successfully for translational object keypoints, in this work, we show that naively applying relative supervision to the rotational
    \ group SO(3) will often fail to converge due to the non-convexity of the rotational space. To tackle this challenge, we propose a new algorithm for
    \ self-supervised orientation estimation which utilizes Modified Rodrigues Parameters to stereographically project the closed manifold of SO(3)
    \ to the open manifold of R3, allowing the optimization to be done in an open Euclidean space. We empirically validate the benefits of the proposed algorithm
    \ for rotational averaging problem in two settings: (1) direct optimization on rotation parameters, and (2) optimization of parameters of a convolutional
    \ neural network that predicts object orientations from images. In both settings, we demonstrate that our proposed algorithm is able to converge to a
    \ consistent relative orientation frame much faster than algorithms that purely operate in the SO(3) space."
  authors: Brian Okorn*, Chuer Pan*, Martial Hebert, David Held
  award: null
  bib: >
    @inproceedings{okorndeep,\n
          title={Deep Projective Rotation Estimation through Relative Supervision},\n
          author={Okorn, Brian and Pan, Chuer and Hebert, Martial and Held, David},\n
          booktitle={Conference on Robot Learning (CoRL)}\n
          year={2022},\n
        }
  img: ../pics/mrp_website_text.png
  links:
    '[arXiv]': https://arxiv.org/pdf/2211.11182.pdf
  short_id: 2022corlmrp
  site: https://sites.google.com/view/deep-projective-rotation/home
  title: Deep Projective Rotation Estimation through Relative Supervision
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: null
  authors: Tarasha Khurana*, Peiyun Hu*, Achal Dave, Jason Ziglar, David Held, Deva
    Ramanan
  award: null
  bib: null
  img: ../pics/new_splash.png
  links: {}
  short_id: null
  site: null
  title: Differentiable Raycasting for Self-supervised Occupancy Forecasting
  venue: European Conference on Computer Vision (ECCV), 2022
  video_embed: null
  tags:
    - autonomous-driving
    - self-supervised
- abs: Robotic manipulation of cloth has applications ranging from fabrics manufacturing
    to handling blankets and laundry. Cloth manipulation is challenging for robots
    largely due to their high degrees of freedom, complex dynamics, and severe self-occlusions
    when in folded or crumpled configurations. Prior work on robotic manipulation
    of cloth relies primarily on vision sensors alone, which may pose challenges for
    fine-grained manipulation tasks such as grasping a desired number of cloth layers
    from a stack of cloth. In this paper, we propose to use tactile sensing for cloth
    manipulation; we attach a tactile sensor (ReSkin) to one of the two fingertips
    of a Franka robot and train a classifier to determine whether the robot is grasping
    a specific number of cloth layers. During test-time experiments, the robot uses
    this classifier as part of its policy to grasp one or two cloth layers using tactile
    feedback to determine suitable grasping points. Experimental results over 180
    physical trials suggest that the proposed method outperforms baselines that do
    not use tactile feedback and has a better generalization to unseen fabrics compared
    to methods that use image classifiers.
  authors: Sashank Tirumala*, Thomas Weng*, Daniel Seita*, Oliver Kroemer, Zeynep
    Temel, David Held
  award: <award>Best Paper at <a href="https://romado-workshop.github.io/ROMADO2022/">ROMADO-SI</a></award>
  bib: >
    @inproceedings{tirumala2022reskin,
      author={Tirumala, Sashank and Weng, Thomas and Seita, Daniel and Kroemer, Oliver and Temel, Zeynep and Held, David},
      booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      title={Learning to Singulate Layers of Cloth using Tactile Feedback},
      year={2022},
      volume={},
      number={},
      pages={7773-7780},
      doi={10.1109/IROS47612.2022.9981341}
    }
  img: ../pics/iros2022-singulating-layers-fabric-reskin-400x210.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2207.11196
    '[Code]': https://github.com/DanielTakeshi/cloth_reskin
  short_id: 2022reskinfabric
  site: https://sites.google.com/view/reskin-cloth/home
  title: Learning to Singulate Layers of Cloth based on Tactile Feedback
  venue: International Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/FTGbx1XDjJ0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - multimodal
- abs: Deformable object manipulation has many applications such as cooking and laundry
    folding in our daily lives. Manipulating elastoplastic objects such as dough is
    particularly challenging because dough lacks a compact state representation and
    requires contact-rich interactions. We consider the task of flattening a piece
    of dough into a specific shape from RGB-D images. While the task is seemingly
    intuitive for humans, there exist local optima for common approaches such as naive
    trajectory optimization. We propose a novel trajectory optimizer that optimizes
    through a differentiable "reset" module, transforming a single-stage, fixed-initialization
    trajectory into a multistage, multi-initialization trajectory where all stages
    are optimized jointly. We then train a closed-loop policy on the demonstrations
    generated by our trajectory optimizer. Our policy receives partial point clouds
    as input, allowing ease of transfer from simulation to the real world. We show
    that our policy can perform real-world dough manipulation, flattening a ball of
    dough into a target shape.
  authors: Carl Qi, Xingyu Lin, David Held
  award: null
  bib: >
    @article{qi2022dough, \nauthor={Qi, Carl and Lin, Xingyu and Held, David},\n\
    journal={IEEE Robotics and Automation Letters}, \ntitle={Learning Closed-Loop\
    \ Dough Manipulation Using a Differentiable Reset Module}, \nyear={2022},\nvolume={7},\n\
    number={4},\npages={9857-9864},\ndoi={10.1109/LRA.2022.3191239}}"
  img: ../pics/carl-22-website.gif
  links:
    '[Talk]': https://drive.google.com/file/d/1ai4tJeHNoZKD8wavViWj3auMPZdblZRR/view?usp=sharing
    '[PDF]': https://arxiv.org/pdf/2207.04638.pdf
  short_id: 2022raldough
  site: https://sites.google.com/view/dough-manipulation
  title: <a href="https://sites.google.com/view/dough-manipulation">Learning Closed-loop
    Dough Manipulation using a Differentiable Reset Module</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/o6Dx6ZLYo44" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
- abs: Robotic manipulation of highly deformable cloth presents a promising opportunity
    to assist people with several daily tasks, such as washing dishes; folding laundry;
    or dressing, bathing, and hygiene assistance for individuals with severe motor
    impairments. In this work, we introduce a formulation that enables a collaborative
    robot to perform visual haptic reasoning with cloth -- the act of inferring the
    location and magnitude of applied forces during physical interaction. We present
    two distinct model representations, trained in physics simulation, that enable
    haptic reasoning using only visual and robot kinematic observations. We conducted
    quantitative evaluations of these models in simulation for robot-assisted dressing,
    bathing, and dish washing tasks, and demonstrate that the trained models can generalize
    across different tasks with varying interactions, human body sizes, and object
    shapes.  We also present results with a real-world mobile manipulator, which used
    our simulation-trained models to estimate applied contact forces while performing
    physically assistive tasks with cloth.
  authors: Yufei Wang, David Held, Zackory Erickson
  award: null
  bib: >
    @article{wang2022visual,
      title={Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable Object Interactions},
      author={Wang, Yufei and Held, David and Erickson, Zackory},
      journal={IEEE Robotics and Automation Letters},
      volume={7},
      number={4},
      pages={11426--11433},
      year={2022},
      publisher={IEEE}
    }
  img: ../pics/visual-haptic-reasoning-IROS.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2208.05632.pdf
    '[Talk]': https://www.youtube.com/watch?v=Qgjdt7vAHCQ
  short_id: 2022vhr
  site: https://sites.google.com/view/visualhapticreasoning/home
  title: 'Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable
    Object Interactions'
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/hXAgi1HYDuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - affordance
- abs: We explore a novel method to perceive and manipulate 3D articulated objects
    that generalizes to enable a robot to articulate unseen classes of objects. We
    propose a vision-based system that learns to predict the potential motions of
    the parts of a variety of articulated objects to guide downstream motion planning
    of the system to articulate the objects. To predict the object motions, we train
    a neural network to output a dense vector field representing the point-wise motion
    direction of the points in the point cloud under articulation. We then deploy
    an analytical motion planner based on this vector field to achieve a policy that
    yields maximum articulation. We train the vision system entirely in simulation,
    and we demonstrate the capability of our system to generalize to unseen object
    instances and novel categories in both simulation and the real world, deploying
    our policy on a Sawyer robot with no finetuning. Results show that our system
    achieves state-of-the-art performance in both simulated and real-world experiments.
  authors: Ben Eisner*, Harry Zhang*, David Held
  award: <award>Best Paper Finalist (Selection Rate 1.5%)</award>
  bib: >
    @inproceedings{EisnerZhang2022FLOW,\n            title={FlowBot3D: Learning\
    \ 3D Articulation Flow to Manipulate Articulated Objects},\n            author={Eisner*,\
    \ Ben and Zhang*, Harry and Held,David},\n            booktitle={Robotics: Science\
    \ and Systems (RSS)},\n            year={2022}\n       }"
  img: ../pics/montage.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2205.04382.pdf
  short_id: 2022flowbot3D
  site: https://sites.google.com/view/articulated-flowbot-3d/home#h.xrrj1zv4o44a
  title: '<a href="https://sites.google.com/view/articulated-flowbot-3d/home#h.xrrj1zv4o44a">FlowBot3D:
    Learning 3D Articulation Flow to Manipulate Articulated Objects</a>'
  venue: 'Robotics: Science and Systems (RSS), 2022'
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/ZxRS8WfZOMg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - affordance
- abs: "Self-occlusion is challenging for cloth manipulation, as it makes it difficult\
    \ to estimate the full state of the cloth. Ideally, a robot trying to unfold a\
    \ crumpled or folded cloth should be able to reason about the cloth's occluded\
    \ regions.\n                    We leverage recent advances in pose estimation\
    \ for cloth to build a system that uses explicit occlusion reasoning to unfold\
    \ a crumpled cloth. Specifically, we first learn a model to reconstruct the mesh\
    \ of the cloth. However, the model will likely have errors due to the complexities\
    \ of the cloth configurations and due to ambiguities from occlusions.  Our main\
    \ insight is that we can further refine the predicted reconstruction by performing\
    \ test-time finetuning with self-supervised losses. The obtained reconstructed\
    \ mesh allows us to use a mesh-based dynamics model for planning while reasoning\
    \ about occlusions. We evaluate our system both on cloth flattening as well as\
    \ on  cloth canonicalization, in which the objective is to manipulate the cloth\
    \ into a canonical pose. Our experiments show that our method significantly outperforms\
    \ prior methods that do not explicitly account for occlusions or perform test-time\
    \ optimization."
  authors: Zixuan Huang, Xingyu Lin, David Held
  award: null
  bib: >
    @inproceedings{huang2022medor,\n            title={Mesh-based Dynamics Model\
    \ with Occlusion Reasoning for Cloth Manipulation},\n            author={Huang,\
    \ Zixuan and Lin, Xingyu and Held,David},\n            booktitle={Robotics: Science\
    \ and Systems (RSS)},\n            year={2022}\n       }"
  img: ../pics/rss2022medor.gif
  links:
    '[PDF]': https://arxiv.org/abs/2206.02881
    '[Poster]': https://drive.google.com/file/d/1xdMVN7moNcdIeAPuPP0tpdvtjIfeFWdZ/view?usp=sharing
    '[Video]': https://youtu.be/0s9PA6EgiqE
  short_id: 2022occnet
  site: https://sites.google.com/view/occlusion-reason/home
  title: <a href="https://sites.google.com/view/occlusion-reason/home">Mesh-based
    Dynamics with Occlusion Reasoning for Cloth Manipulation</a>
  venue: 'Robotics: Science and Systems (RSS), 2022'
  video_embed: null
  tags:
    - deformable
- abs: 'We consider the problem of sequential robotic manipulation of deformable objects
    using tools.


    Previous works have shown that differentiable physics simulators provide gradients
    to the environment state and help trajectory optimization to converge orders of
    magnitude faster than model-free reinforcement learning algorithms for deformable
    object manipulations. However, such gradient-based trajectory optimization typically
    requires access to the full simulator states and can only solve short-horizon,
    single-skill tasks due to local optima. In this work, we propose a novel framework,
    named DiffSkill, that uses a differentiable physics simulator for skill abstraction
    to solve long-horizon deformable object manipulation tasks from sensory observations.
    In particular, we first obtain short-horizon skills for using each individual
    tool from a gradient-based optimizer and then learn a neural skill abstractor
    from the demonstration videos; Finally, we plan over the skills to solve the long-horizon
    task. We show the advantages of our method in a new set of sequential deformable
    object manipulation tasks over previous reinforcement learning algorithms and
    the trajectory optimizer.'
  authors: Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum, David Held, Chuang
    Gan
  award: null
  bib: '@inproceedings{


    lin2022diffskill,

    title={DiffSkill: Skill Abstraction from Differentiable Physics for Deformable
    Object Manipulations with Tools},

    author={Xingyu Lin and Zhiao Huang and Yunzhu Li and David Held and Joshua B.
    Tenenbaum and Chuang Gan},

    booktitle={International Conference on Learning Representations},

    year={2022},

    url={https://openreview.net/forum?id=Kef8cKdHWpP}}'
  img: ../pics/2022_diffskill.gif
  links:
    '[PDF]': https://openreview.net/pdf?id=Kef8cKdHWpP
  short_id: 2022diffskill
  site: https://xingyu-lin.github.io/diffskill/
  title: '<a href="https://sites.google.com/view/iclr2022diffskill">DiffSkill: Skill
    Abstraction from Differentiable Physics for Deformable Object Manipulations with
    Tools</a>'
  venue: International Conference on Learning Representations (ICLR), 2022
  video_embed: null
  tags:
    - deformable
- abs: Liquid state estimation is important for robotics tasks such as pouring; however,
    estimating the state of transparent liquids is a challenging problem. We propose
    a novel segmentation pipeline that can segment transparent liquids such as water
    from a static, RGB image without requiring any manual annotations or heating of
    the liquid for training. Instead, we use a generative model that is capable of
    translating images of colored liquids into synthetically generated transparent
    liquid images, trained only on an unpaired dataset of colored and transparent
    liquid images. Segmentation labels of colored liquids are obtained automatically
    using background subtraction. Our experiments show that we are able to accurately
    predict a segmentation mask for transparent liquids without requiring any manual
    annotations. We demonstrate the utility of transparent liquid segmentation in
    a robotic pouring task that controls pouring by perceiving the liquid height in
    a transparent cup. Accompanying video and supplementary materials can be found
    on our project page.
  authors: Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held
  award: null
  bib: '@inproceedings{icra2022pouring,


    title={Self-supervised Transparent Liquid Segmentation for Robotic Pouring},

    author={Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held},

    booktitle={International Conference on Robotics and Automation (ICRA)},

    year={2022}}'
  img: ../pics/pouring.gif
  links:
    '[Code]': https://github.com/gauthamnarayan/transparent-liquid-segmentation
    '[PDF]': https://arxiv.org/pdf/2203.01538.pdf
    '[Poster]': https://docs.google.com/presentation/d/1kOdSIgoGPlg1CmRILjsdwIpTJIMpfgjY/edit?usp=sharing&ouid=115079833191742902358&rtpof=true&sd=true
    '[Slides]': https://drive.google.com/file/d/1cG9ZDGekFVLKLqSS4ZSlS445Lcxwm08h/view?usp=sharing
    '[Video]': https://www.youtube.com/watch?v=uXGCSd3KVd8
  short_id: 2022pouring
  site: https://sites.google.com/view/transparentliquidpouring
  title: <a href="https://sites.google.com/view/transparentliquidpouring">Self-supervised
    Transparent Liquid Segmentation for Robotic Pouring</a>
  venue: International Conference of Robotics and Automation (ICRA), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/BkAxNCvYmM0" title="Robotic pouring from empty to 75% of the cup height." frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - self-supervised
- abs: Real-time object pose estimation is necessary for many robot manipulation algorithms.
    However, state-of-the-art methods for object pose estimation are trained for a
    specific set of objects; these methods thus need to be retrained to estimate the
    pose of each new object, often requiring tens of GPU-days of training for optimal
    performance. In this paper, we propose the OSSID framework, leveraging a slow
    zero-shot pose estimator to self-supervise the training of a fast detection algorithm.
    This fast detector can then be used to filter the input to the pose estimator,
    drastically improving its inference speed. We show that this self-supervised training
    exceeds the performance of existing zero-shot detection methods on two widely
    used object pose estimation and detection datasets, without requiring any human
    annotations. Further, we show that the resulting method for pose estimation has
    a significantly faster inference speed, due to the ability to filter out large
    parts of the image. Thus, our method for self-supervised online learning of a
    detector (trained using pseudo-labels from a slow pose estimator) leads to accurate
    pose estimation at real-time speeds, without requiring human annotations.
  authors: Qiao Gu, Brian Okorn, David Held
  award: null
  bib: >
    @article{ral2022ossid,\n  author={Gu, Qiao and Okorn, Brian and Held, David},\n\
        journal={IEEE Robotics and Automation Letters}, \n  title={OSSID: Online Self-Supervised\
    \ Instance Detection by (And For) Pose Estimation}, \n  year={2022},\n  volume={7},\n\
    \  number={2},\n  pages={3022-3029},\n  doi={10.1109/LRA.2022.3145488}}"
  img: ../pics/ossid.jpg
  links:
    '[Code]': https://github.com/r-pad/OSSID_code
    '[PDF]': https://arxiv.org/pdf/2201.07309.pdf
    '[Video]': https://www.youtube.com/watch?v=S_pU2FbMN8k
  short_id: 2022ossid
  site: https://georgegu1997.github.io/OSSID/
  title: '<a href="https://arxiv.org/abs/2201.07309">OSSID: Online Self-Supervised
    Instance Detection by (and for) Pose Estimation</a>'
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2022
  video_embed: null
  tags:
    - self-supervised
- abs: When navigating in urban environments, many of the objects that need to be
    tracked and avoided are heavily occluded. Planning and tracking using these partial
    scans can be challenging. The aim of this work is to learn to complete these partial
    point clouds, giving us a full understanding of the object's geometry using only
    partial observations. Previous methods achieve this with the help of complete,
    ground-truth annotations of the target objects, which are available only for simulated
    datasets. However, such ground truth is unavailable for real-world LiDAR data.
    In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet,
    which is trained only on partial scans without assuming access to complete, ground-truth
    annotations. Our method achieves this via inpainting. We remove a portion of the
    input data and train the network to complete the missing region. As it is difficult
    to determine which regions were occluded in the initial cloud and which were synthetically
    removed, our network learns to complete the full cloud, including the missing
    regions in the initial partial cloud. We show that our method outperforms previous
    unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet,
    and real-world LiDAR dataset, Semantic KITTI.
  authors: Himangi Mittal, Brian Okorn, Arpit Jangid, David Held
  award: <award>Oral presentation</award> (Selection rate 3.3%)
  bib: >
    @article{mittal2021self,\n  title={Self-Supervised Point Cloud Completion\
    \ via Inpainting},\n  author={Mittal, Himangi and Okorn, Brian and Jangid, Arpit\
    \ and Held, David},\n  journal={British Machine Vision Conference (BMVC), 2021},\n\
        year={2021}\n}"
  img: ../pics/bmvc2-2021.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2111.10701.pdf
    '[Video]': https://www.bmvc2021-virtualconference.com/conference/papers/paper_0443.html
  short_id: 2021bmvc
  site: https://self-supervised-completion-inpainting.github.io/
  title: <a href="https://arxiv.org/abs/2111.10701">Self-Supervised Point Cloud Completion
    via Inpainting</a>
  venue: British Machine Vision Conference (BMVC), 2021
  video_embed: null
  tags:
    - self-supervised
- abs: "Benchmarks offer a scientific way to compare algorithms using scientific performance\
    \ metrics. Good benchmarks have two features: (a) wide audience appeal; (b) easily\
    \ reproducible. In robotics, there is a tradeoff between reproducibility and broad\
    \ accessibility. If the benchmark is kept restrictive (fixed hardware, objects),\
    \ the numbers are reproducible but it becomes niche. On the other hand, benchmark\
    \ could be just loose set of protocols but the underlying varying setups make\
    \ it hard to reproduce the results. In this paper, we re-imagine robotics benchmarks\
    \ \u2013 we define a robotics benchmark to be a set of experimental protocols\
    \ and state of the art algorithmic implementations. These algorithm implementations\
    \ will provide a way to recreate baseline numbers in a new local robotic setup\
    \ in less than few hours and hence help provide credible relative rankings between\
    \ different approaches. These credible local rankings are pooled from several\
    \ locations to help establish global rankings and SOTA algorithms that work across\
    \ majority of setups. We introduce RB2 \u2014 a benchmark inspired from human\
    \ SHAP tests. Our benchmark was run across three different labs and reveals several\
    \ surprising findings."
  authors: Sudeep Dasari, Jianren Wang, Joyce Hong, Shikhar Bahl, Yixin Lin, Austin
    S Wang, Abitha Thankaraj, Karanbir Singh Chahal, Berk Calli, Saurabh Gupta, David
    Held, Lerrel Pinto, Deepak Pathak, Vikash Kumar, Abhinav Gupta
  award: null
  bib: null
  img: ../pics/rb2b.png
  links: {}
  short_id: div2021rb2
  site: https://agi-labs.github.io/rb2/
  title: '<a href="https://openreview.net/forum?id=e82_BlJL43M">RB2: Robotic Manipulation
    Benchmarking with a Twist</a>'
  venue: NeurIPS 2021 Datasets and Benchmarks Track, 2021
  video_embed: null
- abs: 3D object detection plays an important role in autonomous driving and other
    robotics applications. However, these detectors usually require training on large
    amounts of annotated data that is expensive and time-consuming to collect. Instead,
    we propose leveraging large amounts of unlabeled point cloud videos by semi-supervised
    learning of 3D object detectors via temporal graph neural networks. Our insight
    is that temporal smoothing can create more accurate detection results on unlabeled
    data, and these smoothed detections can then be used to retrain the detector.
    We learn to perform this temporal reasoning with a graph neural network, where
    edges represent the relationship between candidate detections in different time
    frames.
  authors: Jianren Wang, Haiming Gang, Siddharth Ancha, Yi-ting Chen, and David Held
  award: null
  bib: '@article{wang2021sodtgnn,

    title={Semi-supervised 3D Object Detection via Temporal Graph Neural Networks},

    author={Wang, Jianren and Gang, Haiming and Ancha, Siddharth and Chen, Yi-ting
    and Held, David},

    journal={International Conference on 3D Vision (3DV)},

    year={2021}}'
  img: ../pics/sod_tgnn.jpg
  links:
    '[Code]': https://github.com/jianrenw/SOD-TGNN
    '[Video (Long)]': https://youtu.be/L8X4LCEpCaE
    '[Video (Short)]': https://youtu.be/BhW7m3R_Yqo
  short_id: div20213dV
  site: https://www.jianrenw.com/SOD-TGNN/
  title: <a href="https://arxiv.org/pdf/2202.00182.pdf">Semi-supervised 3D Object
    Detection via Temporal Graph Neural Networks</a>
  venue: International Conference on 3D Vision (3DV), 2021
  video_embed: null
  tags:
    - autonomous-driving
- abs: Robotic manipulation of cloth remains challenging for robotics due to the complex
    dynamics of the cloth, lack of a low-dimensional state representation, and self-occlusions.
    In contrast to previous model-based approaches that learn a pixel-based dynamics
    model or a compressed latent vector dynamics, we propose to learn a particle-based
    dynamics model from a partial point cloud observation. To overcome the challenges
    of partial observability, we infer which visible points are connected on the underlying
    cloth mesh. We then learn a dynamics model over this visible connectivity graph.
    Compared to previous learning-based approaches, our model poses strong inductive
    bias with its particle based representation for learning the underlying cloth
    physics; it is invariant to visual features; and the predictions can be more easily
    visualized. We show that our method greatly outperforms previous state-of-the-art
    model-based and model-free reinforcement learning methods in simulation. Furthermore,
    we demonstrate zero-shot sim-to-real transfer where we deploy the model trained
    in simulation on a Franka arm and show that the model can successfully smooth
    different types of cloth from crumpled configurations. Videos can be found on
    our project website.
  authors: Xingyu Lin*, Yufei Wang*, Zixuan Huang, David Held
  award: null
  bib: '@inproceedings{lin2021VCD,

    title={Learning Visible Connectivity Dynamics for Cloth Smoothing},

    author={Lin, Xingyu and Wang, Yufei and Huang, Zixuan and Held, David},

    booktitle={Conference on Robot Learning},

    year={2021}}'
  img: ../pics/vcd.gif
  links:
    '[OpenReview]': https://openreview.net/forum?id=n1hDe9iK6ms
    '[PDF]': https://arxiv.org/pdf/2105.10389.pdf
    '[Code]': https://github.com/Xingyu-Lin/VCD
  short_id: div2021vcd
  site: https://sites.google.com/view/vcd-cloth
  title: <a href="https://arxiv.org/abs/2105.10389">Learning Visible Connectivity
    Dynamics for Cloth Smoothing</a>
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/QgRrHeKylhs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - affordance
- abs: We address the problem of goal-directed cloth manipulation, a challenging task
    due to the deformability of cloth. Our insight is that optical flow, a technique
    normally used for motion estimation in video, can also provide an effective representation
    for corresponding cloth poses across observation and goal images. We introduce
    FabricFlowNet (FFN), a cloth manipulation policy that leverages flow as both an
    input and as an action representation to improve performance. FabricFlowNet also
    elegantly switches between dual-arm and single-arm actions based on the desired
    goal. We show that FabricFlowNet significantly outperforms state-of-the-art model-free
    and model-based cloth manipulation policies. We also present real-world experiments
    on a bimanual system, demonstrating effective sim-to-real transfer. Finally, we
    show that our method generalizes when trained on a single square cloth to other
    cloth shapes, such as T-shirts and rectangular cloths.
  authors: Thomas Weng, Sujay Bajracharya, Yufei Wang, David Held
  award: null
  bib: >
    @inproceedings{weng2021fabricflownet,\n title={FabricFlowNet: Bimanual Cloth\
    \ Manipulation \n    with a Flow-based Policy},\n author={Weng, Thomas and Bajracharya,\
    \ Sujay and \n    Wang, Yufei and Agrawal, Khush and Held, David},\n booktitle={Conference\
    \ on Robot Learning},\n year={2021}\n}"
  img: ../pics/ffn.gif
  links:
    '[Code]': https://github.com/thomasweng15/FabricFlowNet
    '[OpenReview]': https://openreview.net/forum?id=TsqkJJMgHkk
    '[PDF]': https://arxiv.org/pdf/2111.05623.pdf
    '[Poster]': https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fattachment%3Fid%3DTsqkJJMgHkk%26name%3Dposter&sa=D&sntz=1&usg=AFQjCNH3PPtQ_vMMk00WEMRXG28cWK3ylQ
  short_id: weng2021fabricflownet
  site: https://sites.google.com/view/fabricflownet
  title: '<a href="https://arxiv.org/abs/2111.05623">FabricFlowNet: Bimanual Cloth
    Manipulation with a Flow-based Policy</a>'
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/r68EEzzj_Zc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
- abs: Reinforcement learning (RL) in low-data and risk-sensitive domains requires
    performant and flexible deployment policies that can readily incorporate constraints
    during deployment. One such class of policies are the semi-parametric H-step lookahead
    policies, which select actions using trajectory optimization over a dynamics model
    for a fixed horizon with a terminal value function. In this work, we investigate
    a novel instantiation of H-step lookahead with a learned model and a terminal
    value function learned by a model-free off-policy algorithm, named Learning Off-Policy
    with Online Planning (LOOP). We provide a theoretical analysis of this method,
    suggesting a tradeoff between model errors and value function errors and empirically
    demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore,
    we identify the "Actor Divergence" issue in this framework and propose Actor Regularized
    Control (ARC), a modified trajectory optimization procedure. We evaluate our method
    on a set of robotic tasks for Offline and Online RL and demonstrate improved performance.
    We also show the flexibility of LOOP to incorporate safety constraints during
    deployment with a set of navigation environments. We demonstrate that LOOP is
    a desirable framework for robotics applications based on its strong performance
    in various important RL settings.
  authors: Harshit Sikchi, Wenxuan Zhou, David Held
  award: <award>Oral presentation</award> (Selection rate 6.5%); <award>Best Paper
    Finalist</award>
  bib: '@inproceedings{sikchi2021learning,

    title={Learning Off-policy for Online Planning},

    author={Sikchi, Harshit and Zhou, Wenxuan and Held, David},

    booktitle={Conference on Robot Learning},

    year={2021}}'
  img: ../pics/loop_overview.jpeg
  links:
    '[Code]': https://github.com/hari-sikchi/LOOP
    '[OpenReview]': https://openreview.net/forum?id=1GNV9SW95eJ
    '[PDF]': https://arxiv.org/pdf/2008.10066.pdf
    '[Talk]': https://youtu.be/zYDwnj_ghZM
  short_id: sikchi2021learning
  site: https://hari-sikchi.github.io/loop/
  title: <a href="https://arxiv.org/abs/2008.10066">Learning Off-policy for Online
    Planning</a>
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: null
  tags:
    - rl
- abs: 'To safely navigate unknown environments, robots must accurately perceive dynamic
    obstacles. Instead of directly measuring the scene depth with a LiDAR sensor,
    we explore the use of a much cheaper and higher resolution sensor:'
  authors: Siddharth Ancha, Gaurav Pathak, Srinivasa Narasimhan, David Held
  award: null
  bib: >
    @inproceedings{Ancha-RSS-21,\n    AUTHOR    = {Siddharth Ancha AND Gaurav\
    \ Pathak AND Srinivasa G. Narasimhan AND David Held},\n    TITLE     = {Active\
    \ Safety Envelopes using Light Curtains with Probabilistic Guarantees},\n    BOOKTITLE\
    \ = {Proceedings of Robotics: Science and Systems},\n    YEAR      = {2021},\n\
    \    MONTH     = {July}\n}"
  img: ../pics/rss2021lightcurtains.png
  links:
    '[Blog]': https://blog.ml.cmu.edu/2021/11/19/active-safety-envelopes-using-light-curtains-with-probabilistic-guarantees/
    '[Code]': https://github.com/CMU-Light-Curtains/SafetyEnvelopes
    '[PDF]': https://arxiv.org/pdf/2107.04000.pdf
    '[Poster]': https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees/docs/poster.png
    '[Talk]': https://www.youtube.com/watch?v=1PUAjzcTz5g
  short_id: 2021safetyenvelopes
  site: https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees
  title: <a href="https://arxiv.org/abs/2107.04000">Active Safety Envelopes using
    Light Curtains with Probabilistic Guarantees</a>
  venue: 'Robotics: Science and Systems (RSS), 2021'
  video_embed: null
  tags:
    - autonomous-driving
    - active-perception
- abs: Pose estimation is a basic module in many robot manipulation pipelines. Estimating
    the pose of objects in the environment can be useful for grasping, motion planning,
    or manipulation. However, current state-of-the-art methods for pose estimation
    either rely on large annotated training sets or simulated data. Further, the long
    training times for these methods prohibit quick interaction with novel objects.
    To address these issues, we introduce a novel method for zero-shot object pose
    estimation in clutter. Our approach uses a hypothesis generation and scoring framework,
    with a focus on learning a scoring function that generalizes to objects not used
    for training. We achieve zero-shot generalization by rating hypotheses as a function
    of unordered point differences. We evaluate our method on challenging datasets
    with both textured and untextured objects in cluttered scenes and demonstrate
    that our method significantly outperforms previous methods on this task. We also
    demonstrate how our system can be used by quickly scanning and building a model
    of a novel object, which can immediately be used by our method for pose estimation.
    Our work allows users to estimate the pose of novel objects without requiring
    any retraining.
  authors: Brian Okorn*, Qiao Gu*, Martial Hebert, David Held
  award: null
  bib: >
    @inproceedings{okorn2021zephyr,\n                    title={Zephyr: Zero-shot\
    \ pose hypothesis rating},\n                    author={Okorn, Brian and Gu, Qiao\
    \ and Hebert, Martial and Held, David},\n                    booktitle={2021 IEEE\
    \ International Conference on Robotics and Automation (ICRA)},\n             \
    \       pages={14141--14148},\n                    year={2021},\n            \
    \        organization={IEEE}\n                  }"
  img: ../pics/zephyr2.png
  links:
    '[Code]': https://github.com/r-pad/zephyr
    '[PDF]': https://arxiv.org/pdf/2104.13526.pdf
  short_id: 2021zephyr
  site: https://bokorn.github.io/zephyr/
  title: '<a href="https://arxiv.org/abs/2104.13526">ZePHyR: Zero-shot Pose Hypothesis
    Rating</a>'
  venue: International Conference of Robotics and Automation (ICRA), 2021
  video_embed: null
- abs: Active sensing through the use of Adaptive Depth Sensors is a nascent field,
    with potential in areas such as Advanced driver-assistance systems (ADAS). They
    do however require dynamically driving a laser / light-source to a specific location
    to capture information, with one such class of sensor being the Triangulation
    Light Curtains (LC). In this work, we introduce a novel approach that exploits
    prior depth distributions from RGB cameras to drive a Light Curtain's laser line
    to regions of uncertainty to get new measurements. These measurements are utilized
    such that depth uncertainty is reduced and errors get corrected recursively. We
    show real-world experiments that validate our approach in outdoor and driving
    settings, and demonstrate qualitative and quantitative improvements in depth RMSE
    when RGB cameras are used in tandem with a Light Curtain.
  authors: Yaadhav Raaj, Siddharth Ancha, Robert Tamburo, David Held, Srinivasa Narasimhan
  award: null
  bib: >
    @inproceedings{cvpr2021raajexploiting,\n    title     = {Exploiting & Refining\
    \ Depth Distributions with Triangulation Light Curtains},\n    author    = {Yaadhav\
    \ Raaj, Siddharth Ancha, Robert Tamburo, David Held, Srinivasa Narasimhan},\n\
    \    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and\
    \ Pattern Recognition (CVPR)},\n    year      = {2021}\n}"
  img: ../pics/cvpr21.png
  links:
    '[Code]': https://github.com/CMU-Light-Curtains/DepthEstimation
    '[PDF]': https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf
    '[Talk]': https://youtu.be/kIjn3U8luV0
  short_id: 2021exploiting
  site: https://soulslicer.github.io/rgb-lc-fusion/
  title: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf">Exploiting
    &amp; Refining Depth Distributions with Triangulation Light Curtains</a>
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2021
  video_embed: null
  tags:
    - active-perception
- abs: Safe local motion planning for autonomous driving in dynamic environments requires
    forecasting how the scene evolves. Practical autonomy stacks adopt a semantic
    object-centric representation of a dynamic scene and build object detection, tracking,
    and prediction modules to solve forecasting. However, training these modules comes
    at an enormous human cost of manually annotated objects across frames. In this
    work, we explore future freespace as an alternative representation to support
    motion planning. Our key intuition is that it is important to avoid straying into
    occupied space regardless of what is occupying it. Importantly, computing ground-truth
    future freespace is annotation-free. First, we explore freespace forecasting as
    a self-supervised learning task. We then demonstrate how to use forecasted freespace
    to identify collision-prone plans from off-the-shelf motion planners. Finally,
    we propose future freespace as an additional source of annotation-free supervision.
    We demonstrate how to integrate such supervision into the learning process of
    learning-based planners. Experimental results on nuScenes and CARLA suggest both
    approaches lead to significant reduction in collision rates.
  authors: Peiyun Hu, Aaron Huang, John Dolan, David Held, Deva Ramanan
  award: null
  bib: >
    @inproceedings{cvpr2021husafe,\n                    title={Safe Local Motion\
    \ Planning with Self-Supervised Freespace Forecasting},\n                    author={Peiyun\
    \ Hu, Aaron Huang, John Dolan, David Held, Deva Ramanan},\n                  \
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition (CVPR)},\n                    year={2021}}"
  img: ../pics/peiyunff.gif
  links:
    '[Code]': https://github.com/peiyunh/ff
    '[Paper]': https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf
    '[Poster]': https://peiyunh.github.io/ff/poster.pdf
    '[Talk]': https://youtu.be/O_2MojWp7yk
  short_id: 2021cvpr_safe
  site: https://peiyunh.github.io/ff/index.html
  title: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf">Safe
    Local Motion Planning with Self-Supervised Freespace Forecasting</a>
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2021
  video_embed: null
  tags:
    - autonomous-driving
    - self-supervised
- abs: Manipulating deformable objects has long been a challenge in robotics due to
    its high dimensional state representation and complex dynamics. Recent success
    in deep reinforcement learning provides a promising direction for learning to
    manipulate deformable objects with data driven methods. However, existing reinforcement
    learning benchmarks only cover tasks with direct state observability and simple
    low-dimensional dynamics or with relatively simple image-based environments, such
    as those with rigid objects. In this paper, we present SoftGym, a set of open-source
    simulated benchmarks for manipulating deformable objects, with a standard OpenAI
    Gym API and a Python interface for creating new environments. Our benchmark will
    enable reproducible research in this important area. Further, we evaluate a variety
    of algorithms on these tasks and highlight challenges for reinforcement learning
    algorithms, including dealing with a state representation that has a high intrinsic
    dimensionality and is partially observable. The experiments and analysis indicate
    the strengths and limitations of existing methods in the context of deformable
    object manipulation that can help point the way forward for future methods development.
    Code and videos of the learned policies can be found on our project website.
  authors: Xingyu Lin, Yufei Wang, Jake Olkin, David Held
  award: null
  bib: '@inproceedings{corl2020softgym,

    title={SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object
    Manipulation},

    author={Lin, Xingyu and Wang, Yufei and Olkin, Jake and Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}}'
  img: ../pics/corl2020_softgym.png
  links:
    '[Code]': https://github.com/Xingyu-Lin/softgym
    '[PDF]': https://arxiv.org/pdf/2011.07215.pdf
  short_id: 2020softgym
  site: https://sites.google.com/view/softgym/home
  title: '<a href="https://arxiv.org/abs/2011.07215">SoftGym: Benchmarking Deep Reinforcement
    Learning for Deformable Object Manipulation</a>'
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: null
  tags:
    - deformable
- abs: "Current image-based reinforcement learning (RL) algorithms typically operate\
    \ on the whole image without performing object-level reasoning.  This leads to\
    \ inefficient goal sampling and ineffective reward functions. In this paper, we\
    \ improve upon previous visual self-supervised RL by incorporating object-level\
    \ reasoning and occlusion reasoning. Specifically, we use unknown object segmentation\
    \ to ignore distractors in the scene for better reward computation and goal generation;\
    \ we further enable occlusion reasoning by employing a novel auxiliary loss and\
    \ training scheme. We demonstrate that our proposed algorithm,   ROLL (Reinforcement\
    \ learning with Object Level Learning), learns dramatically faster and achieves\
    \ better final performance compared with previous methods in several simulated\
    \ visual control tasks. Project video and code\n                     are available\
    \ at https://sites.google.com/andrew.cmu.edu/roll."
  authors: Yufei Wang*, Gautham Narayan Narasimhan*, Xingyu Lin, Brian Okorn, David
    Held
  award: null
  bib: '@inproceedings{corl2020roll,

    title={ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning},

    author={Wang, Yufei and Narasimhan Gautham and Lin, Xingyu and Okorn, Brian and
    Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}

    }'
  img: ../pics/corl2020_ROLL.jpg
  links:
    '[Code]': https://github.com/yufeiwang63/ROLL
    '[PDF]': https://arxiv.org/pdf/2011.06777.pdf
  short_id: 2020roll
  site: https://sites.google.com/andrew.cmu.edu/roll/home
  title: <a href="https://arxiv.org/abs/2011.06777">Visual Self-Supervised Reinforcement
    Learning with Object Reasoning</a>
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: null
  tags:
    - self-supervised
- abs: The goal of offline reinforcement learning is to learn a policy from a fixed
    dataset, without further interactions with the environment. This setting will
    be an increasingly more important paradigm for real-world applications of reinforcement
    learning such as robotics, in which data collection is slow and potentially dangerous.
    Existing off-policy algorithms have limited performance on static datasets due
    to extrapolation errors from out-of-distribution actions. This leads to the challenge
    of constraining the policy to select actions within the support of the dataset
    during training. We propose to simply learn the Policy in the Latent Action Space
    (PLAS) such that this requirement is naturally satisfied. We evaluate our method
    on continuous control benchmarks in simulation and a deformable object manipulation
    task with a physical robot. We demonstrate that our method provides competitive
    performance consistently across various continuous control tasks and different
    types of datasets, outperforming existing offline reinforcement learning methods
    with explicit constraints.
  authors: Wenxuan Zhou, Sujay Bajracharya, David Held
  award: <award>Plenary talk</award> (Selection rate 4.1%)
  bib: '@inproceedings{PLAS_corl2020,


    title={PLAS: Latent Action Space for Offline Reinforcement Learning},

    author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}

    }'
  img: ../pics/cloth_sliding.gif
  links:
    '[Code]': https://github.com/Wenxuan-Zhou/PLAS
    '[PDF]': https://arxiv.org/pdf/2011.07213.pdf
  short_id: 2020corl
  site: https://sites.google.com/view/latent-policy
  title: '<a href="https://arxiv.org/abs/2011.07213">PLAS: Latent Action Space for
    Offline Reinforcement Learning</a>'
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/d0ykqAxbklA" title="Cloth Sliding with PLAS" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - multimodal
    - rl
- abs: 'Visual data in autonomous driving perception, such as camera image and LiDAR
    point cloud, can be interpreted as a mixture of two aspects: semantic feature
    and geometric structure. Semantics come from the appearance and context of objects
    to the sensor, while geometric structure is the actual 3D shape of point clouds.
    Most detectors on LiDAR point clouds focus only on analyzing the geometric structure
    of objects in real 3D space. Unlike previous works, we propose to learn both semantic
    feature and geometric structure via a unified multi-view framework. Our method
    exploits the nature of LiDAR scans -- 2D range images, and applies well-studied
    2D convolutions to extract semantic features. By fusing semantic and geometric
    features, our method outperforms state-of-the-art approaches in all categories
    by a large margin. The methodology of combining semantic and geometric features
    provides a unique perspective of looking at the problems in real-world 3D point
    cloud detection.'
  authors: Xia Chen, Jianren Wang, David Held, Martial Hebert
  award: null
  bib: >
    @inproceedings{xia20panonet3d,\n    author = \"Chen, Xia \n    and Wang, Jianren\
    \ \n    and Held, David \n    and Hebert, Martial\",\n    title = \"PanoNet3D:\
    \ Combining Semantic and Geometric Understanding for LiDARPoint Cloud Detection\"\
    ,\n    booktitle = \"3DV\",\n    year = \"2020\"\n}"
  img: ../pics/panonet3db.jpg
  links:
    '[Code]': https://github.com/stooloveu/Det3D
    '[PDF]': https://arxiv.org/pdf/2012.09418.pdf
  short_id: 20203dv
  site: https://jianrenw.github.io/PanoNet3D/
  title: '<a href="https://arxiv.org/pdf/2012.09418.pdf">PanoNet3D: Combining Semantic
    and Geometric Understanding for LiDARPoint Cloud Detection</a>'
  venue: International Conference on 3D Vision (3DV), 2020
  video_embed: null
- abs: "Most real-world 3D sensors such as LiDARs perform fixed scans of the entire\
    \ environment, while being decoupled from the recognition system that processes\
    \ the sensor data. In this work, we propose a method for 3D object recognition\
    \ using light curtains, a resource-efficient controllable sensor that measures\
    \ depth at user-specified locations in the environment. Crucially, we propose\
    \ using prediction uncertainty of a deep learning based 3D point cloud detector\
    \ to guide active perception. Given a neural network\u2019s uncertainty, we derive\
    \ an optimization objective to place light curtains using the principle of maximizing\
    \ information gain. Then, we develop a novel and efficient optimization algorithm\
    \ to maximize this objective by encoding the physical constraints of the device\
    \ into a constraint graph and optimizing with dynamic programming. We show how\
    \ a 3D detector can be trained to detect objects in a scene by sequentially placing\
    \ uncertainty-guided light curtains to successively improve detection accuracy."
  authors: Siddharth Ancha, Yaadhav Raaj, Peiyun Hu, Srinivasa Narasimhan, David Held
  award: <award>Spotlight presentation</award> (Selection rate 5.3%)
  bib: >
    @inproceedings{Ancha_2020_ECCV,\n  author=\"Ancha, Siddharth\n  and Raaj,\
    \ Yaadhav\n  and Hu, Peiyun\n  and Narasimhan, Srinivasa G.\n  and Held, David\"\
    ,\n  editor=\"Vedaldi, Andrea\n  and Bischof, Horst\n  and Brox, Thomas\n  and\
    \ Frahm, Jan-Michael\",\n  title=\"Active Perception Using Light Curtains for\
    \ Autonomous Driving\",\n  booktitle=\"Computer Vision -- ECCV 2020\",\n  year=\"\
    2020\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\"\
    ,\n  pages=\"751--766\",\n  isbn=\"978-3-030-58558-7\"\n}"
  img: ../pics/eccv2020lightcurtains.jpg
  links:
    '[Code]': https://github.com/CMU-Light-Curtains/ObjectDetection
    '[Long Talk]': https://www.youtube.com/watch?v=uRP63hHArU0
    '[PDF]': https://arxiv.org/pdf/2008.02191.pdf
    '[Short Talk]': https://www.youtube.com/watch?v=WSb5T3HFE7w
  short_id: 2020eccv
  site: http://siddancha.github.io/projects/active-perception-light-curtains/
  title: <a href="https://arxiv.org/abs/2008.02191">Active Perception using Light
    Curtains for Autonomous Driving</a>
  venue: European Conference on Computer Vision (ECCV), 2020
  video_embed: null
  tags:
    - autonomous-driving
    - active-perception
- abs: 'Cloth detection and manipulation is a common task in domestic and industrial
    settings, yet such tasks remain a challenge for robots due to cloth deformability.
    Furthermore, in many cloth-related tasks like laundry folding and bed making,
    it is crucial to manipulate specific regions like edges and corners, as opposed
    to folds. In this work, we focus on the problem of segmenting and grasping these
    key regions. Our approach trains a network to segment the edges and corners of
    a cloth from a depth image, distinguishing such regions from wrinkles or folds.
    We also provide a novel algorithm for estimating the grasp location, direction,
    and directional uncertainty from the segmentation. We demonstrate our method on
    a real robot system and show that it outperforms baseline methods on grasping
    success. Video and other supplementary materials are available at:'
  authors: Jianing Qian*, Thomas Weng*, Luxin Zhang, Brian Okorn, David Held
  award: null
  bib: >
    @inproceedings{Qian_2020_IROS,\n
    author={Qian, Jianing and Weng, Thomas and Zhang, Luxin and Okorn, Brian and Held, David},
    booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    title={Cloth Region Segmentation for Robust Grasp Selection},
    year={2020},
    volume={},
    number={},
    pages={9553-9560},
    doi={10.1109/IROS45743.2020.9341121}}"
  img: ../pics/cloth_grasping.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2008.05626.pdf
    '[Code]': https://github.com/thomasweng15/cloth-segmentation
  short_id: qian2020iros
  site: https://sites.google.com/view/cloth-segmentation
  title: <a href="https://arxiv.org/abs/2008.05626">Cloth Region Segmentation for
    Robust Grasp Selection</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/iLgSz1PfVm0?start=2" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - deformable
    - multimodal
- abs: 3D object trackers usually require training on large amounts of annotated data
    that is expensive and time-consuming to collect. Instead, we propose leveraging
    vast unlabeled datasets by self-supervised metric learning of 3D object trackers,
    with a focus on data association. Large scale annotations for unlabeled data are
    cheaply obtained by automatic object detection and association across frames.
    We show how these self-supervised annotations can be used in a principled manner
    to learn point-cloud embeddings that are effective for 3D tracking. We estimate
    and incorporate uncertainty in self-supervised tracking to learn more robust embeddings,
    without needing any labeled data. We design embeddings to differentiate objects
    across frames, and learn them using uncertainty-aware self-supervised training.
    Finally, we demonstrate their ability to perform accurate data association across
    frames, towards effective and accurate 3D tracking.
  authors: Jianren Wang, Siddharth Ancha, Yi-Ting Chen, David Held
  award: null
  bib: >
    @inproceedings{jianren20s3da,\n    author = \"Wang, Jianren \n    and Ancha,\
    \ Siddharth \n    and Chen, Yi-Ting \n    and Held, David\",\n    title = \"Uncertainty-aware\
    \ Self-supervised 3D Data Association\",\n    booktitle = \"IROS\",\n    year\
    \ = \"2020\"\n}"
  img: ../pics/jianreniros2020.jpg
  links:
    '[Code]': https://github.com/jianrenw/Self-Supervised-3D-Data-Association
    '[PDF]': https://arxiv.org/pdf/2008.08173v1.pdf
  short_id: jianren2020iros
  site: https://jianrenw.github.io/Self-Supervised-3D-Data-Association/
  title: <a href="https://arxiv.org/abs/2008.08173">Uncertainty-aware Self-supervised
    3D Data Association</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
  tags:
    - autonomous-driving
    - self-supervised
- abs: null
  authors: Brian Okorn, Mengyun Xu, Martial Hebert, David Held
  award: null
  bib: null
  img: ../pics/LearningOrientationDistributions-Website.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/2007.01418">Learning Orientation Distributions
    for Object Pose Estimation</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
- abs: 3D multi-object tracking (MOT) is an essential component for many applications
    such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses
    on developing accurate systems giving less attention to practical considerations
    such as computational cost and system complexity. In contrast, this work proposes
    a simple real-time 3D MOT system. Our system first obtains 3D detections from
    a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter
    and the Hungarian algorithm is used for state estimation and data association.
    Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space
    and standardized 3D MOT evaluation tools are missing for a fair comparison of
    3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with
    three new metrics to comprehensively evaluate 3D MOT methods. We show that, although
    our system employs a combination of classical MOT modules, we achieve state-of-the-art
    3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly,
    although our system does not use any 2D data as inputs, we achieve competitive
    performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate
    of 207.4 FPS on the KITTI dataset, achieving the fastest speed among all modern
    MOT systems.
  authors: Xinshuo Weng, Jianren Wang, David Held, Kris Kitani
  award: null
  bib: >
    @article{Weng2020_AB3DMOT, \nauthor = {Weng, Xinshuo and Wang, Jianren and\
    \ Held, David and Kitani, Kris}, \njournal = {IROS}, \ntitle = {3D Multi-Object\
    \ Tracking: A Baseline and New Evaluation Metrics}, \nyear = {2020} \n}"
  img: ../pics/ab3dmot.jpg
  links:
    '[Code]': https://github.com/xinshuoweng/AB3DMOT
    '[PDF]': http://www.xinshuoweng.com/projects/AB3DMOT/
  short_id: xinshuo2020iros
  site: http://www.xinshuoweng.com/projects/AB3DMOT/
  title: '<a href="https://arxiv.org/abs/1907.03961">3D Multi-Object Tracking: A Baseline
    and New Evaluation Metrics</a>'
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
  tags:
    - autonomous-driving
    - tracking
- abs: When interacting with highly dynamic environments, scene flow allows autonomous
    systems to reason about the non-rigid motion of multiple independent objects.
    This is of particular interest in the field of autonomous driving, in which many
    cars, people, bicycles, and other objects need to be accurately tracked. Current
    state-of-the-art methods require annotated scene flow data from autonomous driving
    scenes to train scene flow networks with supervised learning. As an alternative,
    we present a method of training scene flow that uses two self-supervised losses,
    based on nearest neighbors and cycle consistency. These self-supervised losses
    allow us to train our method on large unlabeled autonomous driving datasets; the
    resulting method matches current state-of-the-art supervised performance using
    no real world annotations and exceeds state-of-the-art performance when combining
    our self-supervised approach with supervised learning on a smaller labeled dataset.
  authors: Himangi Mittal, Brian Okorn, David Held
  award: <award>Oral presentation</award> (Selection rate 5.7%)
  bib: '@InProceedings{Mittal_2020_CVPR,

    author = {Mittal, Himangi and Okorn, Brian and Held, David},

    title = {Just Go With the Flow: Self-Supervised Scene Flow Estimation},

    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)},

    month = {June},

    year = {2020}

    }'
  img: ../pics/jgwtf/Slide1.png
  links:
    '[Code]': https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation
    '[PDF]': https://arxiv.org/pdf/1912.00497.pdf
  short_id: 2020cvprflow
  site: https://just-go-with-the-flow.github.io/
  title: '<a href="https://arxiv.org/abs/1912.00497">Just Go with the Flow: Self-Supervised
    Scene Flow Estimation</a>'
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2020
  video_embed: null
  tags:
    - autonomous-driving
    - self-supervised
    - tracking
- abs: null
  authors: Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan
  award: <award>Oral presentation</award> (Selection rate 5.7%)
  bib: null
  img: ../pics/peiyuncvpr2020/splash2.jpg
  links: {}
  short_id: null
  site: null
  title: '<a href="https://arxiv.org/abs/1912.04986">What You See is What You Get:
    Exploiting Visibility for 3D Object Detection</a>'
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2020
  video_embed: null
  tags:
    - autonomous-driving
- abs: State-of-the-art object grasping methods rely on depth sensing to plan robust
    grasps, but commercially available depth sensors fail to detect transparent and
    specular objects. To improve grasping performance on such objects, we introduce
    a method for learning a multi-modal perception model by bootstrapping from an
    existing uni-modal model. This transfer learning approach requires only a pre-existing
    uni-modal grasping model and paired multi-modal image data for training, foregoing
    the need for ground-truth grasp success labels nor real grasp attempts. Our experiments
    demonstrate that our approach is able to reliably grasp transparent and reflective
    objects. Video and supplementary material are available at
  authors: Thomas Weng, Amith Pallankize, Yimin Tang, Oliver Kroemer, David Held
  award: null
  bib: '@ARTICLE{9001238,

    author={Thomas Weng and Amith Pallankize and Yimin Tang and Oliver Kroemer and
    David Held},

    journal={IEEE Robotics and Automation Letters},

    title={Multi-Modal Transfer Learning for Grasping Transparent and Specular Objects},

    year={2020},

    volume={5},

    number={3},

    pages={3791-3798},

    doi={10.1109/LRA.2020.2974686}}'
  img: ../pics/transparent_grasping.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2006.00028.pdf
  short_id: weng2020ral
  site: https://sites.google.com/view/transparent-specular-grasping
  title: <a href="https://arxiv.org/abs/2006.00028">Multi-Modal Transfer Learning
    for Grasping Transparent and Specular Objects</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2020
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/rYRPWe0xLVo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  tags:
    - multimodal
- abs: null
  authors: Peiyun Hu, David Held*, Deva Ramanan*
  award: null
  bib: null
  img: ../pics/peiyunral2020/output.jpg
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1912.04976">Learning to Optimally Segment
    Point Clouds</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2020
  video_embed: null
  tags:
    - autonomous-driving
- abs: Deep learning object detectors often return false positives with very high
    confidence. Although they optimize generic detection performance, such as mean
    average precision (mAP), they are not designed for reliability. For a reliable
    detection system, if a high confidence detection is made, we would want high certainty
    that the object has indeed been detected. To achieve this, we have developed a
    set of verification tests which a proposed detection must pass to be accepted.
    We develop a theoretical framework which proves that, under certain assumptions,
    our verification tests will not accept any false positives. Based on an approximation
    to this framework, we present a practical detection system that can verify, with
    high precision, whether each detection of a machine-learning based object detector
    is correct. We show that these tests can improve the overall accuracy of a base
    detector and that accepted examples are highly likely to be correct. This allows
    the detector to operate in a high precision regime and can thus be used for robotic
    perception systems as a reliable instance detection method.
  authors: Siddharth Ancha*, Junyu Nan*, David Held
  award: null
  bib: >
    @inproceedings{FlowVerify2019CoRL,\n  author    = {Siddharth Ancha and\n \
    \              Junyu Nan and\n               David Held},\n  editor    = {Leslie\
    \ Pack Kaelbling and\n               Danica Kragic and\n               Komei Sugiura},\n\
        title     = {Combining Deep Learning and Verification for Precise Object Instance\n\
    \               Detection},\n  booktitle = {3rd Annual Conference on Robot Learning,\
    \ CoRL 2019, Osaka, Japan,\n               October 30 - November 1, 2019, Proceedings},\n\
    \  series    = {Proceedings of Machine Learning Research},\n  volume    = {100},\n\
    \  pages     = {122--141},\n  publisher = ,\n  year      = {2019},\n  url    \
    \   = {http://proceedings.mlr.press/v100/ancha20a.html},\n  timestamp = {Mon,\
    \ 25 May 2020 15:01:26 +0200},\n  biburl    = {https://dblp.org/rec/conf/corl/AnchaNH19.bib},\n\
    \  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  img: ../pics/CoRL2019.png
  links:
    '[Code]': https://github.com/siddancha/FlowVerify
    '[PDF]': https://arxiv.org/pdf/1912.12270.pdf
  short_id: 2019corl
  site: https://jnan1.github.io/FlowVerify/
  title: <a href="https://arxiv.org/abs/1912.12270">Combining Deep Learning and Verification
    for Precise Object Instance Detection</a>
  venue: Conference on Robot Learning (CoRL), 2019
  video_embed: null
- abs: null
  authors: Xingyu Lin*, Harjatin Baweja*, George Kantor, David Held
  award: null
  bib: null
  img: ../pics/2019_olaux.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning">Adaptive
    Auxiliary Task Weighting for Reinforcement Learning</a>
  venue: Neural Information Processing Systems (NeurIPS), 2019
  video_embed: null
  tags:
    - rl
- abs: null
  authors: Xingyu Lin, Pengsheng Guo, Carlos Florensa, David Held
  award: null
  bib: null
  img: ../DavidHeld_files/icra19.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1903.06309">Adaptive Variance for Changing
    Sparse-Reward Environments</a>
  venue: International Conference of Robotics and Automation (ICRA), 2019
  video_embed: null
- abs: null
  authors: Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert
  award: null
  bib: null
  img: ../DavidHeld_files/pcn.png
  links: {}
  short_id: null
  site: null
  title: '<a href="http://arxiv.org/abs/1808.00671">PCN: Point Completion Network
    - <award>Best Paper Honorable Mention</award></a>'
  venue: International Conference on 3D Vision (3DV), 2018
  video_embed: null
  tags:
    - autonomous-driving
- abs: null
  authors: Carlos Florensa*, David Held*, Xinyang Geng*, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/ant-maze.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.06366">Automatic Goal Generation for
    Reinforcement Learning Agents</a>
  venue: International Conference on Machine Learning (ICML), 2018
  video_embed: null
  tags:
    - rl
- abs: null
  authors: Sandy Han Huang, David Held, Pieter Abbeel, Anca D. Dragan
  award: null
  bib: null
  img: ../DavidHeld_files/enabling_robots_RSS_2017.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://rdcu.be/ZM50">Enabling Robots to Communicate their Objectives</a>
  venue: Autonomous Robotics (AURO), 2018
  video_embed: null
- abs: null
  authors: Carlos Florensa, David Held, Markus Wulfmeier, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/reverse_curr2.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1707.05300">Reverse Curriculum Generation
    for Reinforcement Learning</a>
  venue: Conference on Robot Learning (CoRL), 2017
  video_embed: null
  tags:
    - rl
- abs: null
  authors: Ignasi Clavera*, David Held*, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/transfer.png
  links: {}
  short_id: null
  site: null
  title: <a href="DavidHeld_files/IROS___RL_pushing.pdf">Policy  Transfer via Modularity</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2017
  video_embed: null
- abs: null
  authors: Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/humanoid.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a>
  venue: International Conference on Machine Learning (ICML), 2017
  video_embed: null
  tags:
    - rl
- abs: null
  authors: Sandy H. Huang, David Held, Pieter Abbeel, Anca D. Dragan
  award: null
  bib: null
  img: ../DavidHeld_files/enabling_robots_RSS_2017.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1702.03465">Enabling Robots to Communicate
    their Objectives</a>
  venue: 'Robotics: Science and Systems (RSS), 2017'
  video_embed: null
- abs: null
  authors: David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/safe_transfer3.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.05394">Probabilistically Safe Policy
    Transfer</a>
  venue: International Conference on Robotics and Automation (ICRA), 2017
  video_embed: null
- abs: null
  authors: David Held, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull7f-web_d.png
  links:
    '[Full Paper]': GOTURN/GOTURN.pdf
    '[Supplement]': GOTURN/supplement.pdf
  short_id: null
  site: GOTURN/GOTURN.html
  title: <a href="GOTURN/GOTURN.pdf">Learning to Track at 100 FPS with Deep Regression
    Networks</a>
  venue: European Conference on Computer Vision (ECCV), 2016
  video_embed: null
- abs: null
  authors: David Held, Devin Guillory, Brice Rebsamen, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull_fig2.png
  links: {}
  short_id: null
  site: null
  title: <a href="segmentation3D/segmentation3D.html">A Probabilistic Framework for
    Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues</a>
  venue: 'Robotics: Science and Systems (RSS), 2016'
  video_embed: null
  tags:
    - autonomous-driving
    - tracking
- abs: null
  authors: David Held, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull6_flatb.png
  links: {}
  short_id: null
  site: null
  title: <a href="http://arxiv.org/abs/1507.08286">Robust Single-View Instance Recognition</a>
  venue: International Conference on Robotics and Automation (ICRA), 2016
  video_embed: null

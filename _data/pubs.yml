# When making a new entry, copy the commented out entry, paste it, uncomment, and fill out the fields. 
# If any of the fields are confusing look at previous examples for guidance. 
# - abs: null
#   authors: null
#   award: null
#   bib: null
#   img: null
#   links: {}
#   short_id: null
#   site: null
#   title: null
#   venue: null
#   video_embed: null
- abs: null
  authors: Wenxuan Zhou, David Held
  award: <award>Oral Presentation</award>  (Selection rate 6.5%)
  bib: null
  img: ../pics/cropped-CoRL2022-1.png
  links: {}
  short_id: null
  site: null
  title: Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: null
  authors: Chuer Pan*, Brian Okorn*, Harry Zhang*, Ben Eisner*, David Held
  award: null
  bib: null
  img: ../pics/cropped-CoRL2022-1.png
  links: {}
  short_id: null
  site: null
  title: 'TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation'
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: null
  authors: "Daniel Seita, Yufei Wang\u2020, Sarthak J Shetty\u2020, Edward Yao Li\u2020\
    , Zackory Erickson, David Held"
  award: null
  bib: null
  img: ../pics/cropped-CoRL2022-1.png
  links: {}
  short_id: null
  site: null
  title: 'ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from
    Point Clouds'
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: "Effective planning of long-horizon deformable object manipulation requires\
    \ suitable \n                    abstractions at both the spatial and temporal\
    \ levels. \n                    Previous methods typically either focus on short-horizon\
    \ tasks or make \n                    strong assumptions that full-state information\
    \ is available, which prevents \n                    their use on deformable objects.\
    \ In this paper, we propose PlAnning with \n                    Spatial-Temporal\
    \ Abstraction (PASTA), which incorporates both spatial abstraction \n        \
    \            (reasoning about objects and their relations to each other) and temporal\
    \ \n                    abstraction (reasoning over skills instead of low-level\
    \ actions). Our framework \n                    maps high-dimension 3D observations\
    \ such as point clouds into a set of latent \n                    vectors and\
    \ plans over skill sequences on top of the latent set representation. \n     \
    \               We show that our method can effectively perform  challenging sequential\
    \ deformable \n                    object manipulation tasks in the real world,\
    \ which require combining multiple \n                    tool-use skills such\
    \ as cutting with a knife, pushing with a pusher, and spreading \n           \
    \         dough with a roller."
  authors: Xingyu Lin*, Carl Qi*, Yunchu Zhang, Zhiao Huang, Katerina Fragkiadaki,
    Yunzhu Li, Chuang Gan, David Held
  award: null
  bib: "@inproceedings{\n                        lin2022planning,\n              \
    \          title={Planning with Spatial-Temporal Abstraction from Point Clouds\
    \ for Deformable Object Manipulation},\n                        author={Xingyu\
    \ Lin and Carl Qi and Yunchu Zhang and Yunzhu Li and Zhiao Huang and Katerina\
    \ Fragkiadaki and Chuang Gan and David Held},\n                        booktitle={6th\
    \ Annual Conference on Robot Learning},\n                        year={2022},\n\
    \                        url={https://openreview.net/forum?id=tyxyBj2w4vw}\n \
    \                       }"
  img: ../pics/2022_pasta.gif
  links:
    '[PDF]': https://openreview.net/forum?id=tyxyBj2w4vw
  short_id: 2022corlpasta
  site: https://sites.google.com/view/pasta-plan
  title: <a href="https://sites.google.com/view/pasta-plan">Planning with Spatial-Temporal
    Abstraction from Point Clouds for Deformable Object Manipulation</a>
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: null
  authors: Brian Okorn*, Chuer Pan*, Martial Hebert, David Held
  award: null
  bib: null
  img: ../pics/cropped-CoRL2022-1.png
  links: {}
  short_id: null
  site: null
  title: Deep Projective Rotation Estimation through Relative Supervision
  venue: Conference on Robot Learning (CoRL), 2022
  video_embed: null
- abs: null
  authors: Tarasha Khurana*, Peiyun Hu*, Achal Dave, Jason Ziglar, David Held, Deva
    Ramanan
  award: null
  bib: null
  img: ../pics/new_splash.png
  links: {}
  short_id: null
  site: null
  title: Differentiable Raycasting for Self-supervised Occupancy Forecasting
  venue: European Conference on Computer Vision (ECCV), 2022
  video_embed: null
- abs: Robotic manipulation of cloth has applications ranging from fabrics manufacturing
    to handling blankets and laundry. Cloth manipulation is challenging for robots
    largely due to their high degrees of freedom, complex dynamics, and severe self-occlusions
    when in folded or crumpled configurations. Prior work on robotic manipulation
    of cloth relies primarily on vision sensors alone, which may pose challenges for
    fine-grained manipulation tasks such as grasping a desired number of cloth layers
    from a stack of cloth. In this paper, we propose to use tactile sensing for cloth
    manipulation; we attach a tactile sensor (ReSkin) to one of the two fingertips
    of a Franka robot and train a classifier to determine whether the robot is grasping
    a specific number of cloth layers. During test-time experiments, the robot uses
    this classifier as part of its policy to grasp one or two cloth layers using tactile
    feedback to determine suitable grasping points. Experimental results over 180
    physical trials suggest that the proposed method outperforms baselines that do
    not use tactile feedback and has a better generalization to unseen fabrics compared
    to methods that use image classifiers.
  authors: Sashank Tirumala*, Thomas Weng*, Daniel Seita*, Oliver Kroemer, Zeynep
    Temel, David Held
  award: <award>Best Paper at <a href="https://romado-workshop.github.io/ROMADO2022/">ROMADO-SI</a></award>
  bib: "@inproceedings{tirumala2022,\n                    title={Learning to Singulate\
    \ Layers of Cloth based on Tactile Feedback},\n                    author={Sashank\
    \ Tirumala and Thomas Weng and Daniel Seita and Oliver Kroemer and Zeynep Temel\
    \ and   David Held},\n                    booktitle={IEEE/RSJ International Conference\
    \ on Intelligent Robots and Systems (IROS)},\n                    year={2022},\n\
    \                    }"
  img: ../pics/iros2022-singulating-layers-fabric-reskin-400x210.gif
  links:
    '[arXiv]': https://arxiv.org/abs/2207.11196
  short_id: 2022reskinfabric
  site: https://sites.google.com/view/reskin-cloth/home
  title: Learning to Singulate Layers of Cloth based on Tactile Feedback
  venue: International Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: null
- abs: Deformable object manipulation has many applications such as cooking and laundry
    folding in our daily lives. Manipulating elastoplastic objects such as dough is
    particularly challenging because dough lacks a compact state representation and
    requires contact-rich interactions. We consider the task of flattening a piece
    of dough into a specific shape from RGB-D images. While the task is seemingly
    intuitive for humans, there exist local optima for common approaches such as naive
    trajectory optimization. We propose a novel trajectory optimizer that optimizes
    through a differentiable "reset" module, transforming a single-stage, fixed-initialization
    trajectory into a multistage, multi-initialization trajectory where all stages
    are optimized jointly. We then train a closed-loop policy on the demonstrations
    generated by our trajectory optimizer. Our policy receives partial point clouds
    as input, allowing ease of transfer from simulation to the real world. We show
    that our policy can perform real-world dough manipulation, flattening a ball of
    dough into a target shape.
  authors: Carl Qi, Xingyu Lin, David Held
  award: null
  bib: "@ARTICLE{qi2022dough, \nauthor={Qi, Carl and Lin, Xingyu and Held, David},\n\
    journal={IEEE Robotics and Automation Letters}, \ntitle={Learning Closed-Loop\
    \ Dough Manipulation Using a Differentiable Reset Module}, \nyear={2022},\nvolume={7},\n\
    number={4},\npages={9857-9864},\ndoi={10.1109/LRA.2022.3191239}}"
  img: ../pics/carl-22-website.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2207.04638.pdf
  short_id: 2022raldough
  site: https://sites.google.com/view/dough-manipulation
  title: <a href="https://sites.google.com/view/dough-manipulation">Learning Closed-loop
    Dough Manipulation using a Differentiable Reset Module</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/b1qKmgmei2U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
- abs: Robotic manipulation of highly deformable cloth presents a promising opportunity
    to assist people with several daily tasks, such as washing dishes; folding laundry;
    or dressing, bathing, and hygiene assistance for individuals with severe motor
    impairments. In this work, we introduce a formulation that enables a collaborative
    robot to perform visual haptic reasoning with cloth -- the act of inferring the
    location and magnitude of applied forces during physical interaction. We present
    two distinct model representations, trained in physics simulation, that enable
    haptic reasoning using only visual and robot kinematic observations. We conducted
    quantitative evaluations of these models in simulation for robot-assisted dressing,
    bathing, and dish washing tasks, and demonstrate that the trained models can generalize
    across different tasks with varying interactions, human body sizes, and object
    shapes.  We also present results with a real-world mobile manipulator, which used
    our simulation-trained models to estimate applied contact forces while performing
    physically assistive tasks with cloth.
  authors: Yufei Wang, David Held, Zackory Erickson
  award: null
  bib: "@inproceedings{wang2022,\n    title={Visual Haptic Reasoning: Estimating Contact\
    \ Forces by Observing Deformable Object Interactions},\n    author={Yufei Wang,\
    \ David Held, and Zackory Erickson},\n    booktitle={IEEE/RSJ International Conference\
    \ on Intelligent Robots and Systems (IROS)},\n    year={2022},\n    }"
  img: ../pics/visual-haptic-reasoning-IROS.gif
  links: {}
  short_id: 2022vhr
  site: https://sites.google.com/view/visualhapticreasoning/home
  title: 'Visual Haptic Reasoning: Estimating Contact Forces by Observing Deformable
    Object Interactions'
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference on Intelligent Robots and Systems (IROS), 2022
  video_embed: null
- abs: We explore a novel method to perceive and manipulate 3D articulated objects
    that generalizes to enable a robot to articulate unseen classes of objects. We
    propose a vision-based system that learns to predict the potential motions of
    the parts of a variety of articulated objects to guide downstream motion planning
    of the system to articulate the objects. To predict the object motions, we train
    a neural network to output a dense vector field representing the point-wise motion
    direction of the points in the point cloud under articulation. We then deploy
    an analytical motion planner based on this vector field to achieve a policy that
    yields maximum articulation. We train the vision system entirely in simulation,
    and we demonstrate the capability of our system to generalize to unseen object
    instances and novel categories in both simulation and the real world, deploying
    our policy on a Sawyer robot with no finetuning. Results show that our system
    achieves state-of-the-art performance in both simulated and real-world experiments.
  authors: Ben Eisner*, Harry Zhang*, David Held
  award: <award>Best Paper Finalist</award>
  bib: "@inproceedings{EisnerZhang2022FLOW,\n            title={FlowBot3D: Learning\
    \ 3D Articulation Flow to Manipulate Articulated Objects},\n            author={Eisner*,\
    \ Ben and Zhang*, Harry and Held,David},\n            booktitle={Robotics: Science\
    \ and Systems (RSS)},\n            year={2022}\n       }"
  img: ../pics/montage.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2205.04382.pdf
  short_id: 2022flowbot3D
  site: https://sites.google.com/view/articulated-flowbot-3d/home#h.xrrj1zv4o44a
  title: '<a href="https://sites.google.com/view/articulated-flowbot-3d/home#h.xrrj1zv4o44a">FlowBot3D:
    Learning 3D Articulation Flow to Manipulate Articulated Objects</a>'
  venue: 'Robotics: Science and Systems (RSS), 2022'
  video_embed: null
- abs: "Self-occlusion is challenging for cloth manipulation, as it makes it difficult\
    \ to estimate the full state of the cloth. Ideally, a robot trying to unfold a\
    \ crumpled or folded cloth should be able to reason about the cloth's occluded\
    \ regions.\n                    We leverage recent advances in pose estimation\
    \ for cloth to build a system that uses explicit occlusion reasoning to unfold\
    \ a crumpled cloth. Specifically, we first learn a model to reconstruct the mesh\
    \ of the cloth. However, the model will likely have errors due to the complexities\
    \ of the cloth configurations and due to ambiguities from occlusions.  Our main\
    \ insight is that we can further refine the predicted reconstruction by performing\
    \ test-time finetuning with self-supervised losses. The obtained reconstructed\
    \ mesh allows us to use a mesh-based dynamics model for planning while reasoning\
    \ about occlusions. We evaluate our system both on cloth flattening as well as\
    \ on  cloth canonicalization, in which the objective is to manipulate the cloth\
    \ into a canonical pose. Our experiments show that our method significantly outperforms\
    \ prior methods that do not explicitly account for occlusions or perform test-time\
    \ optimization."
  authors: Zixuan Huang, Xingyu Lin, David Held
  award: null
  bib: "@inproceedings{huang2022medor,\n            title={Mesh-based Dynamics Model\
    \ with Occlusion Reasoning for Cloth Manipulation},\n            author={Huang,\
    \ Zixuan and Lin, Xingyu and Held,David},\n            booktitle={Robotics: Science\
    \ and Systems (RSS)},\n            year={2022}\n       }"
  img: ../pics/rss2022medor.gif
  links:
    '[PDF]': https://arxiv.org/abs/2206.02881
    '[Poster]': https://drive.google.com/file/d/1xdMVN7moNcdIeAPuPP0tpdvtjIfeFWdZ/view?usp=sharing
    '[Video]': https://youtu.be/0s9PA6EgiqE
  short_id: 2022occnet
  site: https://sites.google.com/view/occlusion-reason/home
  title: <a href="https://sites.google.com/view/occlusion-reason/home">Mesh-based
    Dynamics with Occlusion Reasoning for Cloth Manipulation</a>
  venue: 'Robotics: Science and Systems (RSS), 2022'
  video_embed: null
- abs: 'We consider the problem of sequential robotic manipulation of deformable objects
    using tools.


    Previous works have shown that differentiable physics simulators provide gradients
    to the environment state and help trajectory optimization to converge orders of
    magnitude faster than model-free reinforcement learning algorithms for deformable
    object manipulations. However, such gradient-based trajectory optimization typically
    requires access to the full simulator states and can only solve short-horizon,
    single-skill tasks due to local optima. In this work, we propose a novel framework,
    named DiffSkill, that uses a differentiable physics simulator for skill abstraction
    to solve long-horizon deformable object manipulation tasks from sensory observations.
    In particular, we first obtain short-horizon skills for using each individual
    tool from a gradient-based optimizer and then learn a neural skill abstractor
    from the demonstration videos; Finally, we plan over the skills to solve the long-horizon
    task. We show the advantages of our method in a new set of sequential deformable
    object manipulation tasks over previous reinforcement learning algorithms and
    the trajectory optimizer.'
  authors: Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum, David Held, Chuang
    Gan
  award: null
  bib: '@inproceedings{


    lin2022diffskill,

    title={DiffSkill: Skill Abstraction from Differentiable Physics for Deformable
    Object Manipulations with Tools},

    author={Xingyu Lin and Zhiao Huang and Yunzhu Li and David Held and Joshua B.
    Tenenbaum and Chuang Gan},

    booktitle={International Conference on Learning Representations},

    year={2022},

    url={https://openreview.net/forum?id=Kef8cKdHWpP}}'
  img: ../pics/2022_diffskill.gif
  links:
    '[PDF]': https://openreview.net/pdf?id=Kef8cKdHWpP
  short_id: 2022diffskill
  site: https://xingyu-lin.github.io/diffskill/
  title: '<a href="https://sites.google.com/view/iclr2022diffskill">DiffSkill: Skill
    Abstraction from Differentiable Physics for Deformable Object Manipulations with
    Tools</a>'
  venue: International Conference on Learning Representations (ICLR), 2022
  video_embed: null
- abs: Liquid state estimation is important for robotics tasks such as pouring; however,
    estimating the state of transparent liquids is a challenging problem. We propose
    a novel segmentation pipeline that can segment transparent liquids such as water
    from a static, RGB image without requiring any manual annotations or heating of
    the liquid for training. Instead, we use a generative model that is capable of
    translating images of colored liquids into synthetically generated transparent
    liquid images, trained only on an unpaired dataset of colored and transparent
    liquid images. Segmentation labels of colored liquids are obtained automatically
    using background subtraction. Our experiments show that we are able to accurately
    predict a segmentation mask for transparent liquids without requiring any manual
    annotations. We demonstrate the utility of transparent liquid segmentation in
    a robotic pouring task that controls pouring by perceiving the liquid height in
    a transparent cup. Accompanying video and supplementary materials can be found
    on our project page.
  authors: Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held
  award: null
  bib: '@inproceedings{icra2022pouring,


    title={Self-supervised Transparent Liquid Segmentation for Robotic Pouring},

    author={Gautham Narayan Narasimhan, Kai Zhang, Ben Eisner, Xingyu Lin, David Held},

    booktitle={International Conference on Robotics and Automation (ICRA)},

    year={2022}}'
  img: ../pics/pouring.gif
  links:
    '[Code]': https://github.com/gauthamnarayan/transparent-liquid-segmentation
    '[PDF]': https://arxiv.org/pdf/2203.01538.pdf
    '[Poster]': https://docs.google.com/presentation/d/1kOdSIgoGPlg1CmRILjsdwIpTJIMpfgjY/edit?usp=sharing&ouid=115079833191742902358&rtpof=true&sd=true
    '[Slides]': https://drive.google.com/file/d/1cG9ZDGekFVLKLqSS4ZSlS445Lcxwm08h/view?usp=sharing
    '[Video]': https://www.youtube.com/watch?v=uXGCSd3KVd8
  short_id: 2022pouring
  site: https://sites.google.com/view/transparentliquidpouring
  title: <a href="https://sites.google.com/view/transparentliquidpouring">Self-supervised
    Transparent Liquid Segmentation for Robotic Pouring</a>
  venue: International Conference of Robotics and Automation (ICRA), 2022
  video_embed: null
- abs: Real-time object pose estimation is necessary for many robot manipulation algorithms.
    However, state-of-the-art methods for object pose estimation are trained for a
    specific set of objects; these methods thus need to be retrained to estimate the
    pose of each new object, often requiring tens of GPU-days of training for optimal
    performance. In this paper, we propose the OSSID framework, leveraging a slow
    zero-shot pose estimator to self-supervise the training of a fast detection algorithm.
    This fast detector can then be used to filter the input to the pose estimator,
    drastically improving its inference speed. We show that this self-supervised training
    exceeds the performance of existing zero-shot detection methods on two widely
    used object pose estimation and detection datasets, without requiring any human
    annotations. Further, we show that the resulting method for pose estimation has
    a significantly faster inference speed, due to the ability to filter out large
    parts of the image. Thus, our method for self-supervised online learning of a
    detector (trained using pseudo-labels from a slow pose estimator) leads to accurate
    pose estimation at real-time speeds, without requiring human annotations.
  authors: Qiao Gu, Brian Okorn, David Held
  award: null
  bib: "@ARTICLE{ral2022ossid,\n  author={Gu, Qiao and Okorn, Brian and Held, David},\n\
    \  journal={IEEE Robotics and Automation Letters}, \n  title={OSSID: Online Self-Supervised\
    \ Instance Detection by (And For) Pose Estimation}, \n  year={2022},\n  volume={7},\n\
    \  number={2},\n  pages={3022-3029},\n  doi={10.1109/LRA.2022.3145488}}"
  img: ../pics/ossid.jpg
  links:
    '[Code]': https://github.com/r-pad/OSSID_code
    '[PDF]': https://arxiv.org/pdf/2201.07309.pdf
    '[Video]': https://www.youtube.com/watch?v=S_pU2FbMN8k
  short_id: 2022ossid
  site: https://georgegu1997.github.io/OSSID/
  title: '<a href="https://arxiv.org/abs/2201.07309">OSSID: Online Self-Supervised
    Instance Detection by (and for) Pose Estimation</a>'
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2022
  video_embed: null
- abs: When navigating in urban environments, many of the objects that need to be
    tracked and avoided are heavily occluded. Planning and tracking using these partial
    scans can be challenging. The aim of this work is to learn to complete these partial
    point clouds, giving us a full understanding of the object's geometry using only
    partial observations. Previous methods achieve this with the help of complete,
    ground-truth annotations of the target objects, which are available only for simulated
    datasets. However, such ground truth is unavailable for real-world LiDAR data.
    In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet,
    which is trained only on partial scans without assuming access to complete, ground-truth
    annotations. Our method achieves this via inpainting. We remove a portion of the
    input data and train the network to complete the missing region. As it is difficult
    to determine which regions were occluded in the initial cloud and which were synthetically
    removed, our network learns to complete the full cloud, including the missing
    regions in the initial partial cloud. We show that our method outperforms previous
    unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet,
    and real-world LiDAR dataset, Semantic KITTI.
  authors: Himangi Mittal, Brian Okorn, Arpit Jangid, David Held
  award: <award>Oral presentation</award> (Selection rate 3.3%)
  bib: "@article{mittal2021self,\n  title={Self-Supervised Point Cloud Completion\
    \ via Inpainting},\n  author={Mittal, Himangi and Okorn, Brian and Jangid, Arpit\
    \ and Held, David},\n  journal={British Machine Vision Conference (BMVC), 2021},\n\
    \  year={2021}\n}"
  img: ../pics/bmvc2-2021.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2111.10701.pdf
    '[Video]': https://www.bmvc2021-virtualconference.com/conference/papers/paper_0443.html
  short_id: 2021bmvc
  site: https://self-supervised-completion-inpainting.github.io/
  title: <a href="https://arxiv.org/abs/2111.10701">Self-Supervised Point Cloud Completion
    via Inpainting</a>
  venue: British Machine Vision Conference (BMVC), 2021
  video_embed: null
- abs: "Benchmarks offer a scientific way to compare algorithms using scientific performance\
    \ metrics. Good benchmarks have two features: (a) wide audience appeal; (b) easily\
    \ reproducible. In robotics, there is a tradeoff between reproducibility and broad\
    \ accessibility. If the benchmark is kept restrictive (fixed hardware, objects),\
    \ the numbers are reproducible but it becomes niche. On the other hand, benchmark\
    \ could be just loose set of protocols but the underlying varying setups make\
    \ it hard to reproduce the results. In this paper, we re-imagine robotics benchmarks\
    \ \u2013 we define a robotics benchmark to be a set of experimental protocols\
    \ and state of the art algorithmic implementations. These algorithm implementations\
    \ will provide a way to recreate baseline numbers in a new local robotic setup\
    \ in less than few hours and hence help provide credible relative rankings between\
    \ different approaches. These credible local rankings are pooled from several\
    \ locations to help establish global rankings and SOTA algorithms that work across\
    \ majority of setups. We introduce RB2 \u2014 a benchmark inspired from human\
    \ SHAP tests. Our benchmark was run across three different labs and reveals several\
    \ surprising findings."
  authors: Sudeep Dasari, Jianren Wang, Joyce Hong, Shikhar Bahl, Yixin Lin, Austin
    S Wang, Abitha Thankaraj, Karanbir Singh Chahal, Berk Calli, Saurabh Gupta, David
    Held, Lerrel Pinto, Deepak Pathak, Vikash Kumar, Abhinav Gupta
  award: null
  bib: null
  img: ../pics/rb2b.png
  links: {}
  short_id: div2021rb2
  site: https://rb2.info/
  title: '<a href="https://openreview.net/forum?id=e82_BlJL43M">RB2: Robotic Manipulation
    Benchmarking with a Twist</a>'
  venue: NeurIPS 2021 Datasets and Benchmarks Track, 2021
  video_embed: null
- abs: 3D object detection plays an important role in autonomous driving and other
    robotics applications. However, these detectors usually require training on large
    amounts of annotated data that is expensive and time-consuming to collect. Instead,
    we propose leveraging large amounts of unlabeled point cloud videos by semi-supervised
    learning of 3D object detectors via temporal graph neural networks. Our insight
    is that temporal smoothing can create more accurate detection results on unlabeled
    data, and these smoothed detections can then be used to retrain the detector.
    We learn to perform this temporal reasoning with a graph neural network, where
    edges represent the relationship between candidate detections in different time
    frames.
  authors: Jianren Wang, Haiming Gang, Siddharth Ancha, Yi-ting Chen, and David Held
  award: null
  bib: '@article{wang2021sodtgnn,

    title={Semi-supervised 3D Object Detection via Temporal Graph Neural Networks},

    author={Wang, Jianren and Gang, Haiming and Ancha, Siddharth and Chen, Yi-ting
    and Held, David},

    journal={International Conference on 3D Vision (3DV)},

    year={2021}}'
  img: ../pics/sod_tgnn.jpg
  links:
    '[Code]': https://github.com/jianrenw/SOD-TGNN
    '[Video (Long)]': https://youtu.be/L8X4LCEpCaE
    '[Video (Short)]': https://youtu.be/BhW7m3R_Yqo
  short_id: div20213dV
  site: https://www.jianrenw.com/SOD-TGNN/
  title: <a href="https://arxiv.org/pdf/2202.00182.pdf">Semi-supervised 3D Object
    Detection via Temporal Graph Neural Networks</a>
  venue: International Conference on 3D Vision (3DV), 2021
  video_embed: null
- abs: Robotic manipulation of cloth remains challenging for robotics due to the complex
    dynamics of the cloth, lack of a low-dimensional state representation, and self-occlusions.
    In contrast to previous model-based approaches that learn a pixel-based dynamics
    model or a compressed latent vector dynamics, we propose to learn a particle-based
    dynamics model from a partial point cloud observation. To overcome the challenges
    of partial observability, we infer which visible points are connected on the underlying
    cloth mesh. We then learn a dynamics model over this visible connectivity graph.
    Compared to previous learning-based approaches, our model poses strong inductive
    bias with its particle based representation for learning the underlying cloth
    physics; it is invariant to visual features; and the predictions can be more easily
    visualized. We show that our method greatly outperforms previous state-of-the-art
    model-based and model-free reinforcement learning methods in simulation. Furthermore,
    we demonstrate zero-shot sim-to-real transfer where we deploy the model trained
    in simulation on a Franka arm and show that the model can successfully smooth
    different types of cloth from crumpled configurations. Videos can be found on
    our project website.
  authors: Xingyu Lin*, Yufei Wang*, Zixuan Huang, David Held
  award: null
  bib: '@inproceedings{lin2021VCD,

    title={Learning Visible Connectivity Dynamics for Cloth Smoothing},

    author={Lin, Xingyu and Wang, Yufei and Huang, Zixuan and Held, David},

    booktitle={Conference on Robot Learning},

    year={2021}}'
  img: ../pics/vcd.gif
  links:
    '[OpenReview]': https://openreview.net/forum?id=n1hDe9iK6ms
    '[PDF]': https://arxiv.org/pdf/2105.10389.pdf
  short_id: div2021vcd
  site: https://sites.google.com/view/vcd-cloth
  title: <a href="https://arxiv.org/abs/2105.10389">Learning Visible Connectivity
    Dynamics for Cloth Smoothing</a>
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: null
- abs: We address the problem of goal-directed cloth manipulation, a challenging task
    due to the deformability of cloth. Our insight is that optical flow, a technique
    normally used for motion estimation in video, can also provide an effective representation
    for corresponding cloth poses across observation and goal images. We introduce
    FabricFlowNet (FFN), a cloth manipulation policy that leverages flow as both an
    input and as an action representation to improve performance. FabricFlowNet also
    elegantly switches between dual-arm and single-arm actions based on the desired
    goal. We show that FabricFlowNet significantly outperforms state-of-the-art model-free
    and model-based cloth manipulation policies. We also present real-world experiments
    on a bimanual system, demonstrating effective sim-to-real transfer. Finally, we
    show that our method generalizes when trained on a single square cloth to other
    cloth shapes, such as T-shirts and rectangular cloths.
  authors: Thomas Weng, Sujay Bajracharya, Yufei Wang, David Held
  award: null
  bib: "@inproceedings{weng2021fabricflownet,\n title={FabricFlowNet: Bimanual Cloth\
    \ Manipulation \n    with a Flow-based Policy},\n author={Weng, Thomas and Bajracharya,\
    \ Sujay and \n    Wang, Yufei and Agrawal, Khush and Held, David},\n booktitle={Conference\
    \ on Robot Learning},\n year={2021}\n}"
  img: ../pics/ffn.gif
  links:
    '[Code]': https://github.com/thomasweng15/FabricFlowNet
    '[OpenReview]': https://openreview.net/forum?id=TsqkJJMgHkk
    '[PDF]': https://arxiv.org/pdf/2111.05623.pdf
    '[Poster]': https://www.google.com/url?q=https%3A%2F%2Fopenreview.net%2Fattachment%3Fid%3DTsqkJJMgHkk%26name%3Dposter&sa=D&sntz=1&usg=AFQjCNH3PPtQ_vMMk00WEMRXG28cWK3ylQ
  short_id: weng2021fabricflownet
  site: https://sites.google.com/view/fabricflownet
  title: '<a href="https://arxiv.org/abs/2111.05623">FabricFlowNet: Bimanual Cloth
    Manipulation with a Flow-based Policy</a>'
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: null
- abs: Reinforcement learning (RL) in low-data and risk-sensitive domains requires
    performant and flexible deployment policies that can readily incorporate constraints
    during deployment. One such class of policies are the semi-parametric H-step lookahead
    policies, which select actions using trajectory optimization over a dynamics model
    for a fixed horizon with a terminal value function. In this work, we investigate
    a novel instantiation of H-step lookahead with a learned model and a terminal
    value function learned by a model-free off-policy algorithm, named Learning Off-Policy
    with Online Planning (LOOP). We provide a theoretical analysis of this method,
    suggesting a tradeoff between model errors and value function errors and empirically
    demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore,
    we identify the "Actor Divergence" issue in this framework and propose Actor Regularized
    Control (ARC), a modified trajectory optimization procedure. We evaluate our method
    on a set of robotic tasks for Offline and Online RL and demonstrate improved performance.
    We also show the flexibility of LOOP to incorporate safety constraints during
    deployment with a set of navigation environments. We demonstrate that LOOP is
    a desirable framework for robotics applications based on its strong performance
    in various important RL settings.
  authors: Harshit Sikchi, Wenxuan Zhou, David Held
  award: <award>Oral presentation</award> (Selection rate 6.5%); <award>Best Paper
    Finalist</award>
  bib: '@inproceedings{sikchi2021learning,

    title={Learning Off-policy for Online Planning},

    author={Sikchi, Harshit and Zhou, Wenxuan and Held, David},

    booktitle={Conference on Robot Learning},

    year={2021}}'
  img: ../pics/loop_overview.jpeg
  links:
    '[Code]': https://github.com/hari-sikchi/LOOP
    '[OpenReview]': https://openreview.net/forum?id=1GNV9SW95eJ
    '[PDF]': https://arxiv.org/pdf/2008.10066.pdf
    '[Talk]': https://youtu.be/zYDwnj_ghZM
  short_id: sikchi2021learning
  site: https://hari-sikchi.github.io/loop/
  title: <a href="https://arxiv.org/abs/2008.10066">Learning Off-policy for Online
    Planning</a>
  venue: Conference on Robot Learning (CoRL), 2021
  video_embed: null
- abs: 'To safely navigate unknown environments, robots must accurately perceive dynamic
    obstacles. Instead of directly measuring the scene depth with a LiDAR sensor,
    we explore the use of a much cheaper and higher resolution sensor:'
  authors: Siddharth Ancha, Gaurav Pathak, Srinivasa Narasimhan, David Held
  award: null
  bib: "@INPROCEEDINGS{Ancha-RSS-21,\n    AUTHOR    = {Siddharth Ancha AND Gaurav\
    \ Pathak AND Srinivasa G. Narasimhan AND David Held},\n    TITLE     = {Active\
    \ Safety Envelopes using Light Curtains with Probabilistic Guarantees},\n    BOOKTITLE\
    \ = {Proceedings of Robotics: Science and Systems},\n    YEAR      = {2021},\n\
    \    MONTH     = {July}\n}"
  img: ../pics/rss2021lightcurtains.png
  links:
    '[Blog]': https://blog.ml.cmu.edu/2021/11/19/active-safety-envelopes-using-light-curtains-with-probabilistic-guarantees/
    '[Code]': https://github.com/CMU-Light-Curtains/SafetyEnvelopes
    '[PDF]': https://arxiv.org/pdf/2107.04000.pdf
    '[Poster]': https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees/docs/poster.png
    '[Talk]': https://www.youtube.com/watch?v=1PUAjzcTz5g
  short_id: 2021safetyenvelopes
  site: https://siddancha.github.io/projects/active-safety-envelopes-with-guarantees
  title: <a href="https://arxiv.org/abs/2107.04000">Active Safety Envelopes using
    Light Curtains with Probabilistic Guarantees</a>
  venue: 'Robotics: Science and Systems (RSS), 2021'
  video_embed: null
- abs: Pose estimation is a basic module in many robot manipulation pipelines. Estimating
    the pose of objects in the environment can be useful for grasping, motion planning,
    or manipulation. However, current state-of-the-art methods for pose estimation
    either rely on large annotated training sets or simulated data. Further, the long
    training times for these methods prohibit quick interaction with novel objects.
    To address these issues, we introduce a novel method for zero-shot object pose
    estimation in clutter. Our approach uses a hypothesis generation and scoring framework,
    with a focus on learning a scoring function that generalizes to objects not used
    for training. We achieve zero-shot generalization by rating hypotheses as a function
    of unordered point differences. We evaluate our method on challenging datasets
    with both textured and untextured objects in cluttered scenes and demonstrate
    that our method significantly outperforms previous methods on this task. We also
    demonstrate how our system can be used by quickly scanning and building a model
    of a novel object, which can immediately be used by our method for pose estimation.
    Our work allows users to estimate the pose of novel objects without requiring
    any retraining.
  authors: Brian Okorn*, Qiao Gu*, Martial Hebert, David Held
  award: null
  bib: "@inproceedings{okorn2021zephyr,\n                    title={Zephyr: Zero-shot\
    \ pose hypothesis rating},\n                    author={Okorn, Brian and Gu, Qiao\
    \ and Hebert, Martial and Held, David},\n                    booktitle={2021 IEEE\
    \ International Conference on Robotics and Automation (ICRA)},\n             \
    \       pages={14141--14148},\n                    year={2021},\n            \
    \        organization={IEEE}\n                  }"
  img: ../pics/zephyr2.png
  links:
    '[Code]': https://github.com/r-pad/zephyr
    '[PDF]': https://arxiv.org/pdf/2104.13526.pdf
  short_id: 2021zephyr
  site: https://bokorn.github.io/zephyr/
  title: '<a href="https://arxiv.org/abs/2104.13526">ZePHyR: Zero-shot Pose Hypothesis
    Rating</a>'
  venue: International Conference of Robotics and Automation (ICRA), 2021
  video_embed: null
- abs: Active sensing through the use of Adaptive Depth Sensors is a nascent field,
    with potential in areas such as Advanced driver-assistance systems (ADAS). They
    do however require dynamically driving a laser / light-source to a specific location
    to capture information, with one such class of sensor being the Triangulation
    Light Curtains (LC). In this work, we introduce a novel approach that exploits
    prior depth distributions from RGB cameras to drive a Light Curtain's laser line
    to regions of uncertainty to get new measurements. These measurements are utilized
    such that depth uncertainty is reduced and errors get corrected recursively. We
    show real-world experiments that validate our approach in outdoor and driving
    settings, and demonstrate qualitative and quantitative improvements in depth RMSE
    when RGB cameras are used in tandem with a Light Curtain.
  authors: Yaadhav Raaj, Siddharth Ancha, Robert Tamburo, David Held, Srinivasa Narasimhan
  award: null
  bib: "@inproceedings{cvpr2021raajexploiting,\n    title     = {Exploiting & Refining\
    \ Depth Distributions with Triangulation Light Curtains},\n    author    = {Yaadhav\
    \ Raaj, Siddharth Ancha, Robert Tamburo, David Held, Srinivasa Narasimhan},\n\
    \    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and\
    \ Pattern Recognition (CVPR)},\n    year      = {2021}\n}"
  img: ../pics/cvpr21.png
  links:
    '[Code]': https://github.com/CMU-Light-Curtains/DepthEstimation
    '[PDF]': https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf
    '[Talk]': https://youtu.be/kIjn3U8luV0
  short_id: 2021exploiting
  site: https://soulslicer.github.io/rgb-lc-fusion/
  title: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf">Exploiting
    &amp; Refining Depth Distributions with Triangulation Light Curtains</a>
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2021
  video_embed: null
- abs: Safe local motion planning for autonomous driving in dynamic environments requires
    forecasting how the scene evolves. Practical autonomy stacks adopt a semantic
    object-centric representation of a dynamic scene and build object detection, tracking,
    and prediction modules to solve forecasting. However, training these modules comes
    at an enormous human cost of manually annotated objects across frames. In this
    work, we explore future freespace as an alternative representation to support
    motion planning. Our key intuition is that it is important to avoid straying into
    occupied space regardless of what is occupying it. Importantly, computing ground-truth
    future freespace is annotation-free. First, we explore freespace forecasting as
    a self-supervised learning task. We then demonstrate how to use forecasted freespace
    to identify collision-prone plans from off-the-shelf motion planners. Finally,
    we propose future freespace as an additional source of annotation-free supervision.
    We demonstrate how to integrate such supervision into the learning process of
    learning-based planners. Experimental results on nuScenes and CARLA suggest both
    approaches lead to significant reduction in collision rates.
  authors: Peiyun Hu, Aaron Huang, John Dolan, David Held, Deva Ramanan
  award: null
  bib: "@inproceedings{cvpr2021husafe,\n                    title={Safe Local Motion\
    \ Planning with Self-Supervised Freespace Forecasting},\n                    author={Peiyun\
    \ Hu, Aaron Huang, John Dolan, David Held, Deva Ramanan},\n                  \
    \  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\
    \ Recognition (CVPR)},\n                    year={2021}}"
  img: ../pics/peiyunff.gif
  links:
    '[Code]': https://github.com/peiyunh/ff
    '[Paper]': https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf
    '[Poster]': https://peiyunh.github.io/ff/poster.pdf
    '[Talk]': https://youtu.be/O_2MojWp7yk
  short_id: 2021cvpr_safe
  site: https://peiyunh.github.io/ff/index.html
  title: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Safe_Local_Motion_Planning_With_Self-Supervised_Freespace_Forecasting_CVPR_2021_paper.pdf">Safe
    Local Motion Planning with Self-Supervised Freespace Forecasting</a>
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2021
  video_embed: null
- abs: Manipulating deformable objects has long been a challenge in robotics due to
    its high dimensional state representation and complex dynamics. Recent success
    in deep reinforcement learning provides a promising direction for learning to
    manipulate deformable objects with data driven methods. However, existing reinforcement
    learning benchmarks only cover tasks with direct state observability and simple
    low-dimensional dynamics or with relatively simple image-based environments, such
    as those with rigid objects. In this paper, we present SoftGym, a set of open-source
    simulated benchmarks for manipulating deformable objects, with a standard OpenAI
    Gym API and a Python interface for creating new environments. Our benchmark will
    enable reproducible research in this important area. Further, we evaluate a variety
    of algorithms on these tasks and highlight challenges for reinforcement learning
    algorithms, including dealing with a state representation that has a high intrinsic
    dimensionality and is partially observable. The experiments and analysis indicate
    the strengths and limitations of existing methods in the context of deformable
    object manipulation that can help point the way forward for future methods development.
    Code and videos of the learned policies can be found on our project website.
  authors: Xingyu Lin, Yufei Wang, Jake Olkin, David Held
  award: null
  bib: '@inproceedings{corl2020softgym,

    title={SoftGym: Benchmarking Deep Reinforcement Learning for Deformable Object
    Manipulation},

    author={Lin, Xingyu and Wang, Yufei and Olkin, Jake and Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}}'
  img: ../pics/corl2020_softgym.png
  links:
    '[Code]': https://github.com/Xingyu-Lin/softgym
    '[PDF]': https://arxiv.org/pdf/2011.07215.pdf
  short_id: 2020softgym
  site: https://sites.google.com/view/softgym/home
  title: '<a href="https://arxiv.org/abs/2011.07215">SoftGym: Benchmarking Deep Reinforcement
    Learning for Deformable Object Manipulation</a>'
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: null
- abs: "Current image-based reinforcement learning (RL) algorithms typically operate\
    \ on the whole image without performing object-level reasoning.  This leads to\
    \ inefficient goal sampling and ineffective reward functions. In this paper, we\
    \ improve upon previous visual self-supervised RL by incorporating object-level\
    \ reasoning and occlusion reasoning. Specifically, we use unknown object segmentation\
    \ to ignore distractors in the scene for better reward computation and goal generation;\
    \ we further enable occlusion reasoning by employing a novel auxiliary loss and\
    \ training scheme. We demonstrate that our proposed algorithm,   ROLL (Reinforcement\
    \ learning with Object Level Learning), learns dramatically faster and achieves\
    \ better final performance compared with previous methods in several simulated\
    \ visual control tasks. Project video and code\n                     are available\
    \ at https://sites.google.com/andrew.cmu.edu/roll."
  authors: Yufei Wang*, Gautham Narayan Narasimhan*, Xingyu Lin, Brian Okorn, David
    Held
  award: null
  bib: '@inproceedings{corl2020roll,

    title={ROLL: Visual Self-Supervised Reinforcement Learning with Object Reasoning},

    author={Wang, Yufei and Narasimhan Gautham and Lin, Xingyu and Okorn, Brian and
    Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}

    }'
  img: ../pics/corl2020_ROLL.jpg
  links:
    '[Code]': https://github.com/yufeiwang63/ROLL
    '[PDF]': https://arxiv.org/pdf/2011.06777.pdf
  short_id: 2020roll
  site: https://sites.google.com/andrew.cmu.edu/roll/home
  title: <a href="https://arxiv.org/abs/2011.06777">Visual Self-Supervised Reinforcement
    Learning with Object Reasoning</a>
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: null
- abs: The goal of offline reinforcement learning is to learn a policy from a fixed
    dataset, without further interactions with the environment. This setting will
    be an increasingly more important paradigm for real-world applications of reinforcement
    learning such as robotics, in which data collection is slow and potentially dangerous.
    Existing off-policy algorithms have limited performance on static datasets due
    to extrapolation errors from out-of-distribution actions. This leads to the challenge
    of constraining the policy to select actions within the support of the dataset
    during training. We propose to simply learn the Policy in the Latent Action Space
    (PLAS) such that this requirement is naturally satisfied. We evaluate our method
    on continuous control benchmarks in simulation and a deformable object manipulation
    task with a physical robot. We demonstrate that our method provides competitive
    performance consistently across various continuous control tasks and different
    types of datasets, outperforming existing offline reinforcement learning methods
    with explicit constraints.
  authors: Wenxuan Zhou, Sujay Bajracharya, David Held
  award: <award>Plenary talk</award> (Selection rate 4.1%)
  bib: '@inproceedings{PLAS_corl2020,


    title={PLAS: Latent Action Space for Offline Reinforcement Learning},

    author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},

    booktitle={Conference on Robot Learning},

    year={2020}

    }'
  img: ../pics/cloth_sliding.gif
  links:
    '[Code]': https://github.com/Wenxuan-Zhou/PLAS
    '[PDF]': https://arxiv.org/pdf/2011.07213.pdf
  short_id: 2020corl
  site: https://sites.google.com/view/latent-policy
  title: '<a href="https://arxiv.org/abs/2011.07213">PLAS: Latent Action Space for
    Offline Reinforcement Learning</a>'
  venue: Conference on Robot Learning (CoRL), 2020
  video_embed: null
- abs: 'Visual data in autonomous driving perception, such as camera image and LiDAR
    point cloud, can be interpreted as a mixture of two aspects: semantic feature
    and geometric structure. Semantics come from the appearance and context of objects
    to the sensor, while geometric structure is the actual 3D shape of point clouds.
    Most detectors on LiDAR point clouds focus only on analyzing the geometric structure
    of objects in real 3D space. Unlike previous works, we propose to learn both semantic
    feature and geometric structure via a unified multi-view framework. Our method
    exploits the nature of LiDAR scans -- 2D range images, and applies well-studied
    2D convolutions to extract semantic features. By fusing semantic and geometric
    features, our method outperforms state-of-the-art approaches in all categories
    by a large margin. The methodology of combining semantic and geometric features
    provides a unique perspective of looking at the problems in real-world 3D point
    cloud detection.'
  authors: Xia Chen, Jianren Wang, David Held, Martial Hebert
  award: null
  bib: "@inproceedings{xia20panonet3d,\n    author = \"Chen, Xia \n    and Wang, Jianren\
    \ \n    and Held, David \n    and Hebert, Martial\",\n    title = \"PanoNet3D:\
    \ Combining Semantic and Geometric Understanding for LiDARPoint Cloud Detection\"\
    ,\n    booktitle = \"3DV\",\n    year = \"2020\"\n}"
  img: ../pics/panonet3db.jpg
  links:
    '[Code]': https://github.com/stooloveu/Det3D
    '[PDF]': https://arxiv.org/pdf/2012.09418.pdf
  short_id: 20203dv
  site: https://jianrenw.github.io/PanoNet3D/
  title: '<a href="https://arxiv.org/pdf/2012.09418.pdf">PanoNet3D: Combining Semantic
    and Geometric Understanding for LiDARPoint Cloud Detection</a>'
  venue: International Conference on 3D Vision (3DV), 2020
  video_embed: null
- abs: "Most real-world 3D sensors such as LiDARs perform fixed scans of the entire\
    \ environment, while being decoupled from the recognition system that processes\
    \ the sensor data. In this work, we propose a method for 3D object recognition\
    \ using light curtains, a resource-efficient controllable sensor that measures\
    \ depth at user-specified locations in the environment. Crucially, we propose\
    \ using prediction uncertainty of a deep learning based 3D point cloud detector\
    \ to guide active perception. Given a neural network\u2019s uncertainty, we derive\
    \ an optimization objective to place light curtains using the principle of maximizing\
    \ information gain. Then, we develop a novel and efficient optimization algorithm\
    \ to maximize this objective by encoding the physical constraints of the device\
    \ into a constraint graph and optimizing with dynamic programming. We show how\
    \ a 3D detector can be trained to detect objects in a scene by sequentially placing\
    \ uncertainty-guided light curtains to successively improve detection accuracy."
  authors: Siddharth Ancha, Yaadhav Raaj, Peiyun Hu, Srinivasa Narasimhan, David Held
  award: <award>Spotlight presentation</award> (Selection rate 5.3%)
  bib: "@InProceedings{Ancha_2020_ECCV,\n  author=\"Ancha, Siddharth\n  and Raaj,\
    \ Yaadhav\n  and Hu, Peiyun\n  and Narasimhan, Srinivasa G.\n  and Held, David\"\
    ,\n  editor=\"Vedaldi, Andrea\n  and Bischof, Horst\n  and Brox, Thomas\n  and\
    \ Frahm, Jan-Michael\",\n  title=\"Active Perception Using Light Curtains for\
    \ Autonomous Driving\",\n  booktitle=\"Computer Vision -- ECCV 2020\",\n  year=\"\
    2020\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\"\
    ,\n  pages=\"751--766\",\n  isbn=\"978-3-030-58558-7\"\n}"
  img: ../pics/eccv2020lightcurtains.jpg
  links:
    '[Code]': https://github.com/CMU-Light-Curtains/ObjectDetection
    '[Long Talk]': https://www.youtube.com/watch?v=uRP63hHArU0
    '[PDF]': https://arxiv.org/pdf/2008.02191.pdf
    '[Short Talk]': https://www.youtube.com/watch?v=WSb5T3HFE7w
  short_id: 2020eccv
  site: http://siddancha.github.io/projects/active-perception-light-curtains/
  title: <a href="https://arxiv.org/abs/2008.02191">Active Perception using Light
    Curtains for Autonomous Driving</a>
  venue: European Conference on Computer Vision (ECCV), 2020
  video_embed: null
- abs: 'Cloth detection and manipulation is a common task in domestic and industrial
    settings, yet such tasks remain a challenge for robots due to cloth deformability.
    Furthermore, in many cloth-related tasks like laundry folding and bed making,
    it is crucial to manipulate specific regions like edges and corners, as opposed
    to folds. In this work, we focus on the problem of segmenting and grasping these
    key regions. Our approach trains a network to segment the edges and corners of
    a cloth from a depth image, distinguishing such regions from wrinkles or folds.
    We also provide a novel algorithm for estimating the grasp location, direction,
    and directional uncertainty from the segmentation. We demonstrate our method on
    a real robot system and show that it outperforms baseline methods on grasping
    success. Video and other supplementary materials are available at:'
  authors: Jianing Qian*, Thomas Weng*, Luxin Zhang, Brian Okorn, David Held
  award: null
  bib: '@InProceedings{Qian_2020_IROS,

    author="Qian, Jianing

    and Weng, Thomas

    and Zhang, Luxin

    and Okorn, Brian

    and Held, David",

    title="Cloth Region Segmentation for Robust Grasp Selection",

    booktitle="International Conference on Intelligent Robots and Systems",

    year="2020",

    publisher="IEEE"}'
  img: ../pics/cloth_grasping.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2008.05626.pdf
  short_id: qian2020iros
  site: https://sites.google.com/view/cloth-segmentation
  title: <a href="https://arxiv.org/abs/2008.05626">Cloth Region Segmentation for
    Robust Grasp Selection</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
- abs: 3D object trackers usually require training on large amounts of annotated data
    that is expensive and time-consuming to collect. Instead, we propose leveraging
    vast unlabeled datasets by self-supervised metric learning of 3D object trackers,
    with a focus on data association. Large scale annotations for unlabeled data are
    cheaply obtained by automatic object detection and association across frames.
    We show how these self-supervised annotations can be used in a principled manner
    to learn point-cloud embeddings that are effective for 3D tracking. We estimate
    and incorporate uncertainty in self-supervised tracking to learn more robust embeddings,
    without needing any labeled data. We design embeddings to differentiate objects
    across frames, and learn them using uncertainty-aware self-supervised training.
    Finally, we demonstrate their ability to perform accurate data association across
    frames, towards effective and accurate 3D tracking.
  authors: Jianren Wang, Siddharth Ancha, Yi-Ting Chen, David Held
  award: null
  bib: "@inproceedings{jianren20s3da,\n    author = \"Wang, Jianren \n    and Ancha,\
    \ Siddharth \n    and Chen, Yi-Ting \n    and Held, David\",\n    title = \"Uncertainty-aware\
    \ Self-supervised 3D Data Association\",\n    booktitle = \"IROS\",\n    year\
    \ = \"2020\"\n}"
  img: ../pics/jianreniros2020.jpg
  links:
    '[Code]': https://github.com/jianrenw/Self-Supervised-3D-Data-Association
    '[PDF]': https://arxiv.org/pdf/2008.08173v1.pdf
  short_id: jianren2020iros
  site: https://jianrenw.github.io/Self-Supervised-3D-Data-Association/
  title: <a href="https://arxiv.org/abs/2008.08173">Uncertainty-aware Self-supervised
    3D Data Association</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
- abs: null
  authors: Brian Okorn, Mengyun Xu, Martial Hebert, David Held
  award: null
  bib: null
  img: ../pics/LearningOrientationDistributions-Website.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/2007.01418">Learning Orientation Distributions
    for Object Pose Estimation</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
- abs: 3D multi-object tracking (MOT) is an essential component for many applications
    such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses
    on developing accurate systems giving less attention to practical considerations
    such as computational cost and system complexity. In contrast, this work proposes
    a simple real-time 3D MOT system. Our system first obtains 3D detections from
    a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter
    and the Hungarian algorithm is used for state estimation and data association.
    Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space
    and standardized 3D MOT evaluation tools are missing for a fair comparison of
    3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with
    three new metrics to comprehensively evaluate 3D MOT methods. We show that, although
    our system employs a combination of classical MOT modules, we achieve state-of-the-art
    3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly,
    although our system does not use any 2D data as inputs, we achieve competitive
    performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate
    of 207.4 FPS on the KITTI dataset, achieving the fastest speed among all modern
    MOT systems.
  authors: Xinshuo Weng, Jianren Wang, David Held, Kris Kitani
  award: null
  bib: "@article{Weng2020_AB3DMOT, \nauthor = {Weng, Xinshuo and Wang, Jianren and\
    \ Held, David and Kitani, Kris}, \njournal = {IROS}, \ntitle = {3D Multi-Object\
    \ Tracking: A Baseline and New Evaluation Metrics}, \nyear = {2020} \n}"
  img: ../pics/ab3dmot.jpg
  links:
    '[Code]': https://github.com/xinshuoweng/AB3DMOT
    '[PDF]': http://www.xinshuoweng.com/projects/AB3DMOT/
  short_id: xinshuo2020iros
  site: http://www.xinshuoweng.com/projects/AB3DMOT/
  title: '<a href="https://arxiv.org/abs/1907.03961">3D Multi-Object Tracking: A Baseline
    and New Evaluation Metrics</a>'
  venue: International Conference on Intelligent Robots and Systems (IROS), 2020
  video_embed: null
- abs: When interacting with highly dynamic environments, scene flow allows autonomous
    systems to reason about the non-rigid motion of multiple independent objects.
    This is of particular interest in the field of autonomous driving, in which many
    cars, people, bicycles, and other objects need to be accurately tracked. Current
    state-of-the-art methods require annotated scene flow data from autonomous driving
    scenes to train scene flow networks with supervised learning. As an alternative,
    we present a method of training scene flow that uses two self-supervised losses,
    based on nearest neighbors and cycle consistency. These self-supervised losses
    allow us to train our method on large unlabeled autonomous driving datasets; the
    resulting method matches current state-of-the-art supervised performance using
    no real world annotations and exceeds state-of-the-art performance when combining
    our self-supervised approach with supervised learning on a smaller labeled dataset.
  authors: Himangi Mittal, Brian Okorn, David Held
  award: <award>Oral presentation</award> (Selection rate 5.7%)
  bib: '@InProceedings{Mittal_2020_CVPR,

    author = {Mittal, Himangi and Okorn, Brian and Held, David},

    title = {Just Go With the Flow: Self-Supervised Scene Flow Estimation},

    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)},

    month = {June},

    year = {2020}

    }'
  img: ../pics/jgwtf/Slide1.png
  links:
    '[Code]': https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation
    '[PDF]': https://arxiv.org/pdf/1912.00497.pdf
  short_id: 2020cvprflow
  site: https://just-go-with-the-flow.github.io/
  title: '<a href="https://arxiv.org/abs/1912.00497">Just Go with the Flow: Self-Supervised
    Scene Flow Estimation</a>'
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2020
  video_embed: null
- abs: null
  authors: Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan
  award: <award>Oral presentation</award> (Selection rate 5.7%)
  bib: null
  img: ../pics/peiyuncvpr2020/splash2.jpg
  links: {}
  short_id: null
  site: null
  title: '<a href="https://arxiv.org/abs/1912.04986">What You See is What You Get:
    Exploiting Visibility for 3D Object Detection</a>'
  venue: Conference on Computer Vision and Pattern Recognition (CVPR), 2020
  video_embed: null
- abs: State-of-the-art object grasping methods rely on depth sensing to plan robust
    grasps, but commercially available depth sensors fail to detect transparent and
    specular objects. To improve grasping performance on such objects, we introduce
    a method for learning a multi-modal perception model by bootstrapping from an
    existing uni-modal model. This transfer learning approach requires only a pre-existing
    uni-modal grasping model and paired multi-modal image data for training, foregoing
    the need for ground-truth grasp success labels nor real grasp attempts. Our experiments
    demonstrate that our approach is able to reliably grasp transparent and reflective
    objects. Video and supplementary material are available at
  authors: Thomas Weng, Amith Pallankize, Yimin Tang, Oliver Kroemer, David Held
  award: null
  bib: '@ARTICLE{9001238,

    author={Thomas Weng and Amith Pallankize and Yimin Tang and Oliver Kroemer and
    David Held},

    journal={IEEE Robotics and Automation Letters},

    title={Multi-Modal Transfer Learning for Grasping Transparent and Specular Objects},

    year={2020},

    volume={5},

    number={3},

    pages={3791-3798},

    doi={10.1109/LRA.2020.2974686}}'
  img: ../pics/transparent_grasping.gif
  links:
    '[PDF]': https://arxiv.org/pdf/2006.00028.pdf
  short_id: weng2020ral
  site: https://sites.google.com/view/transparent-specular-grasping
  title: <a href="https://arxiv.org/abs/2006.00028">Multi-Modal Transfer Learning
    for Grasping Transparent and Specular Objects</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2020
  video_embed: <iframe width="560" height="315" src="https://www.youtube.com/embed/rYRPWe0xLVo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
- abs: null
  authors: Peiyun Hu, David Held*, Deva Ramanan*
  award: null
  bib: null
  img: ../pics/peiyunral2020/output.jpg
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1912.04976">Learning to Optimally Segment
    Point Clouds</a>
  venue: Robotics and Automation Letters (RAL) with presentation at the International
    Conference of Robotics and Automation (ICRA), 2020
  video_embed: null 
- abs: Deep learning object detectors often return false positives with very high
    confidence. Although they optimize generic detection performance, such as mean
    average precision (mAP), they are not designed for reliability. For a reliable
    detection system, if a high confidence detection is made, we would want high certainty
    that the object has indeed been detected. To achieve this, we have developed a
    set of verification tests which a proposed detection must pass to be accepted.
    We develop a theoretical framework which proves that, under certain assumptions,
    our verification tests will not accept any false positives. Based on an approximation
    to this framework, we present a practical detection system that can verify, with
    high precision, whether each detection of a machine-learning based object detector
    is correct. We show that these tests can improve the overall accuracy of a base
    detector and that accepted examples are highly likely to be correct. This allows
    the detector to operate in a high precision regime and can thus be used for robotic
    perception systems as a reliable instance detection method.
  authors: Siddharth Ancha*, Junyu Nan*, David Held
  award: null
  bib: "@inproceedings{FlowVerify2019CoRL,\n  author    = {Siddharth Ancha and\n \
    \              Junyu Nan and\n               David Held},\n  editor    = {Leslie\
    \ Pack Kaelbling and\n               Danica Kragic and\n               Komei Sugiura},\n\
    \  title     = {Combining Deep Learning and Verification for Precise Object Instance\n\
    \               Detection},\n  booktitle = {3rd Annual Conference on Robot Learning,\
    \ CoRL 2019, Osaka, Japan,\n               October 30 - November 1, 2019, Proceedings},\n\
    \  series    = {Proceedings of Machine Learning Research},\n  volume    = {100},\n\
    \  pages     = {122--141},\n  publisher = ,\n  year      = {2019},\n  url    \
    \   = {http://proceedings.mlr.press/v100/ancha20a.html},\n  timestamp = {Mon,\
    \ 25 May 2020 15:01:26 +0200},\n  biburl    = {https://dblp.org/rec/conf/corl/AnchaNH19.bib},\n\
    \  bibsource = {dblp computer science bibliography, https://dblp.org}\n}"
  img: ../pics/CoRL2019.png
  links:
    '[Code]': https://github.com/siddancha/FlowVerify
    '[PDF]': https://arxiv.org/pdf/1912.12270.pdf
  short_id: 2019corl
  site: https://jnan1.github.io/FlowVerify/
  title: <a href="https://arxiv.org/abs/1912.12270">Combining Deep Learning and Verification
    for Precise Object Instance Detection</a>
  venue: Conference on Robot Learning (CoRL), 2019
  video_embed: null
- abs: null
  authors: Xingyu Lin*, Harjatin Baweja*, George Kantor, David Held
  award: null
  bib: null
  img: ../pics/2019_olaux.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://papers.nips.cc/paper/8724-adaptive-auxiliary-task-weighting-for-reinforcement-learning">Adaptive
    Auxiliary Task Weighting for Reinforcement Learning</a>
  venue: Neural Information Processing Systems (NeurIPS), 2019
  video_embed: null
- abs: null
  authors: Xingyu Lin, Pengsheng Guo, Carlos Florensa, David Held
  award: null
  bib: null
  img: ../DavidHeld_files/icra19.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1903.06309">Adaptive Variance for Changing
    Sparse-Reward Environments</a>
  venue: International Conference of Robotics and Automation (ICRA), 2019
  video_embed: null
- abs: null
  authors: Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert
  award: null
  bib: null
  img: ../DavidHeld_files/pcn.png
  links: {}
  short_id: null
  site: null
  title: '<a href="http://arxiv.org/abs/1808.00671">PCN: Point Completion Network
    - <award>Best Paper Honorable Mention</award></a>'
  venue: International Conference on 3D Vision (3DV), 2018
  video_embed: null
- abs: null
  authors: Carlos Florensa*, David Held*, Xinyang Geng*, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/ant-maze.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.06366">Automatic Goal Generation for
    Reinforcement Learning Agents</a>
  venue: International Conference on Machine Learning (ICML), 2018
  video_embed: null
- abs: null
  authors: Sandy Han Huang, David Held, Pieter Abbeel, Anca D. Dragan
  award: null
  bib: null
  img: ../DavidHeld_files/enabling_robots_RSS_2017.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://rdcu.be/ZM50">Enabling Robots to Communicate their Objectives</a>
  venue: Autonomous Robotics (AURO), 2018
  video_embed: null
- abs: null
  authors: Carlos Florensa, David Held, Markus Wulfmeier, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/reverse_curr2.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1707.05300">Reverse Curriculum Generation
    for Reinforcement Learning</a>
  venue: Conference on Robot Learning (CoRL), 2017
  video_embed: null
- abs: null
  authors: Ignasi Clavera*, David Held*, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/transfer.png
  links: {}
  short_id: null
  site: null
  title: <a href="DavidHeld_files/IROS___RL_pushing.pdf">Policy  Transfer via Modularity</a>
  venue: International Conference on Intelligent Robots and Systems (IROS), 2017
  video_embed: null
- abs: null
  authors: Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/humanoid.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.10528">Constrained Policy Optimization</a>
  venue: International Conference on Machine Learning (ICML), 2017
  video_embed: null
- abs: null
  authors: Sandy H. Huang, David Held, Pieter Abbeel, Anca D. Dragan
  award: null
  bib: null
  img: ../DavidHeld_files/enabling_robots_RSS_2017.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1702.03465">Enabling Robots to Communicate
    their Objectives</a>
  venue: 'Robotics: Science and Systems (RSS), 2017'
  video_embed: null
- abs: null
  authors: David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, Pieter Abbeel
  award: null
  bib: null
  img: ../DavidHeld_files/safe_transfer3.png
  links: {}
  short_id: null
  site: null
  title: <a href="https://arxiv.org/abs/1705.05394">Probabilistically Safe Policy
    Transfer</a>
  venue: International Conference on Robotics and Automation (ICRA), 2017
  video_embed: null
- abs: null
  authors: David Held, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull7f-web_d.png
  links:
    '[Full Paper]': GOTURN/GOTURN.pdf
    '[Supplement]': GOTURN/supplement.pdf
  short_id: null
  site: GOTURN/GOTURN.html
  title: <a href="GOTURN/GOTURN.pdf">Learning to Track at 100 FPS with Deep Regression
    Networks</a>
  venue: European Conference on Computer Vision (ECCV), 2016
  video_embed: null
- abs: null
  authors: David Held, Devin Guillory, Brice Rebsamen, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull_fig2.png
  links: {}
  short_id: null
  site: null
  title: <a href="segmentation3D/segmentation3D.html">A Probabilistic Framework for
    Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues</a>
  venue: 'Robotics: Science and Systems (RSS), 2016'
  video_embed: null
- abs: null
  authors: David Held, Sebastian Thrun, Silvio Savarese
  award: null
  bib: null
  img: ../DavidHeld_files/pull6_flatb.png
  links: {}
  short_id: null
  site: null
  title: <a href="http://arxiv.org/abs/1507.08286">Robust Single-View Instance Recognition</a>
  venue: International Conference on Robotics and Automation (ICRA), 2016
  video_embed: null